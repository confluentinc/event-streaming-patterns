{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Event Streaming Patterns Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Event Streaming Patterns"},{"location":"#welcome-to-event-streaming-patterns","text":"Event Streaming Platforms provide an architecture that enables software to react and operate as events occur. These platforms allow for software components to work together in a real-time, decoupled, and scalable fashion. When software is modeled as streams of events, new capabilities surface along with new unique technical challenges. This catalog contains simple and reusable architectural patterns that can be applied over event streaming systems. When composed together, these patterns can help meet the design demands of modern real-time distributed systems. The following patterns are categorized by their function in the event streaming system, including sourcing data, processing events as streams, to integrations with external systems. TODO: Brand main image","title":"Welcome to Event Streaming Patterns"},{"location":"compositional-patterns/command-query-responsibility-segregation/","text":"Command Query Responsibility Segregation (CQRS) Databases conflate the writing of data and the reading of data in the same place: the database. In some situations, it is preferable to separate reads from writes. There are several reasons to do this but the most prevalent is that the application can now save data in the exact form in which it arrives, accurately reflecting what happened in the real world, while reading it in a different form, one that is optimized for reading. For example, a user adding and removing items from their cart would all be recorded as a stream of immutable events: t-shirt added, t-shirt removed, etc. These are then summarized into a separate view that used to serve reads, for example summarizing the various user events to represent the accurate contents of the cart. Problem How can we store and hold data in the exact form in which it arrived but read from a summarized and curated view? Solution Represent changes that happen in the real world as Events - an order is shipped, a ride is accepted, etc. - and retain these events as the system of record. Subsequently, aggregate those Events into a view that summarizes the events to represent the current state, allowing applications to query the current values. So for example, the current balance of an account would be the total of all the payment events that added money to or removed it from the account. The system of record is the stream of payment events. The view we read from would be the account balance. Implementation The streaming database ksqlDB can implement a CQRS using an Event Stream and Table . Event Streams are built into to the streaming database design. Creating a new stream is straightforward: CREATE STREAM purchases (customer VARCHAR, item VARCHAR, qty INT WITH (kafka_topic='purchases-topic', value_format='json', partitions=1); Events can be directly using familiar SQL syntax. INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'sweaters', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', -1); We can create a Materialized View of the data as a Table : CREATE TABLE customer_purchases WITH (KEY_FORMAT='JSON') AS SELECT customer, item, SUM(qty) as total_qty from purchases GROUP BY customer, item emit changes; And continuously query for changes to the state of the customer_purchases table: SELECT * FROM customer_purchases EMIT CHANGES; Considerations CQRS adds complexity over a traditional simple CRUD database implementation. High performance applications may benefit from a CQRS design. Isolating the load of writing and reading of data may allow us to scale those aspects independently and properly. Microservices applications often use CQRS to scale-out with many views provided for different services. The same pattern is applicable to geographically dispersed applications such as a flight booking system which are read heavy across many locations. A write to a CQRS system is eventually consistent. Writes cannot be read immediately as there is a delay between the write of the command Event and the query-model being updated. This can cause complexity for some client applications, particularly online services. References See Martin Fowler's detailed explanation of CQRS for more information.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"compositional-patterns/command-query-responsibility-segregation/#command-query-responsibility-segregation-cqrs","text":"Databases conflate the writing of data and the reading of data in the same place: the database. In some situations, it is preferable to separate reads from writes. There are several reasons to do this but the most prevalent is that the application can now save data in the exact form in which it arrives, accurately reflecting what happened in the real world, while reading it in a different form, one that is optimized for reading. For example, a user adding and removing items from their cart would all be recorded as a stream of immutable events: t-shirt added, t-shirt removed, etc. These are then summarized into a separate view that used to serve reads, for example summarizing the various user events to represent the accurate contents of the cart.","title":"Command Query Responsibility Segregation (CQRS)"},{"location":"compositional-patterns/command-query-responsibility-segregation/#problem","text":"How can we store and hold data in the exact form in which it arrived but read from a summarized and curated view?","title":"Problem"},{"location":"compositional-patterns/command-query-responsibility-segregation/#solution","text":"Represent changes that happen in the real world as Events - an order is shipped, a ride is accepted, etc. - and retain these events as the system of record. Subsequently, aggregate those Events into a view that summarizes the events to represent the current state, allowing applications to query the current values. So for example, the current balance of an account would be the total of all the payment events that added money to or removed it from the account. The system of record is the stream of payment events. The view we read from would be the account balance.","title":"Solution"},{"location":"compositional-patterns/command-query-responsibility-segregation/#implementation","text":"The streaming database ksqlDB can implement a CQRS using an Event Stream and Table . Event Streams are built into to the streaming database design. Creating a new stream is straightforward: CREATE STREAM purchases (customer VARCHAR, item VARCHAR, qty INT WITH (kafka_topic='purchases-topic', value_format='json', partitions=1); Events can be directly using familiar SQL syntax. INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'hats', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'sweaters', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', 1); INSERT INTO purchases (customer, item, qty) VALUES ('jsmith', 'pants', -1); We can create a Materialized View of the data as a Table : CREATE TABLE customer_purchases WITH (KEY_FORMAT='JSON') AS SELECT customer, item, SUM(qty) as total_qty from purchases GROUP BY customer, item emit changes; And continuously query for changes to the state of the customer_purchases table: SELECT * FROM customer_purchases EMIT CHANGES;","title":"Implementation"},{"location":"compositional-patterns/command-query-responsibility-segregation/#considerations","text":"CQRS adds complexity over a traditional simple CRUD database implementation. High performance applications may benefit from a CQRS design. Isolating the load of writing and reading of data may allow us to scale those aspects independently and properly. Microservices applications often use CQRS to scale-out with many views provided for different services. The same pattern is applicable to geographically dispersed applications such as a flight booking system which are read heavy across many locations. A write to a CQRS system is eventually consistent. Writes cannot be read immediately as there is a delay between the write of the command Event and the query-model being updated. This can cause complexity for some client applications, particularly online services.","title":"Considerations"},{"location":"compositional-patterns/command-query-responsibility-segregation/#references","text":"See Martin Fowler's detailed explanation of CQRS for more information.","title":"References"},{"location":"compositional-patterns/event-collaboration/","text":"Event Collaboration Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states. Problem How can we build a distributed workflow in a way that allows components to evolve independently? Solution Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete. Considerations In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asynchronously by way of a global identifier which traverses the workflow within the events. References Event Collaboration by Martin Fowler","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#event-collaboration","text":"Building distributed business workflows requires coordinating multiple services and Event Processing Applications . Business actions and reactions must be coordinated asynchronously as complex workflows transition through various states.","title":"Event Collaboration"},{"location":"compositional-patterns/event-collaboration/#problem","text":"How can we build a distributed workflow in a way that allows components to evolve independently?","title":"Problem"},{"location":"compositional-patterns/event-collaboration/#solution","text":"Event Collaboration allows services and applications to collaborate around a single business workflow on top of an Event Streaming Platform . Service components publish Events to Event Streams as notification of the completion of a step in the workflow. The Events serve the additional purpose of carrying state information about the workflow which is used by downstream components in the next steps of the workflow. The process repeats the until the workflow is complete.","title":"Solution"},{"location":"compositional-patterns/event-collaboration/#considerations","text":"In Event Collaboration, the logic for the choreography of the progression of the business workflow is decentralized and spread across many components. This contrasts with traditional orchestration design where the logic is isolated in a dedicated \"controller\" service that coordinates the actions and reactions of the workflow components. With Event Collaboration, some workflow components will need to be able to ascertain the state of the workflow some time after they have generated their own Event. A classic example would be an order request service which generates a new order request event and wants to be notified when the order is complete. These Events need to be correlated through the distributed workflow to support such functionality. The Correlation Identifier pattern describes a method of coupling Events when processed asynchronously by way of a global identifier which traverses the workflow within the events.","title":"Considerations"},{"location":"compositional-patterns/event-collaboration/#references","text":"Event Collaboration by Martin Fowler","title":"References"},{"location":"compositional-patterns/geo-replication/","text":"Geo Replication Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply. Problem How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others? Solution Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data. Implementation Practically, replication is not enabled completely on all data streams, as there are always exceptions, organizational limitations, technical constraints, or other reasons why we wouldn't want to copy absolutely everything. Instead, we can do this on a per topic basis, where we can map a source topic to a destination topic. With Apache Kafka\u00ae, we can do this in one of several ways. Option 1: Cluster Linking Cluster Linking enables easy data sharing between event streaming platforms, mirroring topics across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ, Active MQ, etc., provide similar functionality but without the same levels of parallelism. Option 2: Connect-based Replication Operators can set up such inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers. Considerations Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain. References This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#geo-replication","text":"Many architectures have streams of events deployed across multiple datacenters spanning boundaries of Event Streaming Platforms , datacenters, or geographical regions. In these situations, it may be useful for client applications in one event streaming platform to have access to Events produced in another one. All clients shouldn't be forced to read from the source event streaming platform, which can incur high latency and data egress costs. Instead, with a move-once-read-many approach, the data can be replicated to a local datacenter where clients can do all their processing quickly and cheaply.","title":"Geo Replication"},{"location":"compositional-patterns/geo-replication/#problem","text":"How can multiple Event Streaming Platforms be connected so that events available in one site are also available on the others?","title":"Problem"},{"location":"compositional-patterns/geo-replication/#solution","text":"Create a connection between the two Event Streaming Platforms , enabling the destination platform to read from the source one. Ideally this is done in realtime such that as new events are published in the source event streaming platform, they can be immediately copied, byte for byte, to the destination event streaming platform. This allows the client applications in the destination to leverage the same set of data.","title":"Solution"},{"location":"compositional-patterns/geo-replication/#implementation","text":"Practically, replication is not enabled completely on all data streams, as there are always exceptions, organizational limitations, technical constraints, or other reasons why we wouldn't want to copy absolutely everything. Instead, we can do this on a per topic basis, where we can map a source topic to a destination topic. With Apache Kafka\u00ae, we can do this in one of several ways.","title":"Implementation"},{"location":"compositional-patterns/geo-replication/#option-1-cluster-linking","text":"Cluster Linking enables easy data sharing between event streaming platforms, mirroring topics across them. Because Cluster Linking uses native replication protocols, client applications can easily failover in the case of a disaster recovery scenario. ccloud kafka link create east-west ... ccloud kafka topic create <destination topic> --link east-west --mirror-topic <source topic> ... Other messaging systems like RabbitMQ, Active MQ, etc., provide similar functionality but without the same levels of parallelism.","title":"Option 1: Cluster Linking"},{"location":"compositional-patterns/geo-replication/#option-2-connect-based-replication","text":"Operators can set up such inter-cluster data flows with Confluent's Replicator or Kafka's MirrorMaker (version 2), tools that replicate data between different Kafka environments. Unlike Cluster Linking, these are separate services built upon Kafka Connect, with built-in producers and consumers.","title":"Option 2: Connect-based Replication"},{"location":"compositional-patterns/geo-replication/#considerations","text":"Note that this type of replication between event streaming platforms is asynchronous, which means an event that is recorded in the source may not be immediately available at the destination. There is also synchronous replication across event streaming platforms (e.g. Multi Region Clusters ) but this is often limited to when the event streaming platforms are in the same operational domain.","title":"Considerations"},{"location":"compositional-patterns/geo-replication/#references","text":"This pattern is derived from Messaging Bridge in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"compositional-patterns/pipeline/","text":"Pipeline A single Event Stream or Table can be used by multiple Event Processing Applications , and its Events may go through multiple processing stages along the way (e.g., filters, transformations, joins, aggregations) to implement more complex use cases. Problem How can a single processing objective for a set of Event Streams and/or Tables be achieved through a series of independent processing stages? Solution We can compose Event Streams and Tables in an Event Streaming Platform via an Event Processing Application to a create a pipeline\u2014also called a topology\u2014of Event Processors , which continuously process the events flowing through them. Here, the output of one processor is the input for one or more downstream processors. Pipelines, notably when created for use cases such as Streaming ETL , may include Event Source Connectors and Event Sink Connectors , which continuously import and export data as streams from/to external services and systems, respectively. Connectors are particularly useful for turning data at rest in such systems into data in motion. Taking a step back, we can see that pipelines in an Event Streaming Platform help companies build a \"central nervous system\" for data in motion. Implementation As an example we can use the streaming database ksqlDB to run a stream of events through a series of processing stages, thus creating a Pipeline that continuously processes data in motion. CREATE STREAM orders ( customer_id INTEGER, items ARRAY<STRUCT<name VARCHAR, price DOUBLE>> ) WITH ( KAFKA_TOPIC = 'orders', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); We'll also create a (continuously updated) customers table that will contain the latest profile information about each customer, such as their current home address. CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, name VARCHAR, ADDRESS VARCHAR ) WITH ( KAFKA_TOPIC = 'customers', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); Next, we create a new stream by joining the orders stream with our customer table: CREATE STREAM orders_enriched WITH (KAFKA_TOPIC='orders_enriched', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT o.customer_id AS cust_id, o.items, c.name, c.address FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id EMIT CHANGES; Next, we create a stream, where we add the order total to each order by aggregating the price of the individual items in the order: CREATE STREAM orders_with_totals WITH (KAFKA_TOPIC='orders_totaled', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT cust_id, items, name, address, REDUCE(TRANSFORM(items, i=> i->price ), 0e0, (i,x) => (i + x)) AS total FROM orders_enriched EMIT CHANGES; Considerations The same event stream or table can participate in multiple pipelines. Because streams and tables are stored durably, applications have a lot of flexibility how and when they process the respective data, and they can do so independently from each other. The various processing stages in a pipeline create their own derived streams/tables (such as the orders_enriched stream in the ksqlDB example above), which in turn can be used as input for other pipelines and applications. This allows for further and more complex composition and re-use of events throughout an organization. References This pattern was influenced by Pipes and Filters in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. However, it is much more powerful and flexible because it is using Event Streams as the pipes.","title":"Pipeline"},{"location":"compositional-patterns/pipeline/#pipeline","text":"A single Event Stream or Table can be used by multiple Event Processing Applications , and its Events may go through multiple processing stages along the way (e.g., filters, transformations, joins, aggregations) to implement more complex use cases.","title":"Pipeline"},{"location":"compositional-patterns/pipeline/#problem","text":"How can a single processing objective for a set of Event Streams and/or Tables be achieved through a series of independent processing stages?","title":"Problem"},{"location":"compositional-patterns/pipeline/#solution","text":"We can compose Event Streams and Tables in an Event Streaming Platform via an Event Processing Application to a create a pipeline\u2014also called a topology\u2014of Event Processors , which continuously process the events flowing through them. Here, the output of one processor is the input for one or more downstream processors. Pipelines, notably when created for use cases such as Streaming ETL , may include Event Source Connectors and Event Sink Connectors , which continuously import and export data as streams from/to external services and systems, respectively. Connectors are particularly useful for turning data at rest in such systems into data in motion. Taking a step back, we can see that pipelines in an Event Streaming Platform help companies build a \"central nervous system\" for data in motion.","title":"Solution"},{"location":"compositional-patterns/pipeline/#implementation","text":"As an example we can use the streaming database ksqlDB to run a stream of events through a series of processing stages, thus creating a Pipeline that continuously processes data in motion. CREATE STREAM orders ( customer_id INTEGER, items ARRAY<STRUCT<name VARCHAR, price DOUBLE>> ) WITH ( KAFKA_TOPIC = 'orders', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); We'll also create a (continuously updated) customers table that will contain the latest profile information about each customer, such as their current home address. CREATE TABLE customers ( customer_id INTEGER PRIMARY KEY, name VARCHAR, ADDRESS VARCHAR ) WITH ( KAFKA_TOPIC = 'customers', PARTITIONS = 1, VALUE_FORMAT = 'AVRO' ); Next, we create a new stream by joining the orders stream with our customer table: CREATE STREAM orders_enriched WITH (KAFKA_TOPIC='orders_enriched', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT o.customer_id AS cust_id, o.items, c.name, c.address FROM orders o LEFT JOIN customers c ON o.customer_id = c.customer_id EMIT CHANGES; Next, we create a stream, where we add the order total to each order by aggregating the price of the individual items in the order: CREATE STREAM orders_with_totals WITH (KAFKA_TOPIC='orders_totaled', PARTITIONS=1, VALUE_FORMAT='AVRO') AS SELECT cust_id, items, name, address, REDUCE(TRANSFORM(items, i=> i->price ), 0e0, (i,x) => (i + x)) AS total FROM orders_enriched EMIT CHANGES;","title":"Implementation"},{"location":"compositional-patterns/pipeline/#considerations","text":"The same event stream or table can participate in multiple pipelines. Because streams and tables are stored durably, applications have a lot of flexibility how and when they process the respective data, and they can do so independently from each other. The various processing stages in a pipeline create their own derived streams/tables (such as the orders_enriched stream in the ksqlDB example above), which in turn can be used as input for other pipelines and applications. This allows for further and more complex composition and re-use of events throughout an organization.","title":"Considerations"},{"location":"compositional-patterns/pipeline/#references","text":"This pattern was influenced by Pipes and Filters in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf. However, it is much more powerful and flexible because it is using Event Streams as the pipes.","title":"References"},{"location":"event/command-event/","text":"Command Event The Events in our systems often seem to fall into two categories: messages and commands. Message-like events resemble simple facts - a user sends us their new address, a product leaves the warehouse - and we record those facts first, without immediately considering what happens next. Other events seem more like commands to invoke a specific action - a user clicks a [BUY] button - and it's time to trigger order processing. How do we model command-like events? Problem How can an Event Streaming Platform be used to invoke a procedure in another application? Solution Separate out the function call into a service that writes an event to an Event Stream , detailing the action we need to take and its arguments. Then write a separate service that watches for that event before invoking the procedure. Typically a Command Event is dispatched in a fire-and-forget manner. The writer assumes the event will be handled correctly, and responsibility for monitoring and error-handling lies elsewhere in the system. This is very similar to the Actor model. Actors have an inbox; we write messages to that inbox and trust they'll be handled in due course. If a return value is explicitly required, the downstream service can write a result event back to a second stream. Correlating Command Events with their return value is typically handled with a Correlation Identifier . Implementation Suppose we have a [BUY] button that should trigger a dispatchProduct(12005) function call in our warehousing system. Rather than calling the function directly, we can split the call up as a command stream: CREATE STREAM dispatch_products ( order_id BIGINT KEY, address VARCHAR ) WITH ( KAFKA_TOPIC = ' dispatch_products', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); Some process that inserts into that stream: INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12004, '1 Streetford Road' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12005, '2 Roadford Avenue' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12006, '3 Avenue Fordstreet' ); And a second process that watches the stream of events and invokes the dispatchProduct procedure foreach : ... Serde<GenericRecord> valueGenericAvroSerde = ... StreamsBuilder builder = new StreamsBuilder(); KStream<Long, GenericRecord> dispatchStream = builder.stream( \"dispatch_products\", Consumed.with(Serdes.Long(), valueGenericAvroSerde) ); dispatchStream.foreach((key, value) -> warehouse.dispatchProduct(key, value)); Considerations This approach is fine and it works, but it may be a missed opportunity to improved the overall architecture. Consider what happens when we need more actions. Suppose that [BUY] should also trigger an email and a text notification to the customer. Should the warehouse software finish its work and then write SendEmail and SendText commands to two new topics? Or should these two new events be written by the same process that wrote DispatchProduct ? Then a month later, when we need our sales figures, should we count the number of products dispatched or the number of emails sent? Perhaps both, to check they agree? The system grows a little more and we have to ask, how much code is behind that [BUY] button? What's the release cycle? Is changing it becoming a blocker? [BUY] is important to the whole company, and rightly so, but its maintenance shouldn't hold the company to ransom. The root problem here is that in moving from a function call within a monolith to a system that posts a specific command to a specific recipient, we've decoupled the function call without decoupling the underlying concepts. When we do that, the architecture hits back with growing pains 1 . The real solution is to realize our \"Command Event\" is actually two concepts woven together: \"What happened?\" and \"Who cares?\" By teasing those concepts apart, we can clean up our architecture. We allow one process to focus on recording the facts of what happened, while other processes decide for themselves if they care. When the [BUY] click happens, we should just write an Order event. Then warehousing, notifications and sales can choose to react, without any need to coordinate. In short, commands are tightly coupled to an audience of one, whereas an event should just be a decoupled fact, available for anyone who's interested. Commands aren't bad per se , but they are a flag that signals an opportunity for further decoupling. Seeing systems this way requires a slight shift of perspective - a new way of modeling our processes - and opens up the opportunity for systems that collaborate more easily while actually taking on less individual responsibility. References This can approach become complex if there is a chain of functions, where the result of one is fed into the arguments of the next. In that situation, consider using Event Collaboration . See Designing Event Driven Systems - \"Chapter 5: Events: A Basis for Collaboration\" for further discussion This pattern is derived from Command Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Footnotes 1 It's at that point that someone in the team will say, \"We were better off just calling the function directly.\" And if we stopped there, they'd have a fair point.","title":"Command Event"},{"location":"event/command-event/#command-event","text":"The Events in our systems often seem to fall into two categories: messages and commands. Message-like events resemble simple facts - a user sends us their new address, a product leaves the warehouse - and we record those facts first, without immediately considering what happens next. Other events seem more like commands to invoke a specific action - a user clicks a [BUY] button - and it's time to trigger order processing. How do we model command-like events?","title":"Command Event"},{"location":"event/command-event/#problem","text":"How can an Event Streaming Platform be used to invoke a procedure in another application?","title":"Problem"},{"location":"event/command-event/#solution","text":"Separate out the function call into a service that writes an event to an Event Stream , detailing the action we need to take and its arguments. Then write a separate service that watches for that event before invoking the procedure. Typically a Command Event is dispatched in a fire-and-forget manner. The writer assumes the event will be handled correctly, and responsibility for monitoring and error-handling lies elsewhere in the system. This is very similar to the Actor model. Actors have an inbox; we write messages to that inbox and trust they'll be handled in due course. If a return value is explicitly required, the downstream service can write a result event back to a second stream. Correlating Command Events with their return value is typically handled with a Correlation Identifier .","title":"Solution"},{"location":"event/command-event/#implementation","text":"Suppose we have a [BUY] button that should trigger a dispatchProduct(12005) function call in our warehousing system. Rather than calling the function directly, we can split the call up as a command stream: CREATE STREAM dispatch_products ( order_id BIGINT KEY, address VARCHAR ) WITH ( KAFKA_TOPIC = ' dispatch_products', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); Some process that inserts into that stream: INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12004, '1 Streetford Road' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12005, '2 Roadford Avenue' ); INSERT INTO dispatch_products ( order_id, address ) VALUES ( 12006, '3 Avenue Fordstreet' ); And a second process that watches the stream of events and invokes the dispatchProduct procedure foreach : ... Serde<GenericRecord> valueGenericAvroSerde = ... StreamsBuilder builder = new StreamsBuilder(); KStream<Long, GenericRecord> dispatchStream = builder.stream( \"dispatch_products\", Consumed.with(Serdes.Long(), valueGenericAvroSerde) ); dispatchStream.foreach((key, value) -> warehouse.dispatchProduct(key, value));","title":"Implementation"},{"location":"event/command-event/#considerations","text":"This approach is fine and it works, but it may be a missed opportunity to improved the overall architecture. Consider what happens when we need more actions. Suppose that [BUY] should also trigger an email and a text notification to the customer. Should the warehouse software finish its work and then write SendEmail and SendText commands to two new topics? Or should these two new events be written by the same process that wrote DispatchProduct ? Then a month later, when we need our sales figures, should we count the number of products dispatched or the number of emails sent? Perhaps both, to check they agree? The system grows a little more and we have to ask, how much code is behind that [BUY] button? What's the release cycle? Is changing it becoming a blocker? [BUY] is important to the whole company, and rightly so, but its maintenance shouldn't hold the company to ransom. The root problem here is that in moving from a function call within a monolith to a system that posts a specific command to a specific recipient, we've decoupled the function call without decoupling the underlying concepts. When we do that, the architecture hits back with growing pains 1 . The real solution is to realize our \"Command Event\" is actually two concepts woven together: \"What happened?\" and \"Who cares?\" By teasing those concepts apart, we can clean up our architecture. We allow one process to focus on recording the facts of what happened, while other processes decide for themselves if they care. When the [BUY] click happens, we should just write an Order event. Then warehousing, notifications and sales can choose to react, without any need to coordinate. In short, commands are tightly coupled to an audience of one, whereas an event should just be a decoupled fact, available for anyone who's interested. Commands aren't bad per se , but they are a flag that signals an opportunity for further decoupling. Seeing systems this way requires a slight shift of perspective - a new way of modeling our processes - and opens up the opportunity for systems that collaborate more easily while actually taking on less individual responsibility.","title":"Considerations"},{"location":"event/command-event/#references","text":"This can approach become complex if there is a chain of functions, where the result of one is fed into the arguments of the next. In that situation, consider using Event Collaboration . See Designing Event Driven Systems - \"Chapter 5: Events: A Basis for Collaboration\" for further discussion This pattern is derived from Command Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event/command-event/#footnotes","text":"1 It's at that point that someone in the team will say, \"We were better off just calling the function directly.\" And if we stopped there, they'd have a fair point.","title":"Footnotes"},{"location":"event/correlation-identifier/","text":"Correlation Identifier Event Processing Applications may want to implement an Event Collaboration pattern where Events are used to transport requests and responses. The applications which collaborate via the Events , will need a method for correlating Event response data for specific requests. Problem How does an application, that has requested information and received a response, know for which request a particular response is for? Solution An Event Processor generates an Event which acts as the request. A globally unique identifier is added to the request Event prior to sending. This allows the responding Event Processor to include the identifier in the response Event, allowing the requesting processor to correlate the request and response. Implementation In Kafka, we can add a globally unique identifier to the Kafka record headers when producing the request event. The following code example uses Kafka's Java producer client. ProducerRecord<String, String> requestEvent = new ProducerRecord<>(\"request-event-key\", \"request-event-value\"); requestEvent.headers().add(\"requestID\", UUID.randomUUID().toString()); requestEvent.send(producerRecord); In the responding event processor, we first extract the correlation identifier from the request event (here, requestID ) and then add the identifier to the response event. ProducerRecord<String, String> responseEvent = new ProducerRecord<>(\"response-event-key\", \"response-event-value\"); requestEvent.headers().add(\"requestID\", requestEvent.headers().lastHeader(\"requestID\").value()); requestEvent.send(producerRecord); References This pattern is derived from Correlation Identifier in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf For a case study on coordinating microservices towards higher level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB Correlation Identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution. The idea of tagging requests and their related responses exists in many other protocols. For example, an email client connecting over IMAP will send commands prefixed with a unique ID (typically, a001 , a002 , etc.) and the server will respond asynchronously, tagging its responses with the matching ID.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#correlation-identifier","text":"Event Processing Applications may want to implement an Event Collaboration pattern where Events are used to transport requests and responses. The applications which collaborate via the Events , will need a method for correlating Event response data for specific requests.","title":"Correlation Identifier"},{"location":"event/correlation-identifier/#problem","text":"How does an application, that has requested information and received a response, know for which request a particular response is for?","title":"Problem"},{"location":"event/correlation-identifier/#solution","text":"An Event Processor generates an Event which acts as the request. A globally unique identifier is added to the request Event prior to sending. This allows the responding Event Processor to include the identifier in the response Event, allowing the requesting processor to correlate the request and response.","title":"Solution"},{"location":"event/correlation-identifier/#implementation","text":"In Kafka, we can add a globally unique identifier to the Kafka record headers when producing the request event. The following code example uses Kafka's Java producer client. ProducerRecord<String, String> requestEvent = new ProducerRecord<>(\"request-event-key\", \"request-event-value\"); requestEvent.headers().add(\"requestID\", UUID.randomUUID().toString()); requestEvent.send(producerRecord); In the responding event processor, we first extract the correlation identifier from the request event (here, requestID ) and then add the identifier to the response event. ProducerRecord<String, String> responseEvent = new ProducerRecord<>(\"response-event-key\", \"response-event-value\"); requestEvent.headers().add(\"requestID\", requestEvent.headers().lastHeader(\"requestID\").value()); requestEvent.send(producerRecord);","title":"Implementation"},{"location":"event/correlation-identifier/#references","text":"This pattern is derived from Correlation Identifier in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf For a case study on coordinating microservices towards higher level business goals, see Building a Microservices Ecosystem with Kafka Streams and ksqlDB Correlation Identifiers can be used as part of Event Collaboration , a pattern in which decentralized Event Processing Applications collaborate to implement a distributed workflow solution. The idea of tagging requests and their related responses exists in many other protocols. For example, an email client connecting over IMAP will send commands prefixed with a unique ID (typically, a001 , a002 , etc.) and the server will respond asynchronously, tagging its responses with the matching ID.","title":"References"},{"location":"event/data-contract/","text":"Data Contract An Event Processing Application can send an Event to another Event Processing Application. It's essential that the communicating applications can understand how to process these shared Events . Problem How can an application send an Event such that a receiving application will know how to process it? Solution Using a Data Contract or Schema, different Event Processing Applications can share Events and understand how to process them without either the sender or receiver to know any details of the other. The Data Contract pattern allows these different applications to cooperate while remaining loosely coupled, and thus insulated from any internal changes they may implement. By implementing a data contract or schema, you can provide the same record consistency guarantees as a RDMS which integrate a schema by default. Implementation By using a schema to model event objects, Kafka clients (e.g., a Kafka producer, a Kafka Streams application, the streaming database ksqlDB ) can understand how to handle events from different applications using the same schema. For example, we can use Avro to describe a schema such as: { \"type\":\"record\", \"namespace\": \"io.confluent.developer.avro\", \"name\":\"Purchase\", \"fields\": [ {\"name\": \"item\", \"type\":\"string\"}, {\"name\": \"amount\", \"type\": \"double\"}, {\"name\": \"customer_id\", \"type\": \"string\"} ] } Additionally, using a central repository like Schema Registry makes it easy for Kafka clients to leverage schemas. Considerations Rather than implementing custom support for a data contract or schemas, you should consider using an industry-accepted framework for schema support, such as the following: Avro Protobuf JSON schema . References Why use Schema Registry","title":"Data Contract"},{"location":"event/data-contract/#data-contract","text":"An Event Processing Application can send an Event to another Event Processing Application. It's essential that the communicating applications can understand how to process these shared Events .","title":"Data Contract"},{"location":"event/data-contract/#problem","text":"How can an application send an Event such that a receiving application will know how to process it?","title":"Problem"},{"location":"event/data-contract/#solution","text":"Using a Data Contract or Schema, different Event Processing Applications can share Events and understand how to process them without either the sender or receiver to know any details of the other. The Data Contract pattern allows these different applications to cooperate while remaining loosely coupled, and thus insulated from any internal changes they may implement. By implementing a data contract or schema, you can provide the same record consistency guarantees as a RDMS which integrate a schema by default.","title":"Solution"},{"location":"event/data-contract/#implementation","text":"By using a schema to model event objects, Kafka clients (e.g., a Kafka producer, a Kafka Streams application, the streaming database ksqlDB ) can understand how to handle events from different applications using the same schema. For example, we can use Avro to describe a schema such as: { \"type\":\"record\", \"namespace\": \"io.confluent.developer.avro\", \"name\":\"Purchase\", \"fields\": [ {\"name\": \"item\", \"type\":\"string\"}, {\"name\": \"amount\", \"type\": \"double\"}, {\"name\": \"customer_id\", \"type\": \"string\"} ] } Additionally, using a central repository like Schema Registry makes it easy for Kafka clients to leverage schemas.","title":"Implementation"},{"location":"event/data-contract/#considerations","text":"Rather than implementing custom support for a data contract or schemas, you should consider using an industry-accepted framework for schema support, such as the following: Avro Protobuf JSON schema .","title":"Considerations"},{"location":"event/data-contract/#references","text":"Why use Schema Registry","title":"References"},{"location":"event/event-deserializer/","text":"Event Deserializer Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data originates from a wide variety of systems and programming languages. The more easily we can access that ocean of data, the richer the analysis we can perform. In an online shopping business, data recorded by the order-processing system and from data user behavior may prove invaluable to the website design department, provided they can actually access it . It's vital to be able to read data from an Event Store regardless of which process and which department put it there originally. To a large degree, the accessibility of data is determined at write time, by our choice of Event Serializer . Still, the story is certainly not complete until we've read the data back out. Problem How can I reconstruct the original event from its representation in the event streaming platform? Solution Use an Event Streaming Platform that integrates well with a schema registry. This makes it easy to encourage (or require) writers to record the event's data description for later use. Having both the event data and its schema readily available makes deserialization easy. While some data formats are reasonably discoverable , in practice it becomes invaluable to have a precise, permanent record of how the data was encoded at the time it was written. This is particularly true if the data format has evolved over time and the Event Stream may contain more than one encoding of semantically-equivalent data. Implementation Confluent\u2019s Schema Registry stores a versioned history of the data's schema in Apache Kafka\u00ae itself. The client libraries can then use this metadata to seamlessly reconstruct the original event data, while we can use the registry API to manually inspect the schemas, or to build libraries for other languages. For example, in the Event Serializer pattern we wrote a stream of fx_trade events. If we want to recall the structure of those events we can ask ksqlDB: DESCRIBE fx_trade; Name : FX_TRADE Field | Type ---------------------------------------- TRADE_ID | BIGINT (key) FROM_CURRENCY | VARCHAR(STRING) TO_CURRENCY | VARCHAR(STRING) PRICE | DECIMAL(10, 5) ---------------------------------------- Or we can query the Schema Registry directly to see the structure in a machine-readable format: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq . { \"subject\": \"fx_trade-value\", \"version\": 1, \"id\": 44, \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"KsqlDataSourceSchema\\\",\\\"namespace\\\":\\\"io.confluent.ksql.avro_schemas\\\",\\\"fields\\\":[{\\\"name\\\":\\\"FROM_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"TO_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"PRICE\\\",\\\"type\\\":[\\\"null\\\",{\\\"type\\\":\\\"bytes\\\",\\\"scale\\\":5,\\\"precision\\\":10,\\\"connect.version\\\":1,\\\"connect.parameters\\\":{\\\"scale\\\":\\\"5\\\",\\\"connect.decimal.precision\\\":\\\"10\\\"},\\\"connect.name\\\":\\\"org.apache.kafka.connect.data.Decimal\\\",\\\"logicalType\\\":\\\"decimal\\\"}],\\\"default\\\":null}],\\\"connect.name\\\":\\\"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\\\"}\" } Unpacking that schema field reveals the Avro specification: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq -rc .schema | jq . { \"type\": \"record\", \"name\": \"KsqlDataSourceSchema\", \"namespace\": \"io.confluent.ksql.avro_schemas\", \"fields\": [ { \"name\": \"FROM_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"TO_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"PRICE\", \"type\": [ \"null\", { \"type\": \"bytes\", \"scale\": 5, \"precision\": 10, \"connect.version\": 1, \"connect.parameters\": { \"scale\": \"5\", \"connect.decimal.precision\": \"10\" }, \"connect.name\": \"org.apache.kafka.connect.data.Decimal\", \"logicalType\": \"decimal\" } ], \"default\": null } ], \"connect.name\": \"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\" } An Avro library can use this schema to deserialize the events seamlessly. And any client libraries that are Schema Registry-aware can automate this lookup, allowing us to forget about encodings entirely and focus on the data. Considerations In addition to Avro, Schema Registry supports Protobuf and JSON Schema. See Event Serializer for a discussion of these formats. While the choice of serialization format is important, it doesn't have to be set in stone. For example, it's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And we always have the option of handling encoding problems directly in code with a Schema-on-Read strategy. References The counterpart of an event deserializer (for reading) is an Event Serializer (for writing). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format, and constrain the individual events to a certain schema within that format. See also: Event Mapper .","title":"Event Deserializer"},{"location":"event/event-deserializer/#event-deserializer","text":"Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data originates from a wide variety of systems and programming languages. The more easily we can access that ocean of data, the richer the analysis we can perform. In an online shopping business, data recorded by the order-processing system and from data user behavior may prove invaluable to the website design department, provided they can actually access it . It's vital to be able to read data from an Event Store regardless of which process and which department put it there originally. To a large degree, the accessibility of data is determined at write time, by our choice of Event Serializer . Still, the story is certainly not complete until we've read the data back out.","title":"Event Deserializer"},{"location":"event/event-deserializer/#problem","text":"How can I reconstruct the original event from its representation in the event streaming platform?","title":"Problem"},{"location":"event/event-deserializer/#solution","text":"Use an Event Streaming Platform that integrates well with a schema registry. This makes it easy to encourage (or require) writers to record the event's data description for later use. Having both the event data and its schema readily available makes deserialization easy. While some data formats are reasonably discoverable , in practice it becomes invaluable to have a precise, permanent record of how the data was encoded at the time it was written. This is particularly true if the data format has evolved over time and the Event Stream may contain more than one encoding of semantically-equivalent data.","title":"Solution"},{"location":"event/event-deserializer/#implementation","text":"Confluent\u2019s Schema Registry stores a versioned history of the data's schema in Apache Kafka\u00ae itself. The client libraries can then use this metadata to seamlessly reconstruct the original event data, while we can use the registry API to manually inspect the schemas, or to build libraries for other languages. For example, in the Event Serializer pattern we wrote a stream of fx_trade events. If we want to recall the structure of those events we can ask ksqlDB: DESCRIBE fx_trade; Name : FX_TRADE Field | Type ---------------------------------------- TRADE_ID | BIGINT (key) FROM_CURRENCY | VARCHAR(STRING) TO_CURRENCY | VARCHAR(STRING) PRICE | DECIMAL(10, 5) ---------------------------------------- Or we can query the Schema Registry directly to see the structure in a machine-readable format: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq . { \"subject\": \"fx_trade-value\", \"version\": 1, \"id\": 44, \"schema\": \"{\\\"type\\\":\\\"record\\\",\\\"name\\\":\\\"KsqlDataSourceSchema\\\",\\\"namespace\\\":\\\"io.confluent.ksql.avro_schemas\\\",\\\"fields\\\":[{\\\"name\\\":\\\"FROM_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"TO_CURRENCY\\\",\\\"type\\\":[\\\"null\\\",\\\"string\\\"],\\\"default\\\":null},{\\\"name\\\":\\\"PRICE\\\",\\\"type\\\":[\\\"null\\\",{\\\"type\\\":\\\"bytes\\\",\\\"scale\\\":5,\\\"precision\\\":10,\\\"connect.version\\\":1,\\\"connect.parameters\\\":{\\\"scale\\\":\\\"5\\\",\\\"connect.decimal.precision\\\":\\\"10\\\"},\\\"connect.name\\\":\\\"org.apache.kafka.connect.data.Decimal\\\",\\\"logicalType\\\":\\\"decimal\\\"}],\\\"default\\\":null}],\\\"connect.name\\\":\\\"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\\\"}\" } Unpacking that schema field reveals the Avro specification: curl http://localhost:8081/subjects/fx_trade-value/versions/latest | jq -rc .schema | jq . { \"type\": \"record\", \"name\": \"KsqlDataSourceSchema\", \"namespace\": \"io.confluent.ksql.avro_schemas\", \"fields\": [ { \"name\": \"FROM_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"TO_CURRENCY\", \"type\": [ \"null\", \"string\" ], \"default\": null }, { \"name\": \"PRICE\", \"type\": [ \"null\", { \"type\": \"bytes\", \"scale\": 5, \"precision\": 10, \"connect.version\": 1, \"connect.parameters\": { \"scale\": \"5\", \"connect.decimal.precision\": \"10\" }, \"connect.name\": \"org.apache.kafka.connect.data.Decimal\", \"logicalType\": \"decimal\" } ], \"default\": null } ], \"connect.name\": \"io.confluent.ksql.avro_schemas.KsqlDataSourceSchema\" } An Avro library can use this schema to deserialize the events seamlessly. And any client libraries that are Schema Registry-aware can automate this lookup, allowing us to forget about encodings entirely and focus on the data.","title":"Implementation"},{"location":"event/event-deserializer/#considerations","text":"In addition to Avro, Schema Registry supports Protobuf and JSON Schema. See Event Serializer for a discussion of these formats. While the choice of serialization format is important, it doesn't have to be set in stone. For example, it's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And we always have the option of handling encoding problems directly in code with a Schema-on-Read strategy.","title":"Considerations"},{"location":"event/event-deserializer/#references","text":"The counterpart of an event deserializer (for reading) is an Event Serializer (for writing). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format, and constrain the individual events to a certain schema within that format. See also: Event Mapper .","title":"References"},{"location":"event/event-envelope/","text":"Event Envelope Event Streaming Platforms allow many different types of applications to work together. Event Envelopes provide a standard set of well-known fields across all Event s sent through the Event Streaming Applications . The envelope is independent of the underlying event format and often references attributes such as the encryption type, schema, key, serialization format. Envelopes are analogous to protocol headers in networking (TCP-IP etc.) Problem How to convey information to all participants in an Event Streaming Platform independently of the event payload, e.g. how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event? Solution Use an Event Envelope to wrap the event data using a standard format agreed by all participants of the Event Streaming Platform or more broadly. Cloud Events \u2013which standardize access to ID, Schema, Key, and other common event attributes\u2013are an industry-standard example of the Event Envelope pattern. Example Implementation Using the basic Java consumers and producers, a helper function could be used to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform . static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; } References This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf How to Choose Between Strict and Dynamic Schemas See Cloud Events for a specification on describing event header information in a common way.","title":"Event Envelope"},{"location":"event/event-envelope/#event-envelope","text":"Event Streaming Platforms allow many different types of applications to work together. Event Envelopes provide a standard set of well-known fields across all Event s sent through the Event Streaming Applications . The envelope is independent of the underlying event format and often references attributes such as the encryption type, schema, key, serialization format. Envelopes are analogous to protocol headers in networking (TCP-IP etc.)","title":"Event Envelope"},{"location":"event/event-envelope/#problem","text":"How to convey information to all participants in an Event Streaming Platform independently of the event payload, e.g. how to decrypt an Event , what schema is used, or what ID defines the uniqueness of the event?","title":"Problem"},{"location":"event/event-envelope/#solution","text":"Use an Event Envelope to wrap the event data using a standard format agreed by all participants of the Event Streaming Platform or more broadly. Cloud Events \u2013which standardize access to ID, Schema, Key, and other common event attributes\u2013are an industry-standard example of the Event Envelope pattern.","title":"Solution"},{"location":"event/event-envelope/#example-implementation","text":"Using the basic Java consumers and producers, a helper function could be used to wrap an application's immutable payload into an envelope which conforms to the expected format of the Event Streaming Platform . static <T> Envelope<T> wrap(T payload, Iterable<Header> headers) { return new Envelope(serializer(payload), headers); } static <T> T unwrap(Envelope<T> envelope) { return envelope.payload; }","title":"Example Implementation"},{"location":"event/event-envelope/#references","text":"This pattern is derived from Envelope Wrapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf How to Choose Between Strict and Dynamic Schemas See Cloud Events for a specification on describing event header information in a common way.","title":"References"},{"location":"event/event-serializer/","text":"Event Serializer Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data demands a broad audience - the more accessible our data is, the more departments in our organization can find use for it. In a successful system, data gathered by the sales department in one year may prove invaluable to the marketing department a few years later, provided they can actually access it . For maximum utility and longevity, data should be written in a way that doesn't obscure it from future readers and writers. The data is more important than today's technology choices. How does this affect an event-based system? Are there any special concerns for this kind of architecture, or will the programming language's serialization tools suffice? Problem How can I convert an event into a format understood by the event streaming platform and applications that use it? Solution Use a language-agnostic serialization format. The ideal format would be self-documenting, space-efficient, and designed to support some degree of backwards and forwards -compatibility. We recommend Avro . (See \"Considerations\".) An optional, recommended step is to register the serialization details with a schema registry. A registry provides a reliable, machine-readable reference point for Event Deserializers and Schema Validators , making event consumption vastly simpler. Implementation For example, we can use Avro to define a structure for Foreign Exchange trade deals as: {\"namespace\": \"io.confluent.developer\", \"type\": \"record\", \"name\": \"FxTrade\", \"fields\": [ {\"name\": \"trade_id\", \"type\": \"long\"}, {\"name\": \"from_currency\", \"type\": \"string\"}, {\"name\": \"to_currency\", \"type\": \"string\"}, {\"name\": \"price\", \"type\": \"bytes\", \"logicalType\": \"decimal\", \"precision\": 10, \"scale\": 5} ] } ...and then use our language's Avro libraries to take care of serialization for us: FxTrade fxTrade = new FxTrade( ... ); ProducerRecord<long, FxTrade> producerRecord = new ProducerRecord<>(\"fx_trade\", fxTrade.getTradeId(), fxTrade); producer.send(producerRecord); Alternatively, with the streaming database ksqlDB , we can define an Event Stream in a way that enforces that format and records the Avro definition using Confluent\u2019s Schema Registry : CREATE OR REPLACE STREAM fx_trade ( trade_id BIGINT KEY, from_currency VARCHAR(3), to_currency VARCHAR(3), price DECIMAL(10,5) ) WITH ( KAFKA_TOPIC = 'fx_trade', KEY_FORMAT = 'avro', VALUE_FORMAT = 'avro', PARTITIONS = 3 ); With this setup, both serialization and deserialization of data is performed automatically by ksqlDB behind the scenes. Considerations Event Streaming Platforms are typically serialization-agnostic, accepting any serialized data from human-readable text to raw bytes. However, by constraining ourselves to more widely-accepted, structured data formats, we can open the door to easier collaboration with other projects and programming languages. Finding a \"universal\" serialization format isn't a new problem, or one unique to event streaming. As such we have a number of technology-agnostic solutions readily available. To briefly cover a few: JSON . Arguably the most successful serialization format in the history of computing. JSON is a text-based format that's easy to read, write and discover 1 , as evidenced by the number of languages and projects that produce and consume JSON data across the world with minimal collaboration. Protocol buffers . Backed by Google and supported by a wide variety of languages, Protobuf is a binary format that sacrifices the discoverability of JSON for a much more compact representation that uses less disk space and network bandwidth. Protobuf is also a strongly-typed format, allowing enforcement of a particular data schema from writers, and describing the structure of that data to readers. Avro . A binary format similar to Protocol Buffers, Avro's design has focuses on supporting the evolution of schemas, allowing the data format to change over time while minimizing the impact to future readers and writer. While the choice of serialization format is important, it doesn't have to be set in stone. It's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And as a fallback we can push the problem to the consumer's code with a Schema-on-Read strategy. References The counterpart of an event serializer (for writing) is an Event Deserializer (for reading). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format and constrain the individual events to a certain schema within that format. See also: Event Mapper . Footnotes 1 Older programmers will tell tales of the less-discoverable serialization formats used by banks in the 80s, in which deciphering the meaning of a message meant wading through a thick, ring-bound printout of the data specification which explained the meaning of \"Field 78\" by cross-referencing \"Encoding Subformat 22\".","title":"Event Serializer"},{"location":"event/event-serializer/#event-serializer","text":"Data has a long lifecycle, often outliving the programs that originally gathered and stored it. And data demands a broad audience - the more accessible our data is, the more departments in our organization can find use for it. In a successful system, data gathered by the sales department in one year may prove invaluable to the marketing department a few years later, provided they can actually access it . For maximum utility and longevity, data should be written in a way that doesn't obscure it from future readers and writers. The data is more important than today's technology choices. How does this affect an event-based system? Are there any special concerns for this kind of architecture, or will the programming language's serialization tools suffice?","title":"Event Serializer"},{"location":"event/event-serializer/#problem","text":"How can I convert an event into a format understood by the event streaming platform and applications that use it?","title":"Problem"},{"location":"event/event-serializer/#solution","text":"Use a language-agnostic serialization format. The ideal format would be self-documenting, space-efficient, and designed to support some degree of backwards and forwards -compatibility. We recommend Avro . (See \"Considerations\".) An optional, recommended step is to register the serialization details with a schema registry. A registry provides a reliable, machine-readable reference point for Event Deserializers and Schema Validators , making event consumption vastly simpler.","title":"Solution"},{"location":"event/event-serializer/#implementation","text":"For example, we can use Avro to define a structure for Foreign Exchange trade deals as: {\"namespace\": \"io.confluent.developer\", \"type\": \"record\", \"name\": \"FxTrade\", \"fields\": [ {\"name\": \"trade_id\", \"type\": \"long\"}, {\"name\": \"from_currency\", \"type\": \"string\"}, {\"name\": \"to_currency\", \"type\": \"string\"}, {\"name\": \"price\", \"type\": \"bytes\", \"logicalType\": \"decimal\", \"precision\": 10, \"scale\": 5} ] } ...and then use our language's Avro libraries to take care of serialization for us: FxTrade fxTrade = new FxTrade( ... ); ProducerRecord<long, FxTrade> producerRecord = new ProducerRecord<>(\"fx_trade\", fxTrade.getTradeId(), fxTrade); producer.send(producerRecord); Alternatively, with the streaming database ksqlDB , we can define an Event Stream in a way that enforces that format and records the Avro definition using Confluent\u2019s Schema Registry : CREATE OR REPLACE STREAM fx_trade ( trade_id BIGINT KEY, from_currency VARCHAR(3), to_currency VARCHAR(3), price DECIMAL(10,5) ) WITH ( KAFKA_TOPIC = 'fx_trade', KEY_FORMAT = 'avro', VALUE_FORMAT = 'avro', PARTITIONS = 3 ); With this setup, both serialization and deserialization of data is performed automatically by ksqlDB behind the scenes.","title":"Implementation"},{"location":"event/event-serializer/#considerations","text":"Event Streaming Platforms are typically serialization-agnostic, accepting any serialized data from human-readable text to raw bytes. However, by constraining ourselves to more widely-accepted, structured data formats, we can open the door to easier collaboration with other projects and programming languages. Finding a \"universal\" serialization format isn't a new problem, or one unique to event streaming. As such we have a number of technology-agnostic solutions readily available. To briefly cover a few: JSON . Arguably the most successful serialization format in the history of computing. JSON is a text-based format that's easy to read, write and discover 1 , as evidenced by the number of languages and projects that produce and consume JSON data across the world with minimal collaboration. Protocol buffers . Backed by Google and supported by a wide variety of languages, Protobuf is a binary format that sacrifices the discoverability of JSON for a much more compact representation that uses less disk space and network bandwidth. Protobuf is also a strongly-typed format, allowing enforcement of a particular data schema from writers, and describing the structure of that data to readers. Avro . A binary format similar to Protocol Buffers, Avro's design has focuses on supporting the evolution of schemas, allowing the data format to change over time while minimizing the impact to future readers and writer. While the choice of serialization format is important, it doesn't have to be set in stone. It's straightforward to translate between supported formats with ksqlDB . For more complex scenarios, we have several strategies for managing schema migration: Schema Compatibility discusses the kinds of \"safe\" schema changes that Avro is designed to handle transparently. Event Translators can convert between different encodings to aid consumption by different systems. Schema Evolution discusses splitting and joining streams to simplify serving consumers that can only handle certain versions of the event's schema. An Event Standardizer can reformat disparate data encodings into a single unified format. And as a fallback we can push the problem to the consumer's code with a Schema-on-Read strategy.","title":"Considerations"},{"location":"event/event-serializer/#references","text":"The counterpart of an event serializer (for writing) is an Event Deserializer (for reading). Serializers and deserializers are closely related to Data Contracts , in which we want to adhere to a specific serialization format and constrain the individual events to a certain schema within that format. See also: Event Mapper .","title":"References"},{"location":"event/event-serializer/#footnotes","text":"1 Older programmers will tell tales of the less-discoverable serialization formats used by banks in the 80s, in which deciphering the meaning of a message meant wading through a thick, ring-bound printout of the data specification which explained the meaning of \"Field 78\" by cross-referencing \"Encoding Subformat 22\".","title":"Footnotes"},{"location":"event/event-standardizer/","text":"Event Standardizer In most businesses, a variety of traditional and Event Processing Applications need to exchange Events across Event Streams . Downstream Event Processing Applications will require standardized data formats in order to properly process these events. However, the reality of having many sources for these events often results in the lack of such standards or in different interpretations of the same standard. Problem How do we process Events that are semantically equivalent, but arrive in different formats? Solution Source all the input Event Streams into an Event Standardizer that passes events to a specialized Event Translator , which in turn converts the event to a common format understood by the downstream Event Processors . Implementation As an example, we can use the Kafka Streams client library of Apache Kafka\u00ae to build an Event Processing Application that reads from multiple input Event Streams and then \"maps\" the values to a new type. Specifically, we use the mapValues function to translate each event type into the standard type expected on the output Event Stream . SpecificAvroSerde<SpecificRecord> inputValueSerde = constructSerde(); builder .stream(List.of(\"inputStreamA\", \"inputStreamB\", \"inputStreamC\"), Consumed.with(Serdes.String(), inputValueSerde)) .mapValues((eventKey, eventValue) -> { if (eventValue.getClass() == TypeA.class) return typeATranslator.normalize(eventValue); else if (eventValue.getClass() == TypeB.class) return typeBTranslator.normalize(eventValue); else if (eventValue.getClass() == TypeC.class) return typeCTranslator.normalize(eventValue); else { // exception or dead letter stream } }) .to(\"my-standardized-output-stream\", Produced.with(Serdes.String(), outputSerdeType)); Considerations When possible, diverging data formats should be normalized \"at the source\". This data governance is often called \"Schema on Write\", and may be implemented with the Schema Validator pattern. Enforcing schema validation prior to writing an event to the Event Stream , allows consuming applications to delegate their data format validation logic to the schema validation layer. Error handling should be considered in the design of the standardizer. Categories of errors may include serialization failures, unexpected or missing values, and unknown types (as in the example above). Dead Letter Stream is one pattern commonly used to handle exceptional events in our Event Processing Application . References See also Stream Merger for unifying related streams without changing their format. This pattern is derived from Normalizer in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Kafka Streams map stateless transformation documentation Error Handling Patterns for Apache Kafka Applications is a blog post with details on strategies and patterns for error handling in Event Processing Applications","title":"Event Standardizer"},{"location":"event/event-standardizer/#event-standardizer","text":"In most businesses, a variety of traditional and Event Processing Applications need to exchange Events across Event Streams . Downstream Event Processing Applications will require standardized data formats in order to properly process these events. However, the reality of having many sources for these events often results in the lack of such standards or in different interpretations of the same standard.","title":"Event Standardizer"},{"location":"event/event-standardizer/#problem","text":"How do we process Events that are semantically equivalent, but arrive in different formats?","title":"Problem"},{"location":"event/event-standardizer/#solution","text":"Source all the input Event Streams into an Event Standardizer that passes events to a specialized Event Translator , which in turn converts the event to a common format understood by the downstream Event Processors .","title":"Solution"},{"location":"event/event-standardizer/#implementation","text":"As an example, we can use the Kafka Streams client library of Apache Kafka\u00ae to build an Event Processing Application that reads from multiple input Event Streams and then \"maps\" the values to a new type. Specifically, we use the mapValues function to translate each event type into the standard type expected on the output Event Stream . SpecificAvroSerde<SpecificRecord> inputValueSerde = constructSerde(); builder .stream(List.of(\"inputStreamA\", \"inputStreamB\", \"inputStreamC\"), Consumed.with(Serdes.String(), inputValueSerde)) .mapValues((eventKey, eventValue) -> { if (eventValue.getClass() == TypeA.class) return typeATranslator.normalize(eventValue); else if (eventValue.getClass() == TypeB.class) return typeBTranslator.normalize(eventValue); else if (eventValue.getClass() == TypeC.class) return typeCTranslator.normalize(eventValue); else { // exception or dead letter stream } }) .to(\"my-standardized-output-stream\", Produced.with(Serdes.String(), outputSerdeType));","title":"Implementation"},{"location":"event/event-standardizer/#considerations","text":"When possible, diverging data formats should be normalized \"at the source\". This data governance is often called \"Schema on Write\", and may be implemented with the Schema Validator pattern. Enforcing schema validation prior to writing an event to the Event Stream , allows consuming applications to delegate their data format validation logic to the schema validation layer. Error handling should be considered in the design of the standardizer. Categories of errors may include serialization failures, unexpected or missing values, and unknown types (as in the example above). Dead Letter Stream is one pattern commonly used to handle exceptional events in our Event Processing Application .","title":"Considerations"},{"location":"event/event-standardizer/#references","text":"See also Stream Merger for unifying related streams without changing their format. This pattern is derived from Normalizer in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Kafka Streams map stateless transformation documentation Error Handling Patterns for Apache Kafka Applications is a blog post with details on strategies and patterns for error handling in Event Processing Applications","title":"References"},{"location":"event/event/","text":"Event Events represent facts and can help facilitate decoupled applications, services, and systems exchanging data across an Event Streaming Platform . Problem How do I represent a fact about something that has happened? Solution An event represents an immutable fact about something that happened. Examples of Events might be: orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains at least one or more data fields that describe the fact, as well as a timestamp that denotes when this Event was created by its Event Source . The Event may also contain various metadata about itself, such as its source of origin (e.g., the application or cloud services that created the event) and storage-level information (e.g., its position in the event stream). Implementation With Apache Kafka\u00ae, Events are referred to as records . Records are modeled as a key / value pair with a timestamp and optional metadata (called headers). The value of the record usually contains the representation of an application domain object or some form of raw message value, like the output of a sensor or other metric reading. The record key is useful for a few reasons, but critically they are used by Kafka to determine how the data is partitioned within a stream a.k.a. topic (see Partitioned Parallelism for more details on partitioning). Keys are often best thought of as a categorization of the Event, like the identity of a particular user or connected device. Headers are a place for record metadata which can help describe the Event data itself, and are themselves modeled as a map of keys and values. Record keys, values, and headers are opaque data types, meaning that Kafka, by deliberate design to achieve its high scalability and performance, does not define a type interface for them: they are read, stored, and written by Kafka's server-side brokers as raw arrays of bytes. Instead, it is the responsibility of Kafka's client applications like the streaming database ksqlDB or microservices implemented with the client libraries such as Kafka Streams or the Kafka Go client to perform the serialization and deserialization of the data within the record keys, values, and headers. When using the Java client library, events are created using the ProducerRecord type and sent to Kafka using the KafkaProducer . In this example, we have set the key and value types as strings and added a header: ProducerRecord<String, String> newEvent = new ProducerRecord<>( paymentEvent.getCustomerId().toString() /* key */, paymentEvent.toString() /* value */); producerRecord.headers() .add(\"origin-cloud\", \"aws\".getBytes(StandardCharsets.UTF_8)); producer.send(producerRecord); Considerations To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event Schemas are commonly defined in Avro , Protobuf , or JSON schema . For cloud-based architectures, evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (think: instructions, actions) that an Event Processor reading the events should carry out. See the Command Event for details. References This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Apache Kafka 101: Introduction provides a primer on \"What is Kafka, and how does it work?\" including information on core concepts like Events","title":"Event"},{"location":"event/event/#event","text":"Events represent facts and can help facilitate decoupled applications, services, and systems exchanging data across an Event Streaming Platform .","title":"Event"},{"location":"event/event/#problem","text":"How do I represent a fact about something that has happened?","title":"Problem"},{"location":"event/event/#solution","text":"An event represents an immutable fact about something that happened. Examples of Events might be: orders, payments, activities, or measurements. Events are produced to, stored in, and consumed from an Event Stream . An Event typically contains at least one or more data fields that describe the fact, as well as a timestamp that denotes when this Event was created by its Event Source . The Event may also contain various metadata about itself, such as its source of origin (e.g., the application or cloud services that created the event) and storage-level information (e.g., its position in the event stream).","title":"Solution"},{"location":"event/event/#implementation","text":"With Apache Kafka\u00ae, Events are referred to as records . Records are modeled as a key / value pair with a timestamp and optional metadata (called headers). The value of the record usually contains the representation of an application domain object or some form of raw message value, like the output of a sensor or other metric reading. The record key is useful for a few reasons, but critically they are used by Kafka to determine how the data is partitioned within a stream a.k.a. topic (see Partitioned Parallelism for more details on partitioning). Keys are often best thought of as a categorization of the Event, like the identity of a particular user or connected device. Headers are a place for record metadata which can help describe the Event data itself, and are themselves modeled as a map of keys and values. Record keys, values, and headers are opaque data types, meaning that Kafka, by deliberate design to achieve its high scalability and performance, does not define a type interface for them: they are read, stored, and written by Kafka's server-side brokers as raw arrays of bytes. Instead, it is the responsibility of Kafka's client applications like the streaming database ksqlDB or microservices implemented with the client libraries such as Kafka Streams or the Kafka Go client to perform the serialization and deserialization of the data within the record keys, values, and headers. When using the Java client library, events are created using the ProducerRecord type and sent to Kafka using the KafkaProducer . In this example, we have set the key and value types as strings and added a header: ProducerRecord<String, String> newEvent = new ProducerRecord<>( paymentEvent.getCustomerId().toString() /* key */, paymentEvent.toString() /* value */); producerRecord.headers() .add(\"origin-cloud\", \"aws\".getBytes(StandardCharsets.UTF_8)); producer.send(producerRecord);","title":"Implementation"},{"location":"event/event/#considerations","text":"To ensure that Events from an Event Source can be read correctly by an Event Processor , they are often created in reference to an Event schema. Event Schemas are commonly defined in Avro , Protobuf , or JSON schema . For cloud-based architectures, evaluate the use of CloudEvents . CloudEvents provide a standardized Event Envelope that wraps an event, making common event properties such as source, type, time, and ID universally accessible, regardless of how the event itself was serialized. In certain scenarios, Events may represent commands (think: instructions, actions) that an Event Processor reading the events should carry out. See the Command Event for details.","title":"Considerations"},{"location":"event/event/#references","text":"This pattern is derived in part from Message , Event Message , and Document Message in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Apache Kafka 101: Introduction provides a primer on \"What is Kafka, and how does it work?\" including information on core concepts like Events","title":"References"},{"location":"event/schema-on-read/","text":"Schema-on-Read Schema-on-Read leaves the validation of a schema for an Event to the reader. There are several use cases for this pattern, all of which provide a lot of flexibility to Event Processors and Event Sinks : When there are different versions of the same schema type, and the reader wants to choose which version to apply to a given event. When the sequencing matters between events of different types and all those different event types are put into a single stream. For example, consider a banking use case where first a customer opens an account, then gets approval, then makes a deposit, etc. Put these heterogeneous Event types into the same stream, allowing the Event Streaming Platform maintain event ordering and allowing any consuming Event Processor and Event Sinks deserialize the events as needed. When unstructured data is written into an Event Stream , and the reader then applies whichever schema it wants. Problem How can an Event Processor apply a schema on the data it is reading from an Event Streaming Platform ? Solution The Schema-on-Read approach approach enables each reader to decide on how to read data, and which version of which schema to apply to every Event that it reads. To make schema management easier, the design can use a centralized repository that can store multiple versions of different schemas, and then the client applications then choose which schema to apply to events at runtime. Implementation With Confluent Schema Registry , all the schemas are managed in a centralized repository. In addition to just storing the schema information, Schema Registry can also be configured to check and/or enforce that schema changes are compatible with previous versions. For example, if a business started with a schema definition for an event that has 2 fields, but then the business needs evolved to now warrant an optional 3rd field, then that schema evolves with it. Schema Registry will ensure that the new schema is compatible with the old schema. In this particular case, for backward compatibility, the 3rd field status can be defined with a default value which will be used for the missing field when deserializing the data encoded with the old schema. This ensures that all events in a given stream follow Schema Compatibility rules, and the applications can continue to process those events. { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"user\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"address\", \"type\": \"string\"}, {\"name\": \"status\", \"type\": \"string\", \"default\": \"waiting\"} ] } In another example, if the use case warrants writing different event types into a single stream, with Apache Kafka you could set the \"subject naming strategy\" to register schemas against the record type, instead of the Kafka topic. Schema Registry will then let Schema Evolution and Schema Compatibility checking to happen within the scope of each event type instead of the topic. The consumer application can read schema versions assigned to the data type, and in the case where there are different data types in any given stream, the application can cast each event to the appropriate type at processing time and follow the appropriate code path: if (Account.equals(record.getClass()) { ... } else if (Approval.equals(record.getClass())) { ... } else if (Transaction.equals(record.getClass())) { ... } else { ... } Considerations The schema's subject naming strategy can be set to record type (instead of Kafka topic) in one of two ways. The less restrictive is RecordNameStrategy , which sets the namespace to the record, regardless of which topic the event is written to. The more restrictive is TopicRecordNameStrategy , which sets the namespace to both, the record and the topic the event is written to. References Confluent blog Should You Put Several Event Types in the Same Kafka Topic? Confluent Schema Registry","title":"Schema-on-Read"},{"location":"event/schema-on-read/#schema-on-read","text":"Schema-on-Read leaves the validation of a schema for an Event to the reader. There are several use cases for this pattern, all of which provide a lot of flexibility to Event Processors and Event Sinks : When there are different versions of the same schema type, and the reader wants to choose which version to apply to a given event. When the sequencing matters between events of different types and all those different event types are put into a single stream. For example, consider a banking use case where first a customer opens an account, then gets approval, then makes a deposit, etc. Put these heterogeneous Event types into the same stream, allowing the Event Streaming Platform maintain event ordering and allowing any consuming Event Processor and Event Sinks deserialize the events as needed. When unstructured data is written into an Event Stream , and the reader then applies whichever schema it wants.","title":"Schema-on-Read"},{"location":"event/schema-on-read/#problem","text":"How can an Event Processor apply a schema on the data it is reading from an Event Streaming Platform ?","title":"Problem"},{"location":"event/schema-on-read/#solution","text":"The Schema-on-Read approach approach enables each reader to decide on how to read data, and which version of which schema to apply to every Event that it reads. To make schema management easier, the design can use a centralized repository that can store multiple versions of different schemas, and then the client applications then choose which schema to apply to events at runtime.","title":"Solution"},{"location":"event/schema-on-read/#implementation","text":"With Confluent Schema Registry , all the schemas are managed in a centralized repository. In addition to just storing the schema information, Schema Registry can also be configured to check and/or enforce that schema changes are compatible with previous versions. For example, if a business started with a schema definition for an event that has 2 fields, but then the business needs evolved to now warrant an optional 3rd field, then that schema evolves with it. Schema Registry will ensure that the new schema is compatible with the old schema. In this particular case, for backward compatibility, the 3rd field status can be defined with a default value which will be used for the missing field when deserializing the data encoded with the old schema. This ensures that all events in a given stream follow Schema Compatibility rules, and the applications can continue to process those events. { \"namespace\": \"example.avro\", \"type\": \"record\", \"name\": \"user\", \"fields\": [ {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"address\", \"type\": \"string\"}, {\"name\": \"status\", \"type\": \"string\", \"default\": \"waiting\"} ] } In another example, if the use case warrants writing different event types into a single stream, with Apache Kafka you could set the \"subject naming strategy\" to register schemas against the record type, instead of the Kafka topic. Schema Registry will then let Schema Evolution and Schema Compatibility checking to happen within the scope of each event type instead of the topic. The consumer application can read schema versions assigned to the data type, and in the case where there are different data types in any given stream, the application can cast each event to the appropriate type at processing time and follow the appropriate code path: if (Account.equals(record.getClass()) { ... } else if (Approval.equals(record.getClass())) { ... } else if (Transaction.equals(record.getClass())) { ... } else { ... }","title":"Implementation"},{"location":"event/schema-on-read/#considerations","text":"The schema's subject naming strategy can be set to record type (instead of Kafka topic) in one of two ways. The less restrictive is RecordNameStrategy , which sets the namespace to the record, regardless of which topic the event is written to. The more restrictive is TopicRecordNameStrategy , which sets the namespace to both, the record and the topic the event is written to.","title":"Considerations"},{"location":"event/schema-on-read/#references","text":"Confluent blog Should You Put Several Event Types in the Same Kafka Topic? Confluent Schema Registry","title":"References"},{"location":"event-processing/claim-check/","text":"Claim Check Sometimes compression can reduce message size but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc. Problem How can we handle use cases where the Event payload is too large or too expensive to move through the Event Streaming Platform ? Solution Instead of storing the entire event in the event streaming platform, store the event payload in a persistent external store that can be shared between producers and consumers. The producer can write the reference address into the event streaming platform, and downstream clients use the address to retrieve the event from the external store and then process it as needed. Implementation The event stored in Kafka contains only a reference to the object in the external store. This can be a full URI string, an abstract data type (e.g., Java object) with separate fields for bucket name and filename, or whatever fields are required to identify the object. Optionally, the event may contain additional data fields to better describe the object (e.g., metadata such as who created the object). The following example uses Kafka's Java producer client. Here, we keep things simple as the event's value stores no information other than the reference (URI) to its respective object in external storage. // Write object to external storage storageClient.putObject(bucketName, objectName, object); // Write URI to Kafka URI eventValue = new URI(bucketName, objectName); producer.send(new ProducerRecord<String, URI>(topic, eventKey, eventValue)); Considerations The Event Source is responsible for ensuring that the data is properly stored in the external store, such that the reference passed within the Event is valid. Since the producer should be doing this atomically, take into consideration the same issues as mentioned in Database Write Aside . Also, if a Compacted Event Stream is used for storing the \"reference\" events (e.g., topic compaction in the case of Kafka), then the compaction will remove just the event with the reference. However, it will not remove the referenced (large) object itself from the external store, so that object needs a different expiry mechanism. References This pattern is similar in idea to Claim Check in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf An alternative approach to handling large messages is Event Chunking","title":"Claim Check"},{"location":"event-processing/claim-check/#claim-check","text":"Sometimes compression can reduce message size but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc.","title":"Claim Check"},{"location":"event-processing/claim-check/#problem","text":"How can we handle use cases where the Event payload is too large or too expensive to move through the Event Streaming Platform ?","title":"Problem"},{"location":"event-processing/claim-check/#solution","text":"Instead of storing the entire event in the event streaming platform, store the event payload in a persistent external store that can be shared between producers and consumers. The producer can write the reference address into the event streaming platform, and downstream clients use the address to retrieve the event from the external store and then process it as needed.","title":"Solution"},{"location":"event-processing/claim-check/#implementation","text":"The event stored in Kafka contains only a reference to the object in the external store. This can be a full URI string, an abstract data type (e.g., Java object) with separate fields for bucket name and filename, or whatever fields are required to identify the object. Optionally, the event may contain additional data fields to better describe the object (e.g., metadata such as who created the object). The following example uses Kafka's Java producer client. Here, we keep things simple as the event's value stores no information other than the reference (URI) to its respective object in external storage. // Write object to external storage storageClient.putObject(bucketName, objectName, object); // Write URI to Kafka URI eventValue = new URI(bucketName, objectName); producer.send(new ProducerRecord<String, URI>(topic, eventKey, eventValue));","title":"Implementation"},{"location":"event-processing/claim-check/#considerations","text":"The Event Source is responsible for ensuring that the data is properly stored in the external store, such that the reference passed within the Event is valid. Since the producer should be doing this atomically, take into consideration the same issues as mentioned in Database Write Aside . Also, if a Compacted Event Stream is used for storing the \"reference\" events (e.g., topic compaction in the case of Kafka), then the compaction will remove just the event with the reference. However, it will not remove the referenced (large) object itself from the external store, so that object needs a different expiry mechanism.","title":"Considerations"},{"location":"event-processing/claim-check/#references","text":"This pattern is similar in idea to Claim Check in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf An alternative approach to handling large messages is Event Chunking","title":"References"},{"location":"event-processing/dead-letter-stream/","text":"Dead Letter Stream Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios. Problem How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read? Solution When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed. Implementation Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg) Considerations What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations. References This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#dead-letter-stream","text":"Event Processing Applications may encounter invalid data as they operate over the infinite stream of events. Errors may include invalid data formats, nonsensical, missing or corrupt values, technical failures, or other unexpected scenarios.","title":"Dead Letter Stream"},{"location":"event-processing/dead-letter-stream/#problem","text":"How can an event processing application handle processing failures without terminating, or becoming stuck, when a message cannot be read?","title":"Problem"},{"location":"event-processing/dead-letter-stream/#solution","text":"When the event processing application cannot process an event for an unrecoverable reason, the problematic event is published to a \u201cdead letter stream\u201d. This stream stores the event allowing it to be logged, reprocessed later, or otherwise acted upon. Additional contextual information can be provided in the \"dead letter event\" to ease fault resolution later, such as details on why its processing failed.","title":"Solution"},{"location":"event-processing/dead-letter-stream/#implementation","text":"Java Basic Kafka Consumer while (keepConsuming) { try { final ConsumerRecords<K, V> records = consumer.poll(Duration.ofSeconds(1)); try { eventProcessor.process(records); } catch (Exception ex) { deadEventReporter.report(/*Error Details*/); } } catch (SerializationException se) { deadEventReporter.report(/*Error Details*/); } } Python Basic Kafka Consumer while True: try: event = consumer.poll(1000) except SerializerError as e: deadEventReporter.report(e) break if msg.error(): deadEventReporter.report(msg.error()) continue if msg is None: continue eventProcessor.process(msg)","title":"Implementation"},{"location":"event-processing/dead-letter-stream/#considerations","text":"What should real-world applications do with the events in the dead letter stream? Reprocessing events automatically will often lead to reorderings and hence the potential for corruption to downstream systems if the stream contains events that represent changing states of the same underlying entity, such as orders being booked, processed, or shipped. Manual reprocessing can be useful, but is often viewed more as an error log in many real-world implementations.","title":"Considerations"},{"location":"event-processing/dead-letter-stream/#references","text":"This pattern is derived from Dead Letter Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf Confluent\u2019s Schema Registry provides data governance capabilities including \u201cschema on write\u201d enforcement which can help insulate downstream consumers from unexpected event formats. Kafka Streams provides the ability to register a customer Serde to handle corrupt records allowing for creation of Dead Event data which could be published to a Dead Event Stream. See this Confluent Kafka Streams FAQ for details. ksqlDB emits a stream of records, called the Processing Log , which is analogous to a Dead Letter Stream. The Processing Log contains structured events including processing errors and additional debugging details for ksqlDB applications. See this Kafka Tutorial for more information.","title":"References"},{"location":"event-processing/event-chunking/","text":"Event Chunking Sometimes compression can reduce message size, but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc. Problem How do I handle use cases where the event payload is too large to move through the event streaming platform as a single event? Solution Instead of storing the entire event as a single event in the event streaming platform, break it into chunks (an approach called \"chunking\") so that the large event is sent across as multiple smaller events. The producer can do the chunking when writing events into the event streaming platform. Downstream clients consume the chunks and, when all the smaller chunks have been received, recombine (\"unchunk\") them to restore the original event. Implementation Use metadata to track each chunk so that they can be associated to their respective parent event: Association between any given chunk and its parent event The chunk\u2019s position in the parent event The total number of chunks of the parent event Considerations Chunking places additional burden on client applications. First, implementing the chunking and unchunking logic requires more application development. Second, the consumer application needs to be able to cache the chunks as it waits to receive all the smaller chunks that comprise the original event. This, in turn, can have implications on memory fragmentation and longer garbage collection (GC). Mitigating this depends on the programming language: in Java, for example, the JVM heap size and GC can be tuned. Client applications that are not aware of the protocol used for chunking events may not be able to reconstruct the original event accurately. References To handle large events, an alternative approach that may be preferred is Claim Check","title":"Event Chunking"},{"location":"event-processing/event-chunking/#event-chunking","text":"Sometimes compression can reduce message size, but there are various use cases that entail large message payloads where compression may not be enough. Often these use cases are related to image, video, or audio processing: image recognition, video analytics, audio analytics, etc.","title":"Event Chunking"},{"location":"event-processing/event-chunking/#problem","text":"How do I handle use cases where the event payload is too large to move through the event streaming platform as a single event?","title":"Problem"},{"location":"event-processing/event-chunking/#solution","text":"Instead of storing the entire event as a single event in the event streaming platform, break it into chunks (an approach called \"chunking\") so that the large event is sent across as multiple smaller events. The producer can do the chunking when writing events into the event streaming platform. Downstream clients consume the chunks and, when all the smaller chunks have been received, recombine (\"unchunk\") them to restore the original event.","title":"Solution"},{"location":"event-processing/event-chunking/#implementation","text":"Use metadata to track each chunk so that they can be associated to their respective parent event: Association between any given chunk and its parent event The chunk\u2019s position in the parent event The total number of chunks of the parent event","title":"Implementation"},{"location":"event-processing/event-chunking/#considerations","text":"Chunking places additional burden on client applications. First, implementing the chunking and unchunking logic requires more application development. Second, the consumer application needs to be able to cache the chunks as it waits to receive all the smaller chunks that comprise the original event. This, in turn, can have implications on memory fragmentation and longer garbage collection (GC). Mitigating this depends on the programming language: in Java, for example, the JVM heap size and GC can be tuned. Client applications that are not aware of the protocol used for chunking events may not be able to reconstruct the original event accurately.","title":"Considerations"},{"location":"event-processing/event-chunking/#references","text":"To handle large events, an alternative approach that may be preferred is Claim Check","title":"References"},{"location":"event-processing/event-filter/","text":"Event Filter Event Processors may need to operate over a subset of Events over a particular Event Stream . Problem How can an application discard uninteresting events? Solution Implementation The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\"); References This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"Event Filter"},{"location":"event-processing/event-filter/#event-filter","text":"Event Processors may need to operate over a subset of Events over a particular Event Stream .","title":"Event Filter"},{"location":"event-processing/event-filter/#problem","text":"How can an application discard uninteresting events?","title":"Problem"},{"location":"event-processing/event-filter/#solution","text":"","title":"Solution"},{"location":"event-processing/event-filter/#implementation","text":"The Kafka Streams DSL provides a filter operator which filters out events that do not match a given predicate. KStream<String, Event> eventStream = builder.stream(.....); eventStream.filter((key, value) -> value.type() == \"foo\").to(\"foo-events\");","title":"Implementation"},{"location":"event-processing/event-filter/#references","text":"This pattern is derived from Message Filter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full example of filtering event streams.","title":"References"},{"location":"event-processing/event-mapper/","text":"Event Mapper Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa. Problem How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other? Solution Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events. Implementation In this example, we use Kafka's Java producer client to implement a Mapper that constructs an Event ( PublicationEvent ) from the Domain Model ( Publication ) before the event is written to an Event Stream a.k.a. topic in Kafka. private final IMapper domainToEventMapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author /* event key */, domainToEventMapper.map(newPub)); We can implement the reverse operation in a second Mapper that converts PublicationEvent instances back into Domain Object updates: private final IMapper eventToDomainMapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = eventToDomainMapper.map(pubEvent); domainStore.update(newPub); Considerations The mapper may optionally validate the schema of the converted objects, see the Schema Validator pattern. References Related patterns: Event Serializer and Event Deserializer This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Mapper"},{"location":"event-processing/event-mapper/#event-mapper","text":"Traditional applications (operating with data at rest) and Event Processing Applications (with data in motion), may need to share data via the Event Streaming Platform . These applications will need a common mechanism to convert data from events to domain objects and vice versa.","title":"Event Mapper"},{"location":"event-processing/event-mapper/#problem","text":"How do I move data between an application\u2019s internal data model (with domain objects) and an event streaming platform (with events) while keeping the two independent of each other?","title":"Problem"},{"location":"event-processing/event-mapper/#solution","text":"Event Mappers provide independence between the application and the event streaming platform so that neither is aware of the other, and ideally not even of the event mapper itself. Create (or use an existing) Event Mapper to map the Domain Model (or the application's internal data model) to the data formats accepted by the event streaming platform, and vice versa. The mapper reads the domain model and converts it into outgoing events that are sent to the event streaming platform. Conversely, a mapper can be used to create or update domain objects from incoming events.","title":"Solution"},{"location":"event-processing/event-mapper/#implementation","text":"In this example, we use Kafka's Java producer client to implement a Mapper that constructs an Event ( PublicationEvent ) from the Domain Model ( Publication ) before the event is written to an Event Stream a.k.a. topic in Kafka. private final IMapper domainToEventMapper = mapperFactory.buildMapper(Publication.class); private final Producer<String, PublicationEvent> producer = ... public void newPublication(String author, String title) { Publication newPub = new Publication(author, title); producer.send(author /* event key */, domainToEventMapper.map(newPub)); We can implement the reverse operation in a second Mapper that converts PublicationEvent instances back into Domain Object updates: private final IMapper eventToDomainMapper = mapperFactory.buildMapper(Publication.class); private final Consumer<String, PublicationEvent> consumer = ... public void updatePublication(PublicationEvent pubEvent) { Publication newPub = eventToDomainMapper.map(pubEvent); domainStore.update(newPub);","title":"Implementation"},{"location":"event-processing/event-mapper/#considerations","text":"The mapper may optionally validate the schema of the converted objects, see the Schema Validator pattern.","title":"Considerations"},{"location":"event-processing/event-mapper/#references","text":"Related patterns: Event Serializer and Event Deserializer This pattern is derived from Messaging Mapper in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/event-processing-application/","text":"Event Processing Application Once data such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. is set in motion as streams of events on an Event Streaming Platform, we want to put it to use and create value from it. Event Processors are the building blocks for achieving this, but they solve only a specific part or step of a use case. Problem How can we build a full-fledged application for data in motion that creates, reads, processes, and/or queries Event Streams to solve a use case end-to-end? Solution We build an Event Processing Application by composing one or more Event Processors into an interconnected processing topology for Event Streams and Tables . Here, the continuous output streams of one processor are the continuous input streams to one or more downstream processors. The combined functionality of the application then covers our use case end-to-end, or at least as much of the use case as we are striving for (as the question of how few or many applications should implement a use case is an important design decision, which we are not covering here). The event processors\u2014and thus the application at large\u2014are typically distributed (running across multiple instances) to allow for elastic, parallel, fault-tolerant processing of data in motion at scale. For example, an application can read a stream of customer payments from an Event Store in an Event Streaming Platform , then filter payments for certain customers, and then aggregate those payments per country and per week. The processing mode is stream processing, i.e., data is continuously processed 24x7. As soon as new Events are available, they are processed and propagated through the topology of Event Processors . Implementation Apache Kafka\u00ae is the most popular Event Streaming Platform . There are several options for building Event Processing Applications when using Kafka, and we'll show two here. ksqlDB ksqlDB is a streaming database with which we can build Event Processing Applications using SQL syntax. It has first-class support for Streams and Tables . When we create Tables and Streams in ksqlDB, then Kafka topics are used as the storage layer behind the scenes. In the example below, the ksqlDB table movies is backed by a Kafka topic of the same name. CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (kafka_topic='movies', partitions=1, value_format='avro'); CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (kafka_topic='ratings', partitions=1, value_format='avro'); As one would expect, we can add new Events with INSERT : INSERT INTO movies (id, title, release_year) VALUES (294, 'Die Hard', 1998); INSERT INTO ratings (movie_id, rating) VALUES (294, 8.2); We can also perform stream processing with ksqlDB's SQL. The command CREATE STREAM .. AS SELECT .. in the following example continuously joins the ratings stream and the movies table to create a new stream of enriched ratings. CREATE STREAM rated_movies WITH (kafka_topic='rated_movies', value_format='avro') AS SELECT ratings.movie_id as id, title, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id; Kafka Streams With the Kafka Streams client library of Apache Kafka, we implement an event processing application in Java, Scala, or other JVM languages. A Kafka Streams example similar to the ksqlDB one above is: KStream<Integer, Rating> ratings = builder.table(<blabla>); KTable<Integer, Movie> movies = builder.stream(<blabla>); MovieRatingJoiner joiner = new MovieRatingJoiner(); KStream <Integer, EnrichedRating> enrichedRatings = ratings.join(movies, joiner); See the tutorial How to join a stream and lookup table for a full example using Kafka Streams. Considerations When building an Event Processing Application, it's recommended to confine the application to one problem domain. While it's possible to use any number of event processors in the application, they should be closely related in most cases (similar to how one would design a microservice). Event Processing Applications themselves can be composed, too. This is a common design pattern to implement event-driven architectures, which are powered by a fleet of applications and microservices. In this design, the output of one application forms the input to one or more downstream applications. This is conceptually similar to the topology of Event Processors , as described above. A key difference in practice, however, is that different applications are often built by different teams inside an organization. For example, a customer-facing application built by the Payments team is continuously feeding data via Event Streams to an application built by the Anti-fraud team and to another application built by the Data Science team. References The Event Streaming Platform pattern provides a higher-level overview of how Event Processing Applications are utilized across the streaming platform. The tutorial Joining Streams and Tables in ksqlDB provides a step-by-step example of event processing using SQL. How to sum a stream of events is a ksqlDB tutorial for applying an aggregate function over an Event Stream .","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#event-processing-application","text":"Once data such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. is set in motion as streams of events on an Event Streaming Platform, we want to put it to use and create value from it. Event Processors are the building blocks for achieving this, but they solve only a specific part or step of a use case.","title":"Event Processing Application"},{"location":"event-processing/event-processing-application/#problem","text":"How can we build a full-fledged application for data in motion that creates, reads, processes, and/or queries Event Streams to solve a use case end-to-end?","title":"Problem"},{"location":"event-processing/event-processing-application/#solution","text":"We build an Event Processing Application by composing one or more Event Processors into an interconnected processing topology for Event Streams and Tables . Here, the continuous output streams of one processor are the continuous input streams to one or more downstream processors. The combined functionality of the application then covers our use case end-to-end, or at least as much of the use case as we are striving for (as the question of how few or many applications should implement a use case is an important design decision, which we are not covering here). The event processors\u2014and thus the application at large\u2014are typically distributed (running across multiple instances) to allow for elastic, parallel, fault-tolerant processing of data in motion at scale. For example, an application can read a stream of customer payments from an Event Store in an Event Streaming Platform , then filter payments for certain customers, and then aggregate those payments per country and per week. The processing mode is stream processing, i.e., data is continuously processed 24x7. As soon as new Events are available, they are processed and propagated through the topology of Event Processors .","title":"Solution"},{"location":"event-processing/event-processing-application/#implementation","text":"Apache Kafka\u00ae is the most popular Event Streaming Platform . There are several options for building Event Processing Applications when using Kafka, and we'll show two here.","title":"Implementation"},{"location":"event-processing/event-processing-application/#ksqldb","text":"ksqlDB is a streaming database with which we can build Event Processing Applications using SQL syntax. It has first-class support for Streams and Tables . When we create Tables and Streams in ksqlDB, then Kafka topics are used as the storage layer behind the scenes. In the example below, the ksqlDB table movies is backed by a Kafka topic of the same name. CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (kafka_topic='movies', partitions=1, value_format='avro'); CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (kafka_topic='ratings', partitions=1, value_format='avro'); As one would expect, we can add new Events with INSERT : INSERT INTO movies (id, title, release_year) VALUES (294, 'Die Hard', 1998); INSERT INTO ratings (movie_id, rating) VALUES (294, 8.2); We can also perform stream processing with ksqlDB's SQL. The command CREATE STREAM .. AS SELECT .. in the following example continuously joins the ratings stream and the movies table to create a new stream of enriched ratings. CREATE STREAM rated_movies WITH (kafka_topic='rated_movies', value_format='avro') AS SELECT ratings.movie_id as id, title, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id;","title":"ksqlDB"},{"location":"event-processing/event-processing-application/#kafka-streams","text":"With the Kafka Streams client library of Apache Kafka, we implement an event processing application in Java, Scala, or other JVM languages. A Kafka Streams example similar to the ksqlDB one above is: KStream<Integer, Rating> ratings = builder.table(<blabla>); KTable<Integer, Movie> movies = builder.stream(<blabla>); MovieRatingJoiner joiner = new MovieRatingJoiner(); KStream <Integer, EnrichedRating> enrichedRatings = ratings.join(movies, joiner); See the tutorial How to join a stream and lookup table for a full example using Kafka Streams.","title":"Kafka Streams"},{"location":"event-processing/event-processing-application/#considerations","text":"When building an Event Processing Application, it's recommended to confine the application to one problem domain. While it's possible to use any number of event processors in the application, they should be closely related in most cases (similar to how one would design a microservice). Event Processing Applications themselves can be composed, too. This is a common design pattern to implement event-driven architectures, which are powered by a fleet of applications and microservices. In this design, the output of one application forms the input to one or more downstream applications. This is conceptually similar to the topology of Event Processors , as described above. A key difference in practice, however, is that different applications are often built by different teams inside an organization. For example, a customer-facing application built by the Payments team is continuously feeding data via Event Streams to an application built by the Anti-fraud team and to another application built by the Data Science team.","title":"Considerations"},{"location":"event-processing/event-processing-application/#references","text":"The Event Streaming Platform pattern provides a higher-level overview of how Event Processing Applications are utilized across the streaming platform. The tutorial Joining Streams and Tables in ksqlDB provides a step-by-step example of event processing using SQL. How to sum a stream of events is a ksqlDB tutorial for applying an aggregate function over an Event Stream .","title":"References"},{"location":"event-processing/event-processor/","text":"Event Processor Once data such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. is set in motion as streams of events on an Event Streaming Platform , we want to put it to use and create value from it. How do we do this? Problem How do we process Events in an Event Streaming Platform ? Solution We build an Event Processor, which is a component that reads Events and processes them, and possibly writes new Events as the result of its processing. As such, it may act as an Event Source and/or Event Sink , and in practice often acts as both. An event processor can be distributed, which means it has multiple instances that run across different machines. In this case the processing of Events happens concurrently across these instances. An important characteristic of an event processor is that it should allow for composition with other event processors. That's because, in practice, we rarely use a single event processor in isolation. Instead, we compose and connect (via Event Streams ) one or more event processors inside an Event Processing Application that fully implements one particular use case end-to-end, or (e.g., in the case of microservices) that implements a subset of the overall business logic limited to the bounded context of a particular domain. An event processor performs a specific task within the event processing application. Think of it as one processing node (or processing step) of a larger processing topology. Examples are the mapping of an event type to a domain object, filtering only the important events out of an Event Stream , enriching an event stream with additional data by joining it to another stream or database table, triggering alerts, or creating new events for consumption by other applications. Implementation There are multiple ways to create an Event Processing Application using Event Processors, we will look at two. ksqlDB The streaming database ksqlDB provides a familiar SQL syntax that allows us to create Event Processing Applications . ksqlDB takes parses SQL commands and constructs and manages the Event Processors we define as part of an Event Processing Application . In the following example, we create a ksqlDB query to reading data from the readings Event Stream and \"cleaning\" the Event values. The query publishes the clean readings to a new stream called clean_readings . Here, this query acts as an event processing application comprised of multiple event processors that are connected to each other. CREATE STREAM clean_readings AS SELECT sensor, reading, UCASE(location) AS location FROM readings EMIT CHANGES; With ksqlDB, we can view each section of the command as the construction of a different Event Processor: CREATE STREAM defines the new output Event Stream that this application will produce Events to. SELECT ... is a mapping function, taking each input Event and \"cleaning\" it as defined. In this example, this simply means upper casing the location field in each input reading. FROM ... is a source Event Processor that defines the input Event Stream for the overall application. EMIT CHANGES is ksqlDB syntax which defines our query as continuously running, and that incremental changes will be produced as the query runs perpetually. Kafka Streams The Kafka Streams DSL provides abstractions for Event Streams and Tables as well as stateful and stateless transformation functions ( map , filter , etc...). These functions act as the Event Processor in the larger Event Processing Application we build with the Kafka Streams library. builder .stream(\"readings\"); .mapValues((key, value)-> new Reading(value.sensor, value.reading, value.location.toUpperCase()) .to(\"clean\"); In the above example we use the Kafka Streams Builder to construct the stream processing topology. First we create an input stream with the stream function. This creates an Event Stream from the designated Kafka topic. Next we transform the Events using the mapValues function. This function accepts each Event and returns a new Event with any desired transformations to the values. Finally we write our transformed Events to a destination Kafka topic using the to function. This function terminates our stream processing topology. Considerations While it could be tempting to build a \"multi-purpose\" event processor, it's important that processors are designed in a composable way. By building processors as discrete units, it's easier to reason about what each processor does and, by extension, what the Event Processing Application does. References Event Processing Applications are composed of Event Processors. In Kafka Streams , a processor is a node in the processor topology representing a step to transform Events . Blog post: How real-time stream processing works with ksqlDB, Animated . Introduction to Apache Kafka: How Kafka works provides details on the core Kafka concepts like Events and topics.","title":"Event Processor"},{"location":"event-processing/event-processor/#event-processor","text":"Once data such as financial transactions, tracking information for shipments, IoT sensor measurements, etc. is set in motion as streams of events on an Event Streaming Platform , we want to put it to use and create value from it. How do we do this?","title":"Event Processor"},{"location":"event-processing/event-processor/#problem","text":"How do we process Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-processing/event-processor/#solution","text":"We build an Event Processor, which is a component that reads Events and processes them, and possibly writes new Events as the result of its processing. As such, it may act as an Event Source and/or Event Sink , and in practice often acts as both. An event processor can be distributed, which means it has multiple instances that run across different machines. In this case the processing of Events happens concurrently across these instances. An important characteristic of an event processor is that it should allow for composition with other event processors. That's because, in practice, we rarely use a single event processor in isolation. Instead, we compose and connect (via Event Streams ) one or more event processors inside an Event Processing Application that fully implements one particular use case end-to-end, or (e.g., in the case of microservices) that implements a subset of the overall business logic limited to the bounded context of a particular domain. An event processor performs a specific task within the event processing application. Think of it as one processing node (or processing step) of a larger processing topology. Examples are the mapping of an event type to a domain object, filtering only the important events out of an Event Stream , enriching an event stream with additional data by joining it to another stream or database table, triggering alerts, or creating new events for consumption by other applications.","title":"Solution"},{"location":"event-processing/event-processor/#implementation","text":"There are multiple ways to create an Event Processing Application using Event Processors, we will look at two.","title":"Implementation"},{"location":"event-processing/event-processor/#ksqldb","text":"The streaming database ksqlDB provides a familiar SQL syntax that allows us to create Event Processing Applications . ksqlDB takes parses SQL commands and constructs and manages the Event Processors we define as part of an Event Processing Application . In the following example, we create a ksqlDB query to reading data from the readings Event Stream and \"cleaning\" the Event values. The query publishes the clean readings to a new stream called clean_readings . Here, this query acts as an event processing application comprised of multiple event processors that are connected to each other. CREATE STREAM clean_readings AS SELECT sensor, reading, UCASE(location) AS location FROM readings EMIT CHANGES; With ksqlDB, we can view each section of the command as the construction of a different Event Processor: CREATE STREAM defines the new output Event Stream that this application will produce Events to. SELECT ... is a mapping function, taking each input Event and \"cleaning\" it as defined. In this example, this simply means upper casing the location field in each input reading. FROM ... is a source Event Processor that defines the input Event Stream for the overall application. EMIT CHANGES is ksqlDB syntax which defines our query as continuously running, and that incremental changes will be produced as the query runs perpetually.","title":"ksqlDB"},{"location":"event-processing/event-processor/#kafka-streams","text":"The Kafka Streams DSL provides abstractions for Event Streams and Tables as well as stateful and stateless transformation functions ( map , filter , etc...). These functions act as the Event Processor in the larger Event Processing Application we build with the Kafka Streams library. builder .stream(\"readings\"); .mapValues((key, value)-> new Reading(value.sensor, value.reading, value.location.toUpperCase()) .to(\"clean\"); In the above example we use the Kafka Streams Builder to construct the stream processing topology. First we create an input stream with the stream function. This creates an Event Stream from the designated Kafka topic. Next we transform the Events using the mapValues function. This function accepts each Event and returns a new Event with any desired transformations to the values. Finally we write our transformed Events to a destination Kafka topic using the to function. This function terminates our stream processing topology.","title":"Kafka Streams"},{"location":"event-processing/event-processor/#considerations","text":"While it could be tempting to build a \"multi-purpose\" event processor, it's important that processors are designed in a composable way. By building processors as discrete units, it's easier to reason about what each processor does and, by extension, what the Event Processing Application does.","title":"Considerations"},{"location":"event-processing/event-processor/#references","text":"Event Processing Applications are composed of Event Processors. In Kafka Streams , a processor is a node in the processor topology representing a step to transform Events . Blog post: How real-time stream processing works with ksqlDB, Animated . Introduction to Apache Kafka: How Kafka works provides details on the core Kafka concepts like Events and topics.","title":"References"},{"location":"event-processing/event-router/","text":"Event Router Event Streams may contain a subset of Events which need to be processed in isolation. For example, an inventory check system may be distributed across multiple physical systems, and the target system depends on the category of the item being checked. Problem How can we isolate Events into a dedicated Event Stream based on some attribute of the Events ? Solution Implementation With ksqlDB , we can continuously route events to a different stream using the CREATE STREAM syntax with an appropriate WHERE filter. CREATE STREAM payments ...; CREATE STREAM payments_france AS SELECT * FROM payments WHERE country = 'france'; CREATE STREAM payments_spain AS SELECT * FROM payments WHERE country = 'spain'; With the Kafka Streams library , use a TopicNameExtractor to route events to different streams (topics). The TopicNameExtractor has one method to implement, extract() , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, and other contextual information about the event. We can use any of the given parameters to generate and return the desired destination topic name for the given event, and Kafka Streams will complete the routing. CountryTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.country) { case \"france\": return \"france-topic\"; case \"spain\": return \"spain-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to(new CountryTopicExtractor()); Considerations Event Routers should not modify the Event itself and instead only provide the proper routing to the desired destinations. Consider the use of an Event Envelope if an event router should attach additional information or context to an event. References This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See the tutorial How to dynamically choose the output topic at runtime for a full example of dynamically routing events at runtime.","title":"Event Router"},{"location":"event-processing/event-router/#event-router","text":"Event Streams may contain a subset of Events which need to be processed in isolation. For example, an inventory check system may be distributed across multiple physical systems, and the target system depends on the category of the item being checked.","title":"Event Router"},{"location":"event-processing/event-router/#problem","text":"How can we isolate Events into a dedicated Event Stream based on some attribute of the Events ?","title":"Problem"},{"location":"event-processing/event-router/#solution","text":"","title":"Solution"},{"location":"event-processing/event-router/#implementation","text":"With ksqlDB , we can continuously route events to a different stream using the CREATE STREAM syntax with an appropriate WHERE filter. CREATE STREAM payments ...; CREATE STREAM payments_france AS SELECT * FROM payments WHERE country = 'france'; CREATE STREAM payments_spain AS SELECT * FROM payments WHERE country = 'spain'; With the Kafka Streams library , use a TopicNameExtractor to route events to different streams (topics). The TopicNameExtractor has one method to implement, extract() , which accepts three parameters: The event key The event value The RecordContext , which provides access to headers, partitions, and other contextual information about the event. We can use any of the given parameters to generate and return the desired destination topic name for the given event, and Kafka Streams will complete the routing. CountryTopicExtractor implements TopicNameExtractor<String, String> { String extract(String key, String value, RecordContext recordContext) { switch (value.country) { case \"france\": return \"france-topic\"; case \"spain\": return \"spain-topic\"; } } } KStream<String, String> myStream = builder.stream(...); myStream.mapValues(..).to(new CountryTopicExtractor());","title":"Implementation"},{"location":"event-processing/event-router/#considerations","text":"Event Routers should not modify the Event itself and instead only provide the proper routing to the desired destinations. Consider the use of an Event Envelope if an event router should attach additional information or context to an event.","title":"Considerations"},{"location":"event-processing/event-router/#references","text":"This pattern is derived from Message Router in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See the tutorial How to dynamically choose the output topic at runtime for a full example of dynamically routing events at runtime.","title":"References"},{"location":"event-processing/event-splitter/","text":"Event Splitter One Event may actually contain multiple child events within it, each of which may need to be processed in different ways. Problem How can an Event be split into multiple events? Solution First, split the original event into multiple child events. Then, publish one event per child. Implementation Many event processing technologies support this operation. ksqlDB has the EXPLODE() table function which takes an array and outputs one value for each of the elements of the array. The example below processes each input event, un-nesting the array and generating new events for each element with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; Kafka Streams has an analogous method called flatMap() . The example below processes each input event, generating new events with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } ); Or as my grandmother used to say: There once was a man from Manhattan, With Events that he needed to flatten, He cooked up a scheme, To call flapMap on stream , Then he wrote it all down as a pattern. Considerations If child events need to be routed to different streams, see Event Router for routing events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: a use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier. References This pattern is derived from Splitter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Splitter"},{"location":"event-processing/event-splitter/#event-splitter","text":"One Event may actually contain multiple child events within it, each of which may need to be processed in different ways.","title":"Event Splitter"},{"location":"event-processing/event-splitter/#problem","text":"How can an Event be split into multiple events?","title":"Problem"},{"location":"event-processing/event-splitter/#solution","text":"First, split the original event into multiple child events. Then, publish one event per child.","title":"Solution"},{"location":"event-processing/event-splitter/#implementation","text":"Many event processing technologies support this operation. ksqlDB has the EXPLODE() table function which takes an array and outputs one value for each of the elements of the array. The example below processes each input event, un-nesting the array and generating new events for each element with new column names. SELECT EXPLODE(TOTAL)->TOTALTYPE AS TOTAL_TYPE, EXPLODE(TOTAL)->TOTALAMOUNT AS TOTAL_AMOUNT, EXPLODE(TOTAL)->ID AS CUSTOMER_ID FROM my_stream EMIT CHANGES; Kafka Streams has an analogous method called flatMap() . The example below processes each input event, generating new events with new keys and values. KStream<Long, String> myStream = ...; KStream<String, Integer> splitStream = myStream.flatMap( (eventKey, eventValue) -> { List<KeyValue<String, Integer>> result = new LinkedList<>(); result.add(KeyValue.pair(eventValue.toUpperCase(), 1000)); result.add(KeyValue.pair(eventValue.toLowerCase(), 9000)); return result; } ); Or as my grandmother used to say: There once was a man from Manhattan, With Events that he needed to flatten, He cooked up a scheme, To call flapMap on stream , Then he wrote it all down as a pattern.","title":"Implementation"},{"location":"event-processing/event-splitter/#considerations","text":"If child events need to be routed to different streams, see Event Router for routing events to different locations. Capacity planning and sizing: splitting the original event into N child events leads to write amplification, thereby increasing the volume of events that must be managed by the event streaming platform. Event Lineage: a use case may require tracking the lineage of parent and child events. If so, ensure that the child events include a data field containing a reference to the original parent event, e.g. a unique identifier.","title":"Considerations"},{"location":"event-processing/event-splitter/#references","text":"This pattern is derived from Splitter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-processing/event-streaming-api/","text":"Event Streaming API Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way. Problem How can my application connect to an Event Streaming Platform to send and receive Events ? Solution The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The higher level libraries allow the application to focus on business logic leaving the details of the platform communication to the API. References This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#event-streaming-api","text":"Applications that connect to the Event Streaming Platform need to do so in a consistent and reliable way.","title":"Event Streaming API"},{"location":"event-processing/event-streaming-api/#problem","text":"How can my application connect to an Event Streaming Platform to send and receive Events ?","title":"Problem"},{"location":"event-processing/event-streaming-api/#solution","text":"The Event Streaming Platform provides an Application Programming Interface (API) allowing applications to reliably communicate across the platform. The API provides a logical and well documented protocol which defines the message structure and data exchange methods. Higher level libraries implement these protocols allowing a variety of technologies and programming languages to interface with the platform. The higher level libraries allow the application to focus on business logic leaving the details of the platform communication to the API.","title":"Solution"},{"location":"event-processing/event-streaming-api/#references","text":"This pattern is derived from Message Endpoint in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The Apache Kafka Protocol Guide provides details on the wire protocol implemented in Kafka. The Apache Kafka API documentation contains information on the variety of APIs available for reading, writing, and administering Kafka.","title":"References"},{"location":"event-processing/event-translator/","text":"Event Translator Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them. Problem How can systems using different data formats communicate with each other using Events ? Solution An Event Translator converts a data format into a standard format familiar to downstream Event Processors . This can take the form of field manipulation, for example mapping one event schema (ref) to another event schema. Another common form is different serialization types, for example, translating Avro to JSON or Protobuf to Avro. Implementation The streaming database ksqlDB provides the ability to create Event Streams with SQL statements. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream Considerations In some cases translations will be unidirectional if data is lost, for example translating XML to JSON will often lose information meaning the original form cannot be recreated. References This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Translator"},{"location":"event-processing/event-translator/#event-translator","text":"Event Streaming Platforms will connect a variety of systems over time, and common data formats may not be feasible across them.","title":"Event Translator"},{"location":"event-processing/event-translator/#problem","text":"How can systems using different data formats communicate with each other using Events ?","title":"Problem"},{"location":"event-processing/event-translator/#solution","text":"An Event Translator converts a data format into a standard format familiar to downstream Event Processors . This can take the form of field manipulation, for example mapping one event schema (ref) to another event schema. Another common form is different serialization types, for example, translating Avro to JSON or Protobuf to Avro.","title":"Solution"},{"location":"event-processing/event-translator/#implementation","text":"The streaming database ksqlDB provides the ability to create Event Streams with SQL statements. CREATE STREAM translated_stream AS SELECT fieldX AS fieldC, field.Y AS fieldA, field.Z AS fieldB FROM untranslated_stream","title":"Implementation"},{"location":"event-processing/event-translator/#considerations","text":"In some cases translations will be unidirectional if data is lost, for example translating XML to JSON will often lose information meaning the original form cannot be recreated.","title":"Considerations"},{"location":"event-processing/event-translator/#references","text":"This pattern is derived from Event Translator in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink-connector/","text":"Event Sink Connector Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations. Problem How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ? Solution Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system. Implementation CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Apache Kafka\u00ae, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink . Considerations There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform. References For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#event-sink-connector","text":"Connecting external systems to the Event Streaming Platform allows for advanced and specialized integrations.","title":"Event Sink Connector"},{"location":"event-sink/event-sink-connector/#problem","text":"How can we connect applications or external systems, like databases, to an Event Streaming Platform so that it can receive Events ?","title":"Problem"},{"location":"event-sink/event-sink-connector/#solution","text":"Event Sink Connector is a specific implementation of an Event Sink . Use an Event Sink Connector to transfer Events from the Event Stream into the specific external system.","title":"Solution"},{"location":"event-sink/event-sink-connector/#implementation","text":"CREATE SINK CONNECTOR JDBC_SINK_POSTGRES_01 WITH ( 'connector.class' = 'io.confluent.connect.jdbc.JdbcSinkConnector', 'connection.url' = 'jdbc:postgresql://postgres:5432/', 'connection.user' = 'postgres', 'connection.password' = 'postgres', 'topics' = 'TEMPERATURE_READINGS_TIMESTAMP_MT', 'auto.create' = 'true', 'auto.evolve' = 'true' ); When connecting a system like a relational database to Apache Kafka\u00ae, the most common option is to use Kafka Connect . The connector reads events from the Event Streaming Platform , performs any necessary transformations, and writes the Events to the specified Event Sink .","title":"Implementation"},{"location":"event-sink/event-sink-connector/#considerations","text":"There are many Event Sink Connectors readily available for Apache Kafka, e.g. connectors for relational databases or object storage systems like AWS S3. See Confluent Hub for available connectors. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication and authorization, etc. between event sink, event sink connector, and the event streaming platform.","title":"Considerations"},{"location":"event-sink/event-sink-connector/#references","text":"For an example of using Kafka Connect as an Event Sink Connector, see Timezone conversion and Kafka Connect JDBC sink with ksqlDB . This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-sink/event-sink/","text":"Event Sink Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink. Problem How can we read (or consume / subscribe to) Events in an Event Streaming Platform ? Solution Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB . Implementation ksqlDB example: Reading events from an existing Apache Kafka\u00ae topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); } References The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"Event Sink"},{"location":"event-sink/event-sink/#event-sink","text":"Various components in an Event Streaming Platform will read or receive Events . An Event Sink is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an Event Sink is the opposite of an Event Source . In practice, however, components such as an Event Processing Application can act as both an Event Source and an Event Sink.","title":"Event Sink"},{"location":"event-sink/event-sink/#problem","text":"How can we read (or consume / subscribe to) Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-sink/event-sink/#solution","text":"Use an Event Sink, which typically acts as a client in an Event Streaming Platform . Examples are an Event Sink Connector (which continuously exports Event Streams from the Event Streaming Platform into an external system such as a cloud service or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB .","title":"Solution"},{"location":"event-sink/event-sink/#implementation","text":"ksqlDB example: Reading events from an existing Apache Kafka\u00ae topic into a ksqlDB event stream for further processing. CREATE STREAM clicks (ip_address VARCHAR, url VARCHAR, timestamp VARCHAR) WITH (KAFKA_TOPIC = 'clicks-topic', VALUE_FORMAT = 'json', TIMESTAMP = 'timestamp', TIMESTAMP_FORMAT = 'yyyy-MM-dd''T''HH:mm:ssXXX'); Generic Kafka Consumer application: See Getting Started with Apache Kafka and Java for a full example: consumer.subscribe(Collections.singletonList(\"stream\")); while (keepConsuming) { final ConsumerRecords<String, EventRecord> consumerRecords = consumer.poll(Duration.ofSeconds(1)); recordsHandler.process(consumerRecords); }","title":"Implementation"},{"location":"event-sink/event-sink/#references","text":"The Kafka Streams library of Apache Kafka is another popular choice of developers to implement elastic applications and microservices that read, process, and write events. See Filter a stream of events for a first example.","title":"References"},{"location":"event-source/database-write-aside/","text":"Database Write Aside Problem How do I update a value in a database and create an associated event with the least amount of effort? Solution Pattern Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails). Example Implementation //Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); } Considerations In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled. References TODO: Add references?","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#database-write-aside","text":"","title":"Database Write Aside"},{"location":"event-source/database-write-aside/#problem","text":"How do I update a value in a database and create an associated event with the least amount of effort?","title":"Problem"},{"location":"event-source/database-write-aside/#solution-pattern","text":"Write to a database, then write to Kafka. Perform the write to Kafka as the last step in a database transaction to ensure an atomic dual commit (aborting the transaction if the write to Kafka fails).","title":"Solution Pattern"},{"location":"event-source/database-write-aside/#example-implementation","text":"//Enable transactions db.setAutoCommit(false); try{ //insert into the DB sql = db.prepareStatement(\"insert mydb.events values (?)\"); sql.setString(event.toString()); sql.executeUpdate(); //insert into Kafka producer.send(event.key(), event.value()); //commit to the DB db.commit(); } catch (SQLException e ) { db.rollback(); }","title":"Example Implementation"},{"location":"event-source/database-write-aside/#considerations","text":"In its default form, this pattern guarantees dual-write for most use cases. However, should the database transaction fail at commit time (say, because the database server has crashed) the write to Kafka cannot be rolled back unless transactions have been enabled. For many use cases, this eventuality will be tolerable as the dual-write can be retried once the failure is fixed, and most event consumers will implement idempotence anyway. However, application programmers need to be aware that there is no firm guarantee. Transactional messaging systems like Kafka can be used to provide stronger guarantees so long as all event consumers have the transactions feature enabled.","title":"Considerations"},{"location":"event-source/database-write-aside/#references","text":"TODO: Add references?","title":"References"},{"location":"event-source/database-write-through/","text":"Database Write Through Problem How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform. Solution Pattern Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications. Example Implementation TODO: Example for CDC? Considerations The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row. References TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"Database Write Through"},{"location":"event-source/database-write-through/#database-write-through","text":"","title":"Database Write Through"},{"location":"event-source/database-write-through/#problem","text":"How do I update a value in a database and create an associated event with at-least-once guarantees? This pattern is a specialization of the Event Source Connector that guarantees that all state changes represented in an event source, including changes to tables in an event streaming platform, are captured in an event streaming platform.","title":"Problem"},{"location":"event-source/database-write-through/#solution-pattern","text":"Write to a database table, which is the Event Source. Then set up streaming Change Data Capture (CDC) on that table to continuously ingest any changes\u2014inserts, updates, deletes\u2014into an Event Stream in Kafka. Typically, Kafka Connect is used for this step in combination with an appropriate Event Source Connector for the database. See Confluent Hub for a list of available connectors. The events in stream can then be consumed by Event Processing Applications. Additionally, the event stream can be read into a Projection Table, for example with ksqlDB, so that it can be queried by other applications.","title":"Solution Pattern"},{"location":"event-source/database-write-through/#example-implementation","text":"TODO: Example for CDC?","title":"Example Implementation"},{"location":"event-source/database-write-through/#considerations","text":"The processing guarantees (cf. \"Guaranteed Delivery\") to choose from\u2014e.g., at-least-once, exactly-once\u2014for the CDC data flow depend on the selected Kafka connector. There is a certain delay until changes in the source database table are available in the CDC-ingested event stream. The amount of the delay depends on a variety of factors, including the features and configuration of the event source connector. In many typical scenarios the delay is less than a few seconds. In terms of their data model, events typically require the row key to be used as the Kafka event key (aka record/message key), which is the only way to ensure all events for the same DB table row go to the same Kafka topic-partition and are thus totally ordered. They also typically model deletes as tombstone events, i.e. an event with a non-null key and a null value. By ensuring totally ordered events for each row, consumers see an eventually-consistent representation of these events for each row.","title":"Considerations"},{"location":"event-source/database-write-through/#references","text":"TODO: Pointers to Confluent Source connector(s)? TODO: What about well known CDC providers, like Debizium?","title":"References"},{"location":"event-source/event-gateway/","text":"Event Gateway One of the key benefits of adopting an Event -first architecture is to foster collaboration. Our aim is to ensure that Team A can produce data, Team B can process it and Team C can report on it, with only one thing coupling the teams together - the data itself. Teams shouldn't have to agree on shared libraries, synchronized release schedules, or common tooling. Data becomes the one true interface. In reality though, each team will still have to communicate with the Event Store itself. How do we maximize access? How do we ensure that every team can use the event store, without insisting they choose from a shortlist of supported languages? How do we accommodate the team that insists on using Idris + ? + Or Haskell, or Rust, or Erlang, or whatever other language we didn't plan for... Problem How does an an Event Streaming Platform provide access to the widest-possible range of users? Solution Provide an event gateway via a standardized, well-supported interface that gives access to the widest possible range of users. Implementation Confluent provides a broad set of REST APIs that allow any language or CLI to access the event store using HTTP(S). Further, it provides support to produce and consume Apache Kafka\u00ae data, formatted as JSON, Protobuf, Avro or even raw base64-encoded bytes. As a simple example, we can post JSON-encoded events to a topic called sales using curl : curl -X POST \\ -H \"Content-Type: application/vnd.kafka.json.v2+json\" \\ --data '{\"records\":[{\"key\":\"alice\",\"value\":{\"tickets\":5}},{\"key\":\"bob\",\"value\":{\"tickets\":10}}]}' \\ http://localhost:8082/topics/sales { \"offsets\": [ { \"partition\": 0, \"offset\": 0, \"error_code\": null, \"error\": null }, { \"partition\": 0, \"offset\": 1, \"error_code\": null, \"error\": null } ], \"key_schema_id\": null, \"value_schema_id\": null } Considerations In a perfect world, every Event Streaming Platform (and every relational database) would have first-class support for every language. Realistically some languages will be better accommodated than others, but we can still ensure every language has access to every important feature through a standards-based interface. References The Confluent REST APIs documentation","title":"Event Gateway"},{"location":"event-source/event-gateway/#event-gateway","text":"One of the key benefits of adopting an Event -first architecture is to foster collaboration. Our aim is to ensure that Team A can produce data, Team B can process it and Team C can report on it, with only one thing coupling the teams together - the data itself. Teams shouldn't have to agree on shared libraries, synchronized release schedules, or common tooling. Data becomes the one true interface. In reality though, each team will still have to communicate with the Event Store itself. How do we maximize access? How do we ensure that every team can use the event store, without insisting they choose from a shortlist of supported languages? How do we accommodate the team that insists on using Idris + ? + Or Haskell, or Rust, or Erlang, or whatever other language we didn't plan for...","title":"Event Gateway"},{"location":"event-source/event-gateway/#problem","text":"How does an an Event Streaming Platform provide access to the widest-possible range of users?","title":"Problem"},{"location":"event-source/event-gateway/#solution","text":"Provide an event gateway via a standardized, well-supported interface that gives access to the widest possible range of users.","title":"Solution"},{"location":"event-source/event-gateway/#implementation","text":"Confluent provides a broad set of REST APIs that allow any language or CLI to access the event store using HTTP(S). Further, it provides support to produce and consume Apache Kafka\u00ae data, formatted as JSON, Protobuf, Avro or even raw base64-encoded bytes. As a simple example, we can post JSON-encoded events to a topic called sales using curl : curl -X POST \\ -H \"Content-Type: application/vnd.kafka.json.v2+json\" \\ --data '{\"records\":[{\"key\":\"alice\",\"value\":{\"tickets\":5}},{\"key\":\"bob\",\"value\":{\"tickets\":10}}]}' \\ http://localhost:8082/topics/sales { \"offsets\": [ { \"partition\": 0, \"offset\": 0, \"error_code\": null, \"error\": null }, { \"partition\": 0, \"offset\": 1, \"error_code\": null, \"error\": null } ], \"key_schema_id\": null, \"value_schema_id\": null }","title":"Implementation"},{"location":"event-source/event-gateway/#considerations","text":"In a perfect world, every Event Streaming Platform (and every relational database) would have first-class support for every language. Realistically some languages will be better accommodated than others, but we can still ensure every language has access to every important feature through a standards-based interface.","title":"Considerations"},{"location":"event-source/event-gateway/#references","text":"The Confluent REST APIs documentation","title":"References"},{"location":"event-source/event-source-connector/","text":"Event Source Connector Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources . Problem How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events . Solution Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform . Implementation When connecting a cloud services and traditional systems to Apache Kafka\u00ae , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username'); Considerations End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform. References This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#event-source-connector","text":"Event Processing Applications may want to consume data from existing data systems, which are not themselves Event Sources .","title":"Event Source Connector"},{"location":"event-source/event-source-connector/#problem","text":"How can we connect cloud services and traditional systems, like relational databases, to an Event Streaming Platform , converting their data at rest to data in motion with Events .","title":"Problem"},{"location":"event-source/event-source-connector/#solution","text":"Generally speaking, we need to find a way to extract data as Events from the origin system. For relational databases, for example, a common technique is to use Change Data Capture , where changes to database tables\u2014such as INSERTs, UPDATES, DELETEs\u2014are captured as Events , which can then be ingested into another system. The components that perform this extraction and ingestion of Events are typically called \"connectors\". The connectors turn the origin system into an Event Source , then generate Events from that data, and finally sends these Events to the Event Streaming Platform .","title":"Solution"},{"location":"event-source/event-source-connector/#implementation","text":"When connecting a cloud services and traditional systems to Apache Kafka\u00ae , the most common solution is to use Kafka Connect . There are hundreds of ready-to-use connectors available on Confluent Hub , including blob stores like AWS S3, cloud services like Salesforce and Snowflake, relational databases, data warehouses, traditional message queues, flat files, and more. Confluent also provides many fully managed Kafka connectors in the cloud. There are several options to deploy such connectors. For example, the streaming database ksqlDB provides an ability to manage Kafka connectors with SQL statements. CREATE SOURCE CONNECTOR `jdbc-connector` WITH( \"connector.class\"='io.confluent.connect.jdbc.JdbcSourceConnector', \"connection.url\"='jdbc:postgresql://localhost:5432/my.db', \"mode\"='bulk', \"topic.prefix\"='jdbc-', \"table.whitelist\"='users', \"key\"='username');","title":"Implementation"},{"location":"event-source/event-source-connector/#considerations","text":"End-to-end data delivery guarantees (such as exactly-once delivery or at-least-once delivery) depend primarily on three factors: (1) the capabilities of the origin Event Source, such as a cloud service or relational database; (2) the capabilities of the Event Source Connector, and (3) the capabilities of the destination Event Streaming Platform, such as Apache Kafka or Confluent. Security policies as well as regulatory compliance may require appropriate settings for encrypted communication, authentication, and authorization, etc. between Event Source, Event Source Connector, and the destination Event Streaming Platform.","title":"Considerations"},{"location":"event-source/event-source-connector/#references","text":"This pattern is derived from Channel Adapter in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf See this Kafka Tutorial for a full Kafka Connect example","title":"References"},{"location":"event-source/event-source/","text":"Event Source Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an event source is the opposite of an Event Sink . In practice, however, components such as an event processing application can act as both an event source and an event sink. Problem How can we create Events in an Event Streaming Platform ? Solution Use an Event Source, which typically acts as a client in an Event Streaming Platform . Examples are an Event Source Connector (which continuously imports data as Event Streams into the Event Streaming Platform from an external system such as a cloud services or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB . Implementation Normally, an actual component such as an application for Kafka would be writing Events into an Event Stream , via a client library, API, gateway, etc. We can also write events directly using SQL syntax: the streaming database ksqlDB , for example, provides an INSERT statement. CREATE STREAM users (username VARCHAR, name VARCHAR, phone VARCHAR) with (kafka_topic='users-topic', value_format='json'); INSERT INTO users (username, name, phone) VALUES ('awilson', 'Allison', '+1 555-555-1234'); References ksqlDB The event streaming database purpose-built for stream processing applications. How to build client applications for writing events into an Event Stream .","title":"Event Source"},{"location":"event-source/event-source/#event-source","text":"Various components in an Event Streaming Platform will generate Events . An Event Source is the generalization of these components, which can include Event Processing Applications , cloud services, databases, IoT sensors, mainframes, and more. Conceptually, an event source is the opposite of an Event Sink . In practice, however, components such as an event processing application can act as both an event source and an event sink.","title":"Event Source"},{"location":"event-source/event-source/#problem","text":"How can we create Events in an Event Streaming Platform ?","title":"Problem"},{"location":"event-source/event-source/#solution","text":"Use an Event Source, which typically acts as a client in an Event Streaming Platform . Examples are an Event Source Connector (which continuously imports data as Event Streams into the Event Streaming Platform from an external system such as a cloud services or a relational database) or an Event Processing Application such as a Kafka Streams application and the streaming database ksqlDB .","title":"Solution"},{"location":"event-source/event-source/#implementation","text":"Normally, an actual component such as an application for Kafka would be writing Events into an Event Stream , via a client library, API, gateway, etc. We can also write events directly using SQL syntax: the streaming database ksqlDB , for example, provides an INSERT statement. CREATE STREAM users (username VARCHAR, name VARCHAR, phone VARCHAR) with (kafka_topic='users-topic', value_format='json'); INSERT INTO users (username, name, phone) VALUES ('awilson', 'Allison', '+1 555-555-1234');","title":"Implementation"},{"location":"event-source/event-source/#references","text":"ksqlDB The event streaming database purpose-built for stream processing applications. How to build client applications for writing events into an Event Stream .","title":"References"},{"location":"event-source/schema-validator/","text":"Schema Validator In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process these events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events, which is an important aspect of putting Data Contracts in place for data governance purposes. Problem How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream? Solution Validate whether an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. Such schema validation can be done: On the server side by the Event Streaming Platform that receives the Event . Events that fail schema validation and thus violate the Data Contract are rejected. On the client side by the Event Source that creates the Event . For example, an Event Source Connector can validate Events prior to ingestion into the Event Streaming Platform . Or, an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (e.g., Confluent's serializer/deserializers for Kafka). Implementation With Confluent, schema validation is fully supported with a per-environment hosted Schema Registry . Use the cloud UI to enable schema registry in your cloud provider of choice. Schemas can be managed per topic using the cloud UI or the Confluent Cloud CLI . An example command to create a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO Considerations Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write. Instead, consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (e.g., in regulated industries). Schema validation results in a load increase because it impacts the write path of every event. Client-side validation impacts primarily the load of the client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ). References See the Schema Compatibility pattern for information on how schemas can evolve over time and be verified. Learn more how to Manage and Validate Schemas with Confluent and Kafka .","title":"Schema Validator"},{"location":"event-source/schema-validator/#schema-validator","text":"In an Event Streaming Platform , Event Sources , which create and write Events , are decoupled from Event Sinks and Event Processing Applications , which read and process these events. Ensuring interoperability between the producers and the consumers of events requires that they agree on the data schemas for the events, which is an important aspect of putting Data Contracts in place for data governance purposes.","title":"Schema Validator"},{"location":"event-source/schema-validator/#problem","text":"How do I enforce that Events sent to an Event Stream conform to a defined schema for that stream?","title":"Problem"},{"location":"event-source/schema-validator/#solution","text":"Validate whether an Event conforms to the defined schema(s) of an Event Stream prior to writing the event to the stream. Such schema validation can be done: On the server side by the Event Streaming Platform that receives the Event . Events that fail schema validation and thus violate the Data Contract are rejected. On the client side by the Event Source that creates the Event . For example, an Event Source Connector can validate Events prior to ingestion into the Event Streaming Platform . Or, an Event Processing Application can use the schema validation functionality provided by a serialization library that supports schemas (e.g., Confluent's serializer/deserializers for Kafka).","title":"Solution"},{"location":"event-source/schema-validator/#implementation","text":"With Confluent, schema validation is fully supported with a per-environment hosted Schema Registry . Use the cloud UI to enable schema registry in your cloud provider of choice. Schemas can be managed per topic using the cloud UI or the Confluent Cloud CLI . An example command to create a schema using the CLI: ccloud schema-registry schema create --subject employees-value --schema employees.json --type AVRO","title":"Implementation"},{"location":"event-source/schema-validator/#considerations","text":"Schema Validator is a data governance implementation of \"Schema on Write\", which enforces data conformance prior to event publication. An alternative strategy is Schema On Read , where data formats are not enforced on write. Instead, consuming Event Processing Applications are required to validate data formats as they read each event. Server-side schema validation is preferable when you want to enforce this pattern centrally inside an organization. In contrast, client-side validation assumes the cooperation of client applications and their developers, which may or may not be acceptable (e.g., in regulated industries). Schema validation results in a load increase because it impacts the write path of every event. Client-side validation impacts primarily the load of the client applications. Server-side schema validation increases the load on the event streaming platform, whereas client applications are less affected (here, the main impact is dealing with rejected events; see Dead Letter Stream ).","title":"Considerations"},{"location":"event-source/schema-validator/#references","text":"See the Schema Compatibility pattern for information on how schemas can evolve over time and be verified. Learn more how to Manage and Validate Schemas with Confluent and Kafka .","title":"References"},{"location":"event-storage/compacted-event-stream/","text":"Compacted Event Stream Event Streams often represent keyed snapshots of state, similar to a table in a relational database. That is, the Events contain a primary key (identifier) and data that represents the latest information of the business entity related to the Event, such as the latest balance per customer account. Event Processing Applications will need to process these Events to determine the current state of the business entity. However, processing the entire Event Stream history is often not practical. Problem How can a (keyed) table be stored in an Event Stream forever, using the minimum amount of space? Solution Remove events from the Event Stream that represent outdated information and have been superseded by new Events . The table's current data (i.e., its latest state) is represented by the remaining Events in the stream. This approach bounds the storage space of the table's Event Stream to \u0398(number of unique keys currently in table), rather than \u0398(total number of change events for table). In practice, the number of unique keys (e.g., unique customer IDs) is typically much smaller than the number of table changes (e.g., total number of changes across all customer profiles). A Compacted Event Stream thus reduces the storage space significantly in most cases. Implementation Apache Kafka\u00ae provides this functionality natively through its Topic Compaction feature. An Event Stream (topic in Kafka) is scanned periodically to remove any old Events that have been superseded by newer Events that have the same key, such as as the same customer ID. Note that compaction is an asynchronous process in Kafka, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted Event Stream called customer-profiles with Kafka: \u279c kafka-topics --create \\ --bootstrap-server <bootstrap-url> \\ --replication-factor 3 \\ --partitions 3 \\ --topic customer-profiles \\ --config cleanup.policy=compact Created topic customer-profiles. The kafka-topics command can also verify the current topic's configuration: \u279c kafka-topics --bootstrap-server localhost:9092 --topic customer-profiles --describe Topic: customer-profiles PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: customer-profiles Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline: Considerations Compacted Event Streams allow for some optimizations: First, they allow the Event Streaming Platform to limit the storage growth of the Event Stream in a data-specific way, rather than removing Events universally after a pre-configured period of time. Second, having smaller Event Streams allows for faster recovery and system migration strategies. It is important to understand that compaction, on purpose, removes historical data from an Event Stream by removing superseded Events as defined above. In many use cases, however, historical data should not be removed, such as for a stream of financial transactions, where every single transaction needs to be recorded and stored. Here, if the storage of the Event Stream is the primary concern, use an Infinite Retention Event Stream instead of a compacted stream. References Compacted Event Streams are highly related to the State Table pattern. Compacted Event Streams work a bit like simple Log Structured Merge Trees . Cleanup policy configuration of Kafka topics.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#compacted-event-stream","text":"Event Streams often represent keyed snapshots of state, similar to a table in a relational database. That is, the Events contain a primary key (identifier) and data that represents the latest information of the business entity related to the Event, such as the latest balance per customer account. Event Processing Applications will need to process these Events to determine the current state of the business entity. However, processing the entire Event Stream history is often not practical.","title":"Compacted Event Stream"},{"location":"event-storage/compacted-event-stream/#problem","text":"How can a (keyed) table be stored in an Event Stream forever, using the minimum amount of space?","title":"Problem"},{"location":"event-storage/compacted-event-stream/#solution","text":"Remove events from the Event Stream that represent outdated information and have been superseded by new Events . The table's current data (i.e., its latest state) is represented by the remaining Events in the stream. This approach bounds the storage space of the table's Event Stream to \u0398(number of unique keys currently in table), rather than \u0398(total number of change events for table). In practice, the number of unique keys (e.g., unique customer IDs) is typically much smaller than the number of table changes (e.g., total number of changes across all customer profiles). A Compacted Event Stream thus reduces the storage space significantly in most cases.","title":"Solution"},{"location":"event-storage/compacted-event-stream/#implementation","text":"Apache Kafka\u00ae provides this functionality natively through its Topic Compaction feature. An Event Stream (topic in Kafka) is scanned periodically to remove any old Events that have been superseded by newer Events that have the same key, such as as the same customer ID. Note that compaction is an asynchronous process in Kafka, so a compacted stream may contain some superseded events, which are waiting to be compacted away. To create a compacted Event Stream called customer-profiles with Kafka: \u279c kafka-topics --create \\ --bootstrap-server <bootstrap-url> \\ --replication-factor 3 \\ --partitions 3 \\ --topic customer-profiles \\ --config cleanup.policy=compact Created topic customer-profiles. The kafka-topics command can also verify the current topic's configuration: \u279c kafka-topics --bootstrap-server localhost:9092 --topic customer-profiles --describe Topic: customer-profiles PartitionCount: 3 ReplicationFactor: 1 Configs: cleanup.policy=compact,segment.bytes=1073741824 Topic: customer-profiles Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Offline: Topic: customer-profiles Partition: 2 Leader: 0 Replicas: 0 Isr: 0 Offline:","title":"Implementation"},{"location":"event-storage/compacted-event-stream/#considerations","text":"Compacted Event Streams allow for some optimizations: First, they allow the Event Streaming Platform to limit the storage growth of the Event Stream in a data-specific way, rather than removing Events universally after a pre-configured period of time. Second, having smaller Event Streams allows for faster recovery and system migration strategies. It is important to understand that compaction, on purpose, removes historical data from an Event Stream by removing superseded Events as defined above. In many use cases, however, historical data should not be removed, such as for a stream of financial transactions, where every single transaction needs to be recorded and stored. Here, if the storage of the Event Stream is the primary concern, use an Infinite Retention Event Stream instead of a compacted stream.","title":"Considerations"},{"location":"event-storage/compacted-event-stream/#references","text":"Compacted Event Streams are highly related to the State Table pattern. Compacted Event Streams work a bit like simple Log Structured Merge Trees . Cleanup policy configuration of Kafka topics.","title":"References"},{"location":"event-storage/event-store/","text":"Event Store When considering an architecture based on an Event Streaming Platform , the first fundamental question is, \"How do we store our events?\" This isn't as obvious as it first sounds, as we have to consider persistence, query performance, write throughput, availability, auditing and many other concerns. This decision will affect all the ones that follow. Problem How can events be stored such that they form a reliable source of truth for applications? Solution Incoming events are stored in an Event Stream , implemented as an append-only log. By choosing this data structure, we can guarantee constant-time (\u0398(1)) writes, lock-free concurrent reads, and straightforward replication across multiple machines. Implementation Apache Kafka\u00ae is an event store that maintains a persistent, append-only stream \u2014 a topic \u2014 for each kind of event we need to store. These topics are: Write-efficient - an append-only log is one of the fastest, cheapest data structures to write to. Read efficient - multiple readers (cf. Event Processor ) can consume the same stream without blocking. Durable - all events are written to storage (e.g., local disk, network storage device), either synchronously (for maximum reliability) or asynchronously (for maximum throughput). Events can be as long-lived as needed, and even stored forever. Highly-available - each event is written to multiple storage devices and replicated across multiple machines, and in the case of failure one of the redundant machines takes over. Auditable - every change is captured and persisted. Every result can be traced back to its source event(s). Considerations It's worth briefly contrasting Apache Kafka\u00ae with message queues and relational databases. While queues also concern themselves with a stream of events, they often consider events as short-lived, independent messages. A message may only exist in memory, or it may be durable enough for data to survive server restarts, but in general they aren't intended to hold on to events for months or even years. Further, their querying capabilities may be limited to simple filtering, offloading more complex queries like joins and aggregations to the application level. In contrast, relational databases are very good at maintaining a persistent state of the world in perpetuity, and answering arbitrary questions about it, but they often fall short on auditing - answering which events led up to the current state - and on liveness - what new events do we need to consider. They are predominantly designed for use cases that operate on data at rest, whereas an Event Store is designed from the ground up for data in motion and event streaming. By beginning with a fundamental data-structure for event capture, and building on that to provide long-term persistence and arbitrary analysis capabilities, Apache Kafka\u00ae provides an ideal choice of event store for modern, data-driven architectures. References See also: Geo-Replication . Using logs to build a solid data infrastructure What Is Apache Kafka? Kafka: The Definitive Guide free Ebook.","title":"Event Store"},{"location":"event-storage/event-store/#event-store","text":"When considering an architecture based on an Event Streaming Platform , the first fundamental question is, \"How do we store our events?\" This isn't as obvious as it first sounds, as we have to consider persistence, query performance, write throughput, availability, auditing and many other concerns. This decision will affect all the ones that follow.","title":"Event Store"},{"location":"event-storage/event-store/#problem","text":"How can events be stored such that they form a reliable source of truth for applications?","title":"Problem"},{"location":"event-storage/event-store/#solution","text":"Incoming events are stored in an Event Stream , implemented as an append-only log. By choosing this data structure, we can guarantee constant-time (\u0398(1)) writes, lock-free concurrent reads, and straightforward replication across multiple machines.","title":"Solution"},{"location":"event-storage/event-store/#implementation","text":"Apache Kafka\u00ae is an event store that maintains a persistent, append-only stream \u2014 a topic \u2014 for each kind of event we need to store. These topics are: Write-efficient - an append-only log is one of the fastest, cheapest data structures to write to. Read efficient - multiple readers (cf. Event Processor ) can consume the same stream without blocking. Durable - all events are written to storage (e.g., local disk, network storage device), either synchronously (for maximum reliability) or asynchronously (for maximum throughput). Events can be as long-lived as needed, and even stored forever. Highly-available - each event is written to multiple storage devices and replicated across multiple machines, and in the case of failure one of the redundant machines takes over. Auditable - every change is captured and persisted. Every result can be traced back to its source event(s).","title":"Implementation"},{"location":"event-storage/event-store/#considerations","text":"It's worth briefly contrasting Apache Kafka\u00ae with message queues and relational databases. While queues also concern themselves with a stream of events, they often consider events as short-lived, independent messages. A message may only exist in memory, or it may be durable enough for data to survive server restarts, but in general they aren't intended to hold on to events for months or even years. Further, their querying capabilities may be limited to simple filtering, offloading more complex queries like joins and aggregations to the application level. In contrast, relational databases are very good at maintaining a persistent state of the world in perpetuity, and answering arbitrary questions about it, but they often fall short on auditing - answering which events led up to the current state - and on liveness - what new events do we need to consider. They are predominantly designed for use cases that operate on data at rest, whereas an Event Store is designed from the ground up for data in motion and event streaming. By beginning with a fundamental data-structure for event capture, and building on that to provide long-term persistence and arbitrary analysis capabilities, Apache Kafka\u00ae provides an ideal choice of event store for modern, data-driven architectures.","title":"Considerations"},{"location":"event-storage/event-store/#references","text":"See also: Geo-Replication . Using logs to build a solid data infrastructure What Is Apache Kafka? Kafka: The Definitive Guide free Ebook.","title":"References"},{"location":"event-storage/infinite-retention-event-stream/","text":"Infinite Retention Event Stream Many use cases demand that Events in an Event Stream will be stored for forever so that the dataset is available in its entirety. Problem How can we ensure that events in a stream are retained forever? Solution The solution for infinite retention depends on the specific Event Streaming Platform . Some platforms support infinite retention \"out of the box\", requiring no action on behalf of the end users. If an Event Streaming Platform does not support infinite storage, infinite retention can be partially achieved with an Event Sink Connector pattern which offloads Events into permanent external storage. Implementation When using Confluent Cloud , infinite retention is built into the Event Streaming Platform ( availability may be limited based on cluster type and cloud provider ). Users of the platform can benefit from infinite storage without any changes to their client applications or operations. For on-premises Event Streaming Platforms , Confluent Platform adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived Events are considered \"hot\", but as time moves on, they become \"colder\" and migrate to more cost-effective external storage like an AWS S3 bucket. As cloud-native object stores can effectively scale to infinite size, the Kafka cluster can act as the system of record for infinite Event Streams . Considerations Infinite Retention Streams are typically used to store entire datasets which will be used by many subscribers. For example, storing the canonical customer dataset in an Infinite Retention Event Stream makes it available to any other system, regardless of their database technology. The customer's dataset can be easily imported or reimported as a whole. Compacted Event Streams are often used as a form of Infinite Retention Event Stream. However compacted streams are not infinite. Instead, they retain only the most recent Events for each key, meaning their contents matches the dataset held in an equivalent CRUD database table. References The blog post Infinite Storage in Confluent goes describes the tiered storage approach in more detail. An Event Sink Connector can be used to implement an infinite retention event stream by loading Event into permanent external storage.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#infinite-retention-event-stream","text":"Many use cases demand that Events in an Event Stream will be stored for forever so that the dataset is available in its entirety.","title":"Infinite Retention Event Stream"},{"location":"event-storage/infinite-retention-event-stream/#problem","text":"How can we ensure that events in a stream are retained forever?","title":"Problem"},{"location":"event-storage/infinite-retention-event-stream/#solution","text":"The solution for infinite retention depends on the specific Event Streaming Platform . Some platforms support infinite retention \"out of the box\", requiring no action on behalf of the end users. If an Event Streaming Platform does not support infinite storage, infinite retention can be partially achieved with an Event Sink Connector pattern which offloads Events into permanent external storage.","title":"Solution"},{"location":"event-storage/infinite-retention-event-stream/#implementation","text":"When using Confluent Cloud , infinite retention is built into the Event Streaming Platform ( availability may be limited based on cluster type and cloud provider ). Users of the platform can benefit from infinite storage without any changes to their client applications or operations. For on-premises Event Streaming Platforms , Confluent Platform adds the ability for infinite retention by extending Apache Kafka with Tiered Storage . Tiered storage separates the compute and storage layers, allowing the operator to scale either of those independently as needed. Newly arrived Events are considered \"hot\", but as time moves on, they become \"colder\" and migrate to more cost-effective external storage like an AWS S3 bucket. As cloud-native object stores can effectively scale to infinite size, the Kafka cluster can act as the system of record for infinite Event Streams .","title":"Implementation"},{"location":"event-storage/infinite-retention-event-stream/#considerations","text":"Infinite Retention Streams are typically used to store entire datasets which will be used by many subscribers. For example, storing the canonical customer dataset in an Infinite Retention Event Stream makes it available to any other system, regardless of their database technology. The customer's dataset can be easily imported or reimported as a whole. Compacted Event Streams are often used as a form of Infinite Retention Event Stream. However compacted streams are not infinite. Instead, they retain only the most recent Events for each key, meaning their contents matches the dataset held in an equivalent CRUD database table.","title":"Considerations"},{"location":"event-storage/infinite-retention-event-stream/#references","text":"The blog post Infinite Storage in Confluent goes describes the tiered storage approach in more detail. An Event Sink Connector can be used to implement an infinite retention event stream by loading Event into permanent external storage.","title":"References"},{"location":"event-stream/event-broker/","text":"Event Broker Loosely coupled components in a software architecture allow services and applications to change with minimal impact on dependent systems and applications. On the side of the organization, this loose coupling also allows different development teams to efficiently work independently from each another. Problem How can we decouple Event Sources from Event Sinks , both of which may include cloud services, systems like relational databases, as well as applications and microservices? Solution We use the Event Broker of an Event Streaming Platform for this decoupling. Typically, multiple such brokers are deployed as a distributed cluster to ensure elasticity, scalability, and fault-tolerance during operations. Event brokers collaborate on receiving and durably storing (write operations) as well as serving (read operations) Events into Event Streams from one or many clients in parallel. Clients that produce events are called the Event Sources , which are decoupled and isolated through the brokers from the clients that consume events, called the Event Sinks . Typically, the technical architecture follows the design of \"dumb brokers, smart clients\". Here, the broker intentionally limits its client-facing functionality to achieve the best performance and scalability. This means that additional work has to be performed by the broker's clients. For example, unlike in traditional messaging brokers, it is the responsibility of an event sink (consumer) to track its individual progress of reading and processing from an event stream. Implementation Apache Kafka\u00ae is an open-source distributed Event Streaming Platform , which implements this Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. Many Event Processing Applications can produce, consume, and process Events from the cluster in parallel with strong guarantees such as transactions, using a fully decoupled yet coordinated architecture. Additionally, Kafka's protocol provides strong backwards and forwards compatibility guarantees between the server-side brokers and their client applications that produce, consume, and process events. For example, a cluster of brokers running an older version of Kafka can be used by client applications using a new version of Kafka. Similarly, older client applications keep working even when the cluster of brokers is upgraded to a newer version of Kafka (and Kafka also supports in-place version upgrades of clusters). This is another facet of decoupling the various components in a Kafka-based architecture, which results in even better flexibility during design and operations. Considerations Counter to traditional message brokers, event brokers provide a distributed, durable, and fault-tolerant storage layer. This has several important benefits. For instance, client applications can initiate and resume event production and consumption independently from each other. Similarly, they don't need to be connected to the brokers perpetually in order to not miss any events. When an application is taken offline for maintenance and subsequently restarted, then it will automatically resume its consumption and processing of an event stream exactly at the point where it stopped before. The strong guarantees provided by the brokers in the event streaming platform ensure that applications do not suffer from duplicate data or from data loss (e.g., missing out on events that were written during the maintenance window) in these situations, even in the face of failures such as machine or network outages. Another benefit is that client applications can \"rewind the time\" and re-consume historical data in event streams as often as needed. This is useful in many situations, including training and retraining models for machine learning, A/B testing, auditing and compliance, as well as fixing unexpected application errors and bugs that happened in production. References This pattern is derived from Message Broker in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"Event Broker"},{"location":"event-stream/event-broker/#event-broker","text":"Loosely coupled components in a software architecture allow services and applications to change with minimal impact on dependent systems and applications. On the side of the organization, this loose coupling also allows different development teams to efficiently work independently from each another.","title":"Event Broker"},{"location":"event-stream/event-broker/#problem","text":"How can we decouple Event Sources from Event Sinks , both of which may include cloud services, systems like relational databases, as well as applications and microservices?","title":"Problem"},{"location":"event-stream/event-broker/#solution","text":"We use the Event Broker of an Event Streaming Platform for this decoupling. Typically, multiple such brokers are deployed as a distributed cluster to ensure elasticity, scalability, and fault-tolerance during operations. Event brokers collaborate on receiving and durably storing (write operations) as well as serving (read operations) Events into Event Streams from one or many clients in parallel. Clients that produce events are called the Event Sources , which are decoupled and isolated through the brokers from the clients that consume events, called the Event Sinks . Typically, the technical architecture follows the design of \"dumb brokers, smart clients\". Here, the broker intentionally limits its client-facing functionality to achieve the best performance and scalability. This means that additional work has to be performed by the broker's clients. For example, unlike in traditional messaging brokers, it is the responsibility of an event sink (consumer) to track its individual progress of reading and processing from an event stream.","title":"Solution"},{"location":"event-stream/event-broker/#implementation","text":"Apache Kafka\u00ae is an open-source distributed Event Streaming Platform , which implements this Event Broker pattern. Kafka runs as a highly scalable and fault-tolerant cluster of brokers. Many Event Processing Applications can produce, consume, and process Events from the cluster in parallel with strong guarantees such as transactions, using a fully decoupled yet coordinated architecture. Additionally, Kafka's protocol provides strong backwards and forwards compatibility guarantees between the server-side brokers and their client applications that produce, consume, and process events. For example, a cluster of brokers running an older version of Kafka can be used by client applications using a new version of Kafka. Similarly, older client applications keep working even when the cluster of brokers is upgraded to a newer version of Kafka (and Kafka also supports in-place version upgrades of clusters). This is another facet of decoupling the various components in a Kafka-based architecture, which results in even better flexibility during design and operations.","title":"Implementation"},{"location":"event-stream/event-broker/#considerations","text":"Counter to traditional message brokers, event brokers provide a distributed, durable, and fault-tolerant storage layer. This has several important benefits. For instance, client applications can initiate and resume event production and consumption independently from each other. Similarly, they don't need to be connected to the brokers perpetually in order to not miss any events. When an application is taken offline for maintenance and subsequently restarted, then it will automatically resume its consumption and processing of an event stream exactly at the point where it stopped before. The strong guarantees provided by the brokers in the event streaming platform ensure that applications do not suffer from duplicate data or from data loss (e.g., missing out on events that were written during the maintenance window) in these situations, even in the face of failures such as machine or network outages. Another benefit is that client applications can \"rewind the time\" and re-consume historical data in event streams as often as needed. This is useful in many situations, including training and retraining models for machine learning, A/B testing, auditing and compliance, as well as fixing unexpected application errors and bugs that happened in production.","title":"Considerations"},{"location":"event-stream/event-broker/#references","text":"This pattern is derived from Message Broker in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf","title":"References"},{"location":"event-stream/event-stream/","text":"Event Stream Event Processing Applications need to communicate, and ideally the communication is facilitated with Events . The applications need a standard mechanism to use for this communication. Problem How can Event Processors and applications communicate with each other, using event streaming? Solution Connect the Event Processing Applications with an Event Stream. Event Sources produce Events to the Event Stream, and Event Processors and Event Sinks consume them. Event Streams are named, allowing communication over a specific stream of Events . Note how Event Streams decouple the source and sink applications, which communicate indirectly and asynchronously with each other through events. Additionally, Event data formats are often validated in order to govern the communication between applications. Generally speaking, an Event Stream records the history of what has happened in the world as a sequence of events (think: a sequence of facts). An example stream is a sales ledger or the sequence of moves in a chess match. This history is an ordered sequence or chain of events, so we know which event happened before another event to infer causality (e.g., \u201cWhite moved the e2 pawn to e4, then Black moved the e7 pawn to e5\u201d). A stream thus represents both the past and the present: as we go from today to tomorrow\u2014or from one millisecond to the next\u2014new events are constantly being appended to the history. Technically, a stream provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and we can have many events for one key, such as the customer ID as the key for a stream of payments of all customers. Implementation The streaming database ksqlDB supports Event Streams using a familiar SQL syntax. The following example creates a stream of events named riderLocations , representing locations of riders in a car-sharing service. The data format is JSON . CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) WITH (kafka_topic='locations', value_format='json'); New Events can be written to the riderLocations stream using the INSERT syntax: INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('c2309eec', 37.7877, -122.4205); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('18f4ea86', 37.3903, -122.0643); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ab5cbad', 37.3952, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('8b6eae59', 37.3944, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4a7c7b41', 37.4049, -122.0822); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ddad000', 37.7857, -122.4011); A push query a.k.a. a streaming query can be ran continuously over the stream using a SELECT , using the EMIT CHANGES clause. As new events arrive, this query will emit new results matching the WHERE conditionals. The following query looks for riders in close proximity to Mountain View, California, in the United States. -- Mountain View lat, long: 37.4133, -122.1162 SELECT * FROM riderLocations WHERE GEO_DISTANCE(latitude, longitude, 37.4133, -122.1162) <= 5 EMIT CHANGES; References This pattern is derived from Message Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The blog post series, Streams and Tables in Apache Kafka: A Primer goes into detail on streams, tables and other Kafka fundamentals.","title":"Event Stream"},{"location":"event-stream/event-stream/#event-stream","text":"Event Processing Applications need to communicate, and ideally the communication is facilitated with Events . The applications need a standard mechanism to use for this communication.","title":"Event Stream"},{"location":"event-stream/event-stream/#problem","text":"How can Event Processors and applications communicate with each other, using event streaming?","title":"Problem"},{"location":"event-stream/event-stream/#solution","text":"Connect the Event Processing Applications with an Event Stream. Event Sources produce Events to the Event Stream, and Event Processors and Event Sinks consume them. Event Streams are named, allowing communication over a specific stream of Events . Note how Event Streams decouple the source and sink applications, which communicate indirectly and asynchronously with each other through events. Additionally, Event data formats are often validated in order to govern the communication between applications. Generally speaking, an Event Stream records the history of what has happened in the world as a sequence of events (think: a sequence of facts). An example stream is a sales ledger or the sequence of moves in a chess match. This history is an ordered sequence or chain of events, so we know which event happened before another event to infer causality (e.g., \u201cWhite moved the e2 pawn to e4, then Black moved the e7 pawn to e5\u201d). A stream thus represents both the past and the present: as we go from today to tomorrow\u2014or from one millisecond to the next\u2014new events are constantly being appended to the history. Technically, a stream provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and we can have many events for one key, such as the customer ID as the key for a stream of payments of all customers.","title":"Solution"},{"location":"event-stream/event-stream/#implementation","text":"The streaming database ksqlDB supports Event Streams using a familiar SQL syntax. The following example creates a stream of events named riderLocations , representing locations of riders in a car-sharing service. The data format is JSON . CREATE STREAM riderLocations (profileId VARCHAR, latitude DOUBLE, longitude DOUBLE) WITH (kafka_topic='locations', value_format='json'); New Events can be written to the riderLocations stream using the INSERT syntax: INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('c2309eec', 37.7877, -122.4205); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('18f4ea86', 37.3903, -122.0643); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ab5cbad', 37.3952, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('8b6eae59', 37.3944, -122.0813); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4a7c7b41', 37.4049, -122.0822); INSERT INTO riderLocations (profileId, latitude, longitude) VALUES ('4ddad000', 37.7857, -122.4011); A push query a.k.a. a streaming query can be ran continuously over the stream using a SELECT , using the EMIT CHANGES clause. As new events arrive, this query will emit new results matching the WHERE conditionals. The following query looks for riders in close proximity to Mountain View, California, in the United States. -- Mountain View lat, long: 37.4133, -122.1162 SELECT * FROM riderLocations WHERE GEO_DISTANCE(latitude, longitude, 37.4133, -122.1162) <= 5 EMIT CHANGES;","title":"Implementation"},{"location":"event-stream/event-stream/#references","text":"This pattern is derived from Message Channel in Enterprise Integration Patterns by Gregor Hohpe and Bobby Woolf The blog post series, Streams and Tables in Apache Kafka: A Primer goes into detail on streams, tables and other Kafka fundamentals.","title":"References"},{"location":"event-stream/partitioned-parallelism/","text":"Partitioned Parallelism If service goals mandate high throughput, it is useful to have the ability to distribute event storage as well as event production and consumption for parallel processing. Being able to distribute events and process them concurrently enables an application to scale. Problem How can we allocate events across Event Streams and Tables so that they can be processed concurrently by distributed Event Processors ? Solution We should use a partitioned event stream, and then assign the events to different partitions of the stream. Essentially, a partition is a unit of parallelism for storage as well as reading, writing, and processing events. Partitioning helps concurrency and scalability in these main ways: Platform scalability: enables different Event Brokers to store and serve Events to Event Processing Applications concurrently Application scalability: enable different Event Processing Applications to process Events concurrently Partitioning events also impacts application semantics: placing events into a given partition guarantees that the ordering of events is preserved, per partition (but typically not across different partitions of the same stream). This ordering guarantee is crucial for many use cases as, very often, the sequencing of events matters, e.g., when processing retail orders (an order must be paid before it can be shipped). Implementation With Apache Kafka\u00ae, streams (called topics ) are created either by an administrator or by a streaming application like ksqlDB . The number of partitions is specified at the time the topic is created. For example: ccloud kafka topic create myTopic --partitions 30 Events are placed into a specific partition according to the partitioning algorithm of the Event Source , such as an Event Processing Application . All events assigned into the same partition have strong ordering guarantees. The common partitioning schemes are: Partitioning based on the event key (e.g., the customer ID for a stream of customer payments), where events with the same key are stored in the same partition Partitioning events round-robin across all partitions to achieve an even distribution of events per partition A custom partitioning algorithm, tailored to a specific use case. If we are using a Kafka-based technology such as a Kafka Streams application or the streaming database ksqlDB , the processors can scale by working on a set of partitions concurrently and in distributed manner. If an event stream's key content changes because of how the query wants to process the rows, for example to execute a JOIN operation in ksqlDB between two streams of events, the underlying keys are recalculated, and the events are sent to a new partition in the new topic to perform the computation (this internal operation is often called distributed data shuffling ). CREATE STREAM stream_name WITH ([...,] PARTITIONS=number_of_partitions) AS SELECT select_expr [, ...] FROM from_stream PARTITION BY new_key_expr [, ...] EMIT CHANGES; Considerations In general, a higher number of stream partitions results in higher throughput, and to maximize throughput, we want enough partitions to utilize all distributed instances of an Event Processor (e.g., servers in a ksqlDB cluster). Be sure to choose the partition count carefully based on the throughput of Event Sources (e.g., producers in Kafka, including connectors), Event Processors (e.g., ksqlDB, Kafka Streams applications), and Event Sinks (e.g., consumers in Kafka, including connectors), and to benchmark performance in the environment. Also take into consideration the design of data patterns and key assignments so that events are distributed as evenly as possible across the stream partitions. This will prevent certain stream partitions from getting overloaded relative to other stream partitions. See Streams and Tables in Apache Kafka: Elasticity, Fault Tolerance, and Other Advanced Concepts for further details on understanding and dealing with partition skew. References The blog post How to choose the number of topics/partitions in a Kafka cluster provides helpful guidance for selecting partition counts for your topics. For another approach to processing parallelism that subdivides the unit of work from a partition down to an event or an event key, see the Confluent Parallel Consumer for Kafka .","title":"Partitioned Parallelism"},{"location":"event-stream/partitioned-parallelism/#partitioned-parallelism","text":"If service goals mandate high throughput, it is useful to have the ability to distribute event storage as well as event production and consumption for parallel processing. Being able to distribute events and process them concurrently enables an application to scale.","title":"Partitioned Parallelism"},{"location":"event-stream/partitioned-parallelism/#problem","text":"How can we allocate events across Event Streams and Tables so that they can be processed concurrently by distributed Event Processors ?","title":"Problem"},{"location":"event-stream/partitioned-parallelism/#solution","text":"We should use a partitioned event stream, and then assign the events to different partitions of the stream. Essentially, a partition is a unit of parallelism for storage as well as reading, writing, and processing events. Partitioning helps concurrency and scalability in these main ways: Platform scalability: enables different Event Brokers to store and serve Events to Event Processing Applications concurrently Application scalability: enable different Event Processing Applications to process Events concurrently Partitioning events also impacts application semantics: placing events into a given partition guarantees that the ordering of events is preserved, per partition (but typically not across different partitions of the same stream). This ordering guarantee is crucial for many use cases as, very often, the sequencing of events matters, e.g., when processing retail orders (an order must be paid before it can be shipped).","title":"Solution"},{"location":"event-stream/partitioned-parallelism/#implementation","text":"With Apache Kafka\u00ae, streams (called topics ) are created either by an administrator or by a streaming application like ksqlDB . The number of partitions is specified at the time the topic is created. For example: ccloud kafka topic create myTopic --partitions 30 Events are placed into a specific partition according to the partitioning algorithm of the Event Source , such as an Event Processing Application . All events assigned into the same partition have strong ordering guarantees. The common partitioning schemes are: Partitioning based on the event key (e.g., the customer ID for a stream of customer payments), where events with the same key are stored in the same partition Partitioning events round-robin across all partitions to achieve an even distribution of events per partition A custom partitioning algorithm, tailored to a specific use case. If we are using a Kafka-based technology such as a Kafka Streams application or the streaming database ksqlDB , the processors can scale by working on a set of partitions concurrently and in distributed manner. If an event stream's key content changes because of how the query wants to process the rows, for example to execute a JOIN operation in ksqlDB between two streams of events, the underlying keys are recalculated, and the events are sent to a new partition in the new topic to perform the computation (this internal operation is often called distributed data shuffling ). CREATE STREAM stream_name WITH ([...,] PARTITIONS=number_of_partitions) AS SELECT select_expr [, ...] FROM from_stream PARTITION BY new_key_expr [, ...] EMIT CHANGES;","title":"Implementation"},{"location":"event-stream/partitioned-parallelism/#considerations","text":"In general, a higher number of stream partitions results in higher throughput, and to maximize throughput, we want enough partitions to utilize all distributed instances of an Event Processor (e.g., servers in a ksqlDB cluster). Be sure to choose the partition count carefully based on the throughput of Event Sources (e.g., producers in Kafka, including connectors), Event Processors (e.g., ksqlDB, Kafka Streams applications), and Event Sinks (e.g., consumers in Kafka, including connectors), and to benchmark performance in the environment. Also take into consideration the design of data patterns and key assignments so that events are distributed as evenly as possible across the stream partitions. This will prevent certain stream partitions from getting overloaded relative to other stream partitions. See Streams and Tables in Apache Kafka: Elasticity, Fault Tolerance, and Other Advanced Concepts for further details on understanding and dealing with partition skew.","title":"Considerations"},{"location":"event-stream/partitioned-parallelism/#references","text":"The blog post How to choose the number of topics/partitions in a Kafka cluster provides helpful guidance for selecting partition counts for your topics. For another approach to processing parallelism that subdivides the unit of work from a partition down to an event or an event key, see the Confluent Parallel Consumer for Kafka .","title":"References"},{"location":"stream-processing/event-grouper/","text":"Event Grouper An event grouper is a specialized form of an Event Processor that groups events together by a common field, such as a customer ID, and/or by event timestamps (often called windowing or time-based windowing ). Problem How can we group individual but related events from the same Event Stream or Table , so that they can subsequently be processed as a whole? Solution For time-based grouping a.k.a. time-based windowing , we use an Event Processor that groups the related events into windows based on their event timestamps. Most window types have a pre-defined window size, such as 10 minutes or 24 hours. An exception is session windows, where the size of each window varies depending on the time characteristics of the grouped events. For field-based grouping, we use an Event Processor that groups events by one or more data fields, irrespective of the event timestamps. The two grouping approaches are orthogonal and can be composed. For example, to compute 7-day averages for every customer in a stream of payments, we first group the events in the stream by customer ID and by 7-day windows, and then compute the respective averages for each customer+window grouping. Implementation As an example, the streaming database ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES; Considerations When grouping events into time windows, there are various types of groupings possible. Hopping Windows are based on time intervals. They model fixed-sized, possibly overlapping windows. A hopping window is defined by two properties: the window's duration and its advance or \"hop\", interval. Tumbling Windows are a special case of hopping windows. Like hopping windows, tumbling windows are based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window's duration. Session Windows aggregate events into a session, which represents a period of activity separated by a specified gap of inactivity, or \"idleness\". Any records with timestamps that occur within the inactivity gap of existing sessions are merged into the existing session. If a record's timestamp occurs outside of the session gap, a new session is created. See the ksqlDB supported window types and the Kafka Streams supported window types for details and diagrams explaining window types. References The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. Related full tutorials are Session Windows in ksqlDB and Session Windows in Kafka Streams , as well as Hopping Windows in ksqlDB .","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#event-grouper","text":"An event grouper is a specialized form of an Event Processor that groups events together by a common field, such as a customer ID, and/or by event timestamps (often called windowing or time-based windowing ).","title":"Event Grouper"},{"location":"stream-processing/event-grouper/#problem","text":"How can we group individual but related events from the same Event Stream or Table , so that they can subsequently be processed as a whole?","title":"Problem"},{"location":"stream-processing/event-grouper/#solution","text":"For time-based grouping a.k.a. time-based windowing , we use an Event Processor that groups the related events into windows based on their event timestamps. Most window types have a pre-defined window size, such as 10 minutes or 24 hours. An exception is session windows, where the size of each window varies depending on the time characteristics of the grouped events. For field-based grouping, we use an Event Processor that groups events by one or more data fields, irrespective of the event timestamps. The two grouping approaches are orthogonal and can be composed. For example, to compute 7-day averages for every customer in a stream of payments, we first group the events in the stream by customer ID and by 7-day windows, and then compute the respective averages for each customer+window grouping.","title":"Solution"},{"location":"stream-processing/event-grouper/#implementation","text":"As an example, the streaming database ksqlDB provides the capability to group related events by a column and group them into \"windows\" where all the related events have a timestamp within the defined time-window. SELECT product-name, COUNT(*), SUM(price) FROM purchases WINDOW TUMBLING (SIZE 1 MINUTE) GROUP BY product-name EMIT CHANGES;","title":"Implementation"},{"location":"stream-processing/event-grouper/#considerations","text":"When grouping events into time windows, there are various types of groupings possible. Hopping Windows are based on time intervals. They model fixed-sized, possibly overlapping windows. A hopping window is defined by two properties: the window's duration and its advance or \"hop\", interval. Tumbling Windows are a special case of hopping windows. Like hopping windows, tumbling windows are based on time intervals. They model fixed-size, non-overlapping, gap-less windows. A tumbling window is defined by a single property: the window's duration. Session Windows aggregate events into a session, which represents a period of activity separated by a specified gap of inactivity, or \"idleness\". Any records with timestamps that occur within the inactivity gap of existing sessions are merged into the existing session. If a record's timestamp occurs outside of the session gap, a new session is created. See the ksqlDB supported window types and the Kafka Streams supported window types for details and diagrams explaining window types.","title":"Considerations"},{"location":"stream-processing/event-grouper/#references","text":"The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. The Tumbling Windows in ksqlDB and the Tumbling Windows in Kafka Streams tutorials provide an end-to-end example for calculating an aggregate calculation over a window of events. Related full tutorials are Session Windows in ksqlDB and Session Windows in Kafka Streams , as well as Hopping Windows in ksqlDB .","title":"References"},{"location":"stream-processing/event-joiner/","text":"Event Joiner Event Streams may need to be joined (i.e., enriched) with a Table or another stream to provide more comprehensive details about their respective Events . Problem How can I enrich an event stream or table with additional context? Solution We can combine events in a stream with another stream or table by performing a join between the two. The join is based on a key the stream and the \"other\" stream or table have in common. Also we can provide a window buffering mechanism based on timestamps so we can produce join results when events from both streams aren't immediately available. Another approach is to join a stream and a table where the table contains more static data resulting in an enriched event stream. Implementation With ksqlDB we can create a stream of events from an existing Kafka topic (note this example's similarity to fact tables in data warehouses): CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (KAFKA_TOPIC='ratings'); Then create a table from on another existing Kafka topic that changes less frequently. This table serves as our reference data (cf. dimension tables in data warehouses). CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (KAFKA_TOPIC='movies'); To create a stream of enriched events, we perform a join between the stream and the table. SELECT ratings.movie_id AS ID, title, release_year, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id EMIT CHANGES; Considerations In ksqlDB, joins between a stream and a table are driven by the stream side of the join. Updates to the table only update the state of the table. It's the new event in the stream that results in a new join result. For example, if we're joining a stream of orders to a table of customers a new order will be enriched if there is a customer record in the table. But if a new customer is added to the table it will not trigger the join condition. The ksqlDB documentation contains more information on stream-table join semantics . We can perform an inner or left-outer join between a stream and a table. Joins are also useful to initiate subsequent processing when two (or more) corresponding events arrive on different streams or tables. References Tutorial: How to join a stream and a lookup table in ksqlDB Tutorial: Joining a stream and a stream in ksqlDB Tutorial: How to join a table and a table in ksqlDB Tutorial: Performing N-way joins in ksqlDB Joining Collections in the ksqlDB documentation","title":"Event Joiner"},{"location":"stream-processing/event-joiner/#event-joiner","text":"Event Streams may need to be joined (i.e., enriched) with a Table or another stream to provide more comprehensive details about their respective Events .","title":"Event Joiner"},{"location":"stream-processing/event-joiner/#problem","text":"How can I enrich an event stream or table with additional context?","title":"Problem"},{"location":"stream-processing/event-joiner/#solution","text":"We can combine events in a stream with another stream or table by performing a join between the two. The join is based on a key the stream and the \"other\" stream or table have in common. Also we can provide a window buffering mechanism based on timestamps so we can produce join results when events from both streams aren't immediately available. Another approach is to join a stream and a table where the table contains more static data resulting in an enriched event stream.","title":"Solution"},{"location":"stream-processing/event-joiner/#implementation","text":"With ksqlDB we can create a stream of events from an existing Kafka topic (note this example's similarity to fact tables in data warehouses): CREATE STREAM ratings (MOVIE_ID INT KEY, rating DOUBLE) WITH (KAFKA_TOPIC='ratings'); Then create a table from on another existing Kafka topic that changes less frequently. This table serves as our reference data (cf. dimension tables in data warehouses). CREATE TABLE movies (ID INT PRIMARY KEY, title VARCHAR, release_year INT) WITH (KAFKA_TOPIC='movies'); To create a stream of enriched events, we perform a join between the stream and the table. SELECT ratings.movie_id AS ID, title, release_year, rating FROM ratings LEFT JOIN movies ON ratings.movie_id = movies.id EMIT CHANGES;","title":"Implementation"},{"location":"stream-processing/event-joiner/#considerations","text":"In ksqlDB, joins between a stream and a table are driven by the stream side of the join. Updates to the table only update the state of the table. It's the new event in the stream that results in a new join result. For example, if we're joining a stream of orders to a table of customers a new order will be enriched if there is a customer record in the table. But if a new customer is added to the table it will not trigger the join condition. The ksqlDB documentation contains more information on stream-table join semantics . We can perform an inner or left-outer join between a stream and a table. Joins are also useful to initiate subsequent processing when two (or more) corresponding events arrive on different streams or tables.","title":"Considerations"},{"location":"stream-processing/event-joiner/#references","text":"Tutorial: How to join a stream and a lookup table in ksqlDB Tutorial: Joining a stream and a stream in ksqlDB Tutorial: How to join a table and a table in ksqlDB Tutorial: Performing N-way joins in ksqlDB Joining Collections in the ksqlDB documentation","title":"References"},{"location":"stream-processing/event-stream-merger/","text":"Event Stream Merger An Event Streaming Application may contain multiple Event Stream instances. But in some cases it may make sense for the application to merge the different event streams into a single event stream, without changing the individual events. While this may seem logically related to a join, the merge is a completely different operation. A join produces results by combining events with the same key to produce a new event, possibly of a different type. Whereas the merge combines the events from multiple streams into a single stream, but the individual events are unchanged and remain independent of each other. Problem How can an application merge different event streams? Solution Implementation The Kafka Streams DSL provides a merge operator which merges two streams into a single stream. We can then take the merged stream and use any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()... Considerations Kafka Streams provides no guarantees on the processing order of records from the underlying streams. When merging streams the key and value types must be the same. References Kafka Tutorial : Merging with Kafka Streams. Kafka Tutorial : Merging streams with ksqlDB.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#event-stream-merger","text":"An Event Streaming Application may contain multiple Event Stream instances. But in some cases it may make sense for the application to merge the different event streams into a single event stream, without changing the individual events. While this may seem logically related to a join, the merge is a completely different operation. A join produces results by combining events with the same key to produce a new event, possibly of a different type. Whereas the merge combines the events from multiple streams into a single stream, but the individual events are unchanged and remain independent of each other.","title":"Event Stream Merger"},{"location":"stream-processing/event-stream-merger/#problem","text":"How can an application merge different event streams?","title":"Problem"},{"location":"stream-processing/event-stream-merger/#solution","text":"","title":"Solution"},{"location":"stream-processing/event-stream-merger/#implementation","text":"The Kafka Streams DSL provides a merge operator which merges two streams into a single stream. We can then take the merged stream and use any number of operations on it. KStream<String, Event> eventStream = builder.stream(...); KStream<String, Event> eventStreamII = builder.stream(...); KStream<String, Event> allEventsStream = eventStream.merge(eventStreamII); allEventsStream.groupByKey()...","title":"Implementation"},{"location":"stream-processing/event-stream-merger/#considerations","text":"Kafka Streams provides no guarantees on the processing order of records from the underlying streams. When merging streams the key and value types must be the same.","title":"Considerations"},{"location":"stream-processing/event-stream-merger/#references","text":"Kafka Tutorial : Merging with Kafka Streams. Kafka Tutorial : Merging streams with ksqlDB.","title":"References"},{"location":"stream-processing/event-time-processing/","text":"Event-Time Processing Consistent time semantics are of particular importance in stream processing. Many operations in an Event Processor are dependent on time, such as joins, aggregations when computed over a window of time (e.g., 5-minute averages), and the handling out-of-order and \"late\" data. In many systems, developers have the choice between different variants of time for an event: Event-time, which captures the time at which an event was originally created by its Event Source . Ingestion-time, which captures the time an event was received on the event stream in an Event Streaming Platform . Wallclock-time or processing-time, which is the time at which a downstream Event Processor happens to process the event (which can be milliseconds, hours, months, etc. after event-time) . Depending on the use case, developers need to pick one variant over the others. Problem How do we implement event-time based processing of events? Solution For event-time processing, the Event Source must include a timestamp in the event (e.g., in a data field or header metadata) that denotes the time at which the event was created by the source. Then, on the consuming side in an Event Processing Application , we need to extract this timestamp from the event, which allows the application process events based on their original timeline. Implementation ksqlDB In the streaming database ksqlDB, every event/record has a system-column named ROWTIME representing the timestamp for the event, which defaults to the time at which the event was originally created by its Event Source . For example, when we create a ksqlDB STREAM or TABLE from an existing Kafka topic, then the timestamp embedded in a Kafka message is extracted and assigned to the event in ksqlDB. (cf. CreateTime of a Kafka ProducerRecord and the Kafka message format ). Sometimes, this default behavior of ksqlDB is not what we want. Maybe the events have a custom data field containing their actual timestamps (e.g., some legacy data that has been around for a while was ingested into Kafka only recently, so we can't trust the CreateTime information in the Kafka messages because they are much newer than the original timestamps). To use a timestamp in the event payload itself, we can add a WITH(TIMESTAMP='some-field') clause when creating a stream or table, which instructs ksqlDB to get the timestamp from the specified field in the record: CREATE STREAM my_event_stream WITH (kafka_topic='events', timestamp='eventTime'); Kafka Streams The Kafka Streams client library of Apache Kafka provides the TimestampExtractor interface for extracting the timestamp from events. The default implementation retrieves the timestamp from the Kafka message (see discussion above) as set by the producer of the message. Normally, this setup results in event-time processing, which is what we want. But for those cases where we need the timestamp from the event payload, we can create our own TimestampExtractor implementation: class OrderTimestampExtractor implements TimestampExtractor { @Override public long extract(ConsumerRecord<Object, Object> record, long partitionTime) { ElectronicOrder order = (ElectronicOrder)record.value(); return order.getTime(); } Generally speaking, this functionality of custom timestamp assignment makes it easy to integrate data from other applications that are not using Kafka Streams or ksqlDB themselves. Additionally, Kafka has the notion of event-time vs. processing-time (wallclock) vs. ingestion time, similar to ksqlDB. Clients like Kafka Streams make it possible to select which variant of time we want to work with in our application. Considerations When considering which time semantics to use, it comes down to the problem domain. In most cases, event-time processing is the recommended option. For example, when re-processing historical event streams (such as for A/B testing, for training machine learning models), only event-time yields correct processing results. If we use processing-time (wall-clock time) to process the last four weeks of events, then an Event Processor will falsely believe that these four weeks of data were created just now in a matter of minutes, which totally breaks the original timeline and temporal distribution of the data and thus leads to incorrect processing results. The difference of event-time to ingestion-time is typically less pronounced than to processing-time as described above, but ingestion-time still suffers from the same conceptual discrepancy between when an event actually occurred in the real world (event-time) vs. when the event was received and stored in the Event Streaming Platform (ingestion-time). If, for some reason, there is a significant delay between event capture and delivery to the Event Streaming Platform , then event-time is the better option. One reason not to use event-time is when we cannot trust the Event Source to provide us with reliable data, which includes the embedded timestamps of events. In this case, ingestion-time can become the preferred option, if fixing the root cause (unreliable event sources) is not a feasible option. References Timestamp assignment in ksqlDB See the tutorial Event-time semantics in ksqlDB for further details on time concepts","title":"Event-Time Processing"},{"location":"stream-processing/event-time-processing/#event-time-processing","text":"Consistent time semantics are of particular importance in stream processing. Many operations in an Event Processor are dependent on time, such as joins, aggregations when computed over a window of time (e.g., 5-minute averages), and the handling out-of-order and \"late\" data. In many systems, developers have the choice between different variants of time for an event: Event-time, which captures the time at which an event was originally created by its Event Source . Ingestion-time, which captures the time an event was received on the event stream in an Event Streaming Platform . Wallclock-time or processing-time, which is the time at which a downstream Event Processor happens to process the event (which can be milliseconds, hours, months, etc. after event-time) . Depending on the use case, developers need to pick one variant over the others.","title":"Event-Time Processing"},{"location":"stream-processing/event-time-processing/#problem","text":"How do we implement event-time based processing of events?","title":"Problem"},{"location":"stream-processing/event-time-processing/#solution","text":"For event-time processing, the Event Source must include a timestamp in the event (e.g., in a data field or header metadata) that denotes the time at which the event was created by the source. Then, on the consuming side in an Event Processing Application , we need to extract this timestamp from the event, which allows the application process events based on their original timeline.","title":"Solution"},{"location":"stream-processing/event-time-processing/#implementation","text":"","title":"Implementation"},{"location":"stream-processing/event-time-processing/#ksqldb","text":"In the streaming database ksqlDB, every event/record has a system-column named ROWTIME representing the timestamp for the event, which defaults to the time at which the event was originally created by its Event Source . For example, when we create a ksqlDB STREAM or TABLE from an existing Kafka topic, then the timestamp embedded in a Kafka message is extracted and assigned to the event in ksqlDB. (cf. CreateTime of a Kafka ProducerRecord and the Kafka message format ). Sometimes, this default behavior of ksqlDB is not what we want. Maybe the events have a custom data field containing their actual timestamps (e.g., some legacy data that has been around for a while was ingested into Kafka only recently, so we can't trust the CreateTime information in the Kafka messages because they are much newer than the original timestamps). To use a timestamp in the event payload itself, we can add a WITH(TIMESTAMP='some-field') clause when creating a stream or table, which instructs ksqlDB to get the timestamp from the specified field in the record: CREATE STREAM my_event_stream WITH (kafka_topic='events', timestamp='eventTime');","title":"ksqlDB"},{"location":"stream-processing/event-time-processing/#kafka-streams","text":"The Kafka Streams client library of Apache Kafka provides the TimestampExtractor interface for extracting the timestamp from events. The default implementation retrieves the timestamp from the Kafka message (see discussion above) as set by the producer of the message. Normally, this setup results in event-time processing, which is what we want. But for those cases where we need the timestamp from the event payload, we can create our own TimestampExtractor implementation: class OrderTimestampExtractor implements TimestampExtractor { @Override public long extract(ConsumerRecord<Object, Object> record, long partitionTime) { ElectronicOrder order = (ElectronicOrder)record.value(); return order.getTime(); } Generally speaking, this functionality of custom timestamp assignment makes it easy to integrate data from other applications that are not using Kafka Streams or ksqlDB themselves. Additionally, Kafka has the notion of event-time vs. processing-time (wallclock) vs. ingestion time, similar to ksqlDB. Clients like Kafka Streams make it possible to select which variant of time we want to work with in our application.","title":"Kafka Streams"},{"location":"stream-processing/event-time-processing/#considerations","text":"When considering which time semantics to use, it comes down to the problem domain. In most cases, event-time processing is the recommended option. For example, when re-processing historical event streams (such as for A/B testing, for training machine learning models), only event-time yields correct processing results. If we use processing-time (wall-clock time) to process the last four weeks of events, then an Event Processor will falsely believe that these four weeks of data were created just now in a matter of minutes, which totally breaks the original timeline and temporal distribution of the data and thus leads to incorrect processing results. The difference of event-time to ingestion-time is typically less pronounced than to processing-time as described above, but ingestion-time still suffers from the same conceptual discrepancy between when an event actually occurred in the real world (event-time) vs. when the event was received and stored in the Event Streaming Platform (ingestion-time). If, for some reason, there is a significant delay between event capture and delivery to the Event Streaming Platform , then event-time is the better option. One reason not to use event-time is when we cannot trust the Event Source to provide us with reliable data, which includes the embedded timestamps of events. In this case, ingestion-time can become the preferred option, if fixing the root cause (unreliable event sources) is not a feasible option.","title":"Considerations"},{"location":"stream-processing/event-time-processing/#references","text":"Timestamp assignment in ksqlDB See the tutorial Event-time semantics in ksqlDB for further details on time concepts","title":"References"},{"location":"stream-processing/logical-and/","text":"Logical AND Event Streams become more interesting when they're considered together. It's often the case that when two separate Events occur, it triggers a new fact that we want to capture. A product can only be dispatched when there's an order and a successful payment. If someone places a bet and their horse wins, then we transfer money to them. How do we combine information from several different event streams and use them to make new events? Problem How can an application trigger processing when two (or more) related events arrive on different streams? Solution Multiple streams of events can be joined together, similar to joins in a relational database. We watch the streams and remember their most recent events (e.g., via an in-memory cache, a local or network storage device) for a certain amount of time. Whenever a new event arrives, we consider it alongside the other recently-captured events and look for matches. If we find one, we emit a new event. For stream-stream joins, it's important to think about what we consider to be a \"recent\" event. We can't join brand new events with arbitrarily-old ones - to join potentially-infinite streams would require potentially-infinite memory. Instead we decide on a retention period that counts as \"new enough\", and only hold on to events in that period. This is often just fine - a payment will usually happen soon after a order is placed. If it doesn't go through within the hour, we can reasonably expect a different process to chase the user for updated credit card details. Implementation As an example, imagine a bank that captures logins to their website, and withdrawals from an ATM. The fraud department might be keen to hear if the same user_id logs in in one country, and makes a withdrawal in a different country, within the same day. (This would not necessarily be fraud, but it's certainly suspicious!) To implement this example, we'll use ksqlDB. We start with two event streams: -- For simplicity's sake, we'll assume that IP addresses -- have already been converted into country codes. CREATE OR REPLACE STREAM logins ( user_id BIGINT, country_code VARCHAR ) WITH ( KAFKA_TOPIC = 'logins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); CREATE OR REPLACE STREAM withdrawals ( user_id BIGINT, country_code VARCHAR, amount DECIMAL(10,2), success BOOLEAN ) WITH ( KAFKA_TOPIC = 'withdrawals_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); We can now join those two streams. Events with the same user_id are considered equal, and we'll specifically look at events that happen WITHIN 1 DAY : CREATE STREAM possible_frauds AS SELECT l.user_id, l.country_code, w.country_code, w.amount, w.success FROM logins l JOIN withdrawals w WITHIN 1 DAY ON l.user_id = w.user_id WHERE l.country_code != w.country_code EMIT CHANGES; Querying that stream in one terminal: SELECT * FROM possible_frauds EMIT CHANGES; ...and inserting some data in another: INSERT INTO logins (user_id, country_code) VALUES (1, 'gb'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO logins (user_id, country_code) VALUES (3, 'be'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'gb', 10.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'au', 250.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'us', 50.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (3, 'be', 20.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'fr', 20.00, true); Results in a stream of possible fraud cases that need further investigation: +-----------+----------------+----------------+--------+---------+ |L_USER_ID |L_COUNTRY_CODE |W_COUNTRY_CODE |AMOUNT |SUCCESS | +-----------+----------------+----------------+--------+---------+ |1 |gb |au |250.00 |true | |2 |us |fr |20.00 |true | |2 |us |fr |20.00 |true | Considerations Joining event streams is fairly simple. The big consideration is how large a retention period we need, and so the resources our join will use. Planning that tradeoff requires careful consideration of the specific problem we're solving. For large retention periods, consider joining a stream to a Projection Table instead. References See also: Joining Streams and Tables in the ksqlDB documentation. The Pipeline pattern, for considering events in series (rather than in parallel). The Projection Table pattern, for a memory-efficient way of considering a stream over a potentially-infinite time-period. Designing Event Driven Systems - \"Chapter 14: Kafka Streams and KSQL\" for further discussion.","title":"Logical AND"},{"location":"stream-processing/logical-and/#logical-and","text":"Event Streams become more interesting when they're considered together. It's often the case that when two separate Events occur, it triggers a new fact that we want to capture. A product can only be dispatched when there's an order and a successful payment. If someone places a bet and their horse wins, then we transfer money to them. How do we combine information from several different event streams and use them to make new events?","title":"Logical AND"},{"location":"stream-processing/logical-and/#problem","text":"How can an application trigger processing when two (or more) related events arrive on different streams?","title":"Problem"},{"location":"stream-processing/logical-and/#solution","text":"Multiple streams of events can be joined together, similar to joins in a relational database. We watch the streams and remember their most recent events (e.g., via an in-memory cache, a local or network storage device) for a certain amount of time. Whenever a new event arrives, we consider it alongside the other recently-captured events and look for matches. If we find one, we emit a new event. For stream-stream joins, it's important to think about what we consider to be a \"recent\" event. We can't join brand new events with arbitrarily-old ones - to join potentially-infinite streams would require potentially-infinite memory. Instead we decide on a retention period that counts as \"new enough\", and only hold on to events in that period. This is often just fine - a payment will usually happen soon after a order is placed. If it doesn't go through within the hour, we can reasonably expect a different process to chase the user for updated credit card details.","title":"Solution"},{"location":"stream-processing/logical-and/#implementation","text":"As an example, imagine a bank that captures logins to their website, and withdrawals from an ATM. The fraud department might be keen to hear if the same user_id logs in in one country, and makes a withdrawal in a different country, within the same day. (This would not necessarily be fraud, but it's certainly suspicious!) To implement this example, we'll use ksqlDB. We start with two event streams: -- For simplicity's sake, we'll assume that IP addresses -- have already been converted into country codes. CREATE OR REPLACE STREAM logins ( user_id BIGINT, country_code VARCHAR ) WITH ( KAFKA_TOPIC = 'logins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); CREATE OR REPLACE STREAM withdrawals ( user_id BIGINT, country_code VARCHAR, amount DECIMAL(10,2), success BOOLEAN ) WITH ( KAFKA_TOPIC = 'withdrawals_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); We can now join those two streams. Events with the same user_id are considered equal, and we'll specifically look at events that happen WITHIN 1 DAY : CREATE STREAM possible_frauds AS SELECT l.user_id, l.country_code, w.country_code, w.amount, w.success FROM logins l JOIN withdrawals w WITHIN 1 DAY ON l.user_id = w.user_id WHERE l.country_code != w.country_code EMIT CHANGES; Querying that stream in one terminal: SELECT * FROM possible_frauds EMIT CHANGES; ...and inserting some data in another: INSERT INTO logins (user_id, country_code) VALUES (1, 'gb'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO logins (user_id, country_code) VALUES (3, 'be'); INSERT INTO logins (user_id, country_code) VALUES (2, 'us'); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'gb', 10.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (1, 'au', 250.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'us', 50.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (3, 'be', 20.00, true); INSERT INTO withdrawals (user_id, country_code, amount, success) VALUES (2, 'fr', 20.00, true); Results in a stream of possible fraud cases that need further investigation: +-----------+----------------+----------------+--------+---------+ |L_USER_ID |L_COUNTRY_CODE |W_COUNTRY_CODE |AMOUNT |SUCCESS | +-----------+----------------+----------------+--------+---------+ |1 |gb |au |250.00 |true | |2 |us |fr |20.00 |true | |2 |us |fr |20.00 |true |","title":"Implementation"},{"location":"stream-processing/logical-and/#considerations","text":"Joining event streams is fairly simple. The big consideration is how large a retention period we need, and so the resources our join will use. Planning that tradeoff requires careful consideration of the specific problem we're solving. For large retention periods, consider joining a stream to a Projection Table instead.","title":"Considerations"},{"location":"stream-processing/logical-and/#references","text":"See also: Joining Streams and Tables in the ksqlDB documentation. The Pipeline pattern, for considering events in series (rather than in parallel). The Projection Table pattern, for a memory-efficient way of considering a stream over a potentially-infinite time-period. Designing Event Driven Systems - \"Chapter 14: Kafka Streams and KSQL\" for further discussion.","title":"References"},{"location":"stream-processing/suppressed-event-aggregator/","text":"Suppressed Event Aggregator An Event Streaming Application can perform continuous aggregation operations like an Event Aggregator . If the input data is not windowed (cf. Event Grouper ), the aggregator will emit \"intermediate\" processing results. That's because an event stream is potentially infinite, so generally we do not know when the input is considered \"complete\". So, with few exceptions, there's not really a point where we can have a \"final\" result. However, if the input data is windowed (e.g., the input is grouped into 5-minute windows in order to compute 5-minute averages), then emitting a \"final\" result per window is possible, because the aggregator knows when the input for a given window is considered complete, and thus it can be configured to suppress \"intermediate\" results until the window time passes. Problem How can an event aggregator provide a final aggregation result, rather than \"intermediate\" results that keep being updated? Solution First, the input events of the aggregator must be windowed via an Event Grouper , i.e., the events are being grouped into \"windows\" based on their timestamps. Depending on the configured grouping, an event is placed exclusively into a single window, or it can be placed into multiple windows. Then, the event aggregator performs its operation on each window. Only once the window is considered to have \"passed\" (e.g., a 5-minute window starting at 09:00am and ending at 09:05am) will the aggregator output a single, final result for this window. For example, consider an aggregation for an event stream of customer payments, where we want to compute the number of payments per hour. By using a window size of 1 hour, we can emit a final count for the hourly number of payments once the respective 1-hour window closes. Ideally, the aggregator is able to handle out-of-order or \"late\" events, which is a common situation to deal with in an event streaming platform (e.g., an event created at 09:03 arrives only at 09:07). Here, a common technique is to let users define a so-called grace period for windows to give delayed events some extra time to arrive. Events that arrive within the grace period of a window will be processed, whereas any later events will be not be processed (e.g., if the grace period is 3 minutes, then the 09:03 event arriving at 09:07 would be included in the 09:00-09:05 window; any events arriving at or after 09:08 would be ignored by this window). Note that the use of a grace period increases the processing latency, because the aggregator has to wait for an additional period of time before it knows the input for a given window is complete and thus before it can output the single, final result for that window. Implementation For Apache Kafka\u00ae, the Kafka Streams client library provides a suppress operator in its DSL, which we can apply to windowed aggregations. In the following example we compute hourly aggregations on a stream of orders, using a grace period of five minutes to wait for any orders arriving with a slight delay. The suppress operator ensures that there's only a single result event for each hourly window. KStream<String, OrderEvent> orderStream = builder.stream(...); orderStream.groupByKey() .windowedBy(TimeWindows.of(Duration.ofHours(1)).grace(Duration.ofMinutes(5))) .aggregate(() -> 0.0 /* initial value of `total`, per window */, (key, order, total) -> total + order.getPrice(), Materialized.with(Serdes.String(), Serdes.Double())) .suppress(untilWindowCloses(unbounded())) .toStream() .map((windowKey, value) -> KeyValue.pair(windowKey.key(),value)) .to(outputTopic, Produced.with(Serdes.String(), Serdes.Double())); Considerations To honor the contract of outputting only a single result per window, the suppressed aggregator typically buffers events in some way until the window closes. If its implementation uses an in-memory buffer, then, depending on the number of events per window and their payload sizes, there's the risk to run into out-of-memory errors. References The tutorial Emit a final result from a time window with Kafka Streams provides more details about the suppress operator.","title":"Suppressed Event Aggregator"},{"location":"stream-processing/suppressed-event-aggregator/#suppressed-event-aggregator","text":"An Event Streaming Application can perform continuous aggregation operations like an Event Aggregator . If the input data is not windowed (cf. Event Grouper ), the aggregator will emit \"intermediate\" processing results. That's because an event stream is potentially infinite, so generally we do not know when the input is considered \"complete\". So, with few exceptions, there's not really a point where we can have a \"final\" result. However, if the input data is windowed (e.g., the input is grouped into 5-minute windows in order to compute 5-minute averages), then emitting a \"final\" result per window is possible, because the aggregator knows when the input for a given window is considered complete, and thus it can be configured to suppress \"intermediate\" results until the window time passes.","title":"Suppressed Event Aggregator"},{"location":"stream-processing/suppressed-event-aggregator/#problem","text":"How can an event aggregator provide a final aggregation result, rather than \"intermediate\" results that keep being updated?","title":"Problem"},{"location":"stream-processing/suppressed-event-aggregator/#solution","text":"First, the input events of the aggregator must be windowed via an Event Grouper , i.e., the events are being grouped into \"windows\" based on their timestamps. Depending on the configured grouping, an event is placed exclusively into a single window, or it can be placed into multiple windows. Then, the event aggregator performs its operation on each window. Only once the window is considered to have \"passed\" (e.g., a 5-minute window starting at 09:00am and ending at 09:05am) will the aggregator output a single, final result for this window. For example, consider an aggregation for an event stream of customer payments, where we want to compute the number of payments per hour. By using a window size of 1 hour, we can emit a final count for the hourly number of payments once the respective 1-hour window closes. Ideally, the aggregator is able to handle out-of-order or \"late\" events, which is a common situation to deal with in an event streaming platform (e.g., an event created at 09:03 arrives only at 09:07). Here, a common technique is to let users define a so-called grace period for windows to give delayed events some extra time to arrive. Events that arrive within the grace period of a window will be processed, whereas any later events will be not be processed (e.g., if the grace period is 3 minutes, then the 09:03 event arriving at 09:07 would be included in the 09:00-09:05 window; any events arriving at or after 09:08 would be ignored by this window). Note that the use of a grace period increases the processing latency, because the aggregator has to wait for an additional period of time before it knows the input for a given window is complete and thus before it can output the single, final result for that window.","title":"Solution"},{"location":"stream-processing/suppressed-event-aggregator/#implementation","text":"For Apache Kafka\u00ae, the Kafka Streams client library provides a suppress operator in its DSL, which we can apply to windowed aggregations. In the following example we compute hourly aggregations on a stream of orders, using a grace period of five minutes to wait for any orders arriving with a slight delay. The suppress operator ensures that there's only a single result event for each hourly window. KStream<String, OrderEvent> orderStream = builder.stream(...); orderStream.groupByKey() .windowedBy(TimeWindows.of(Duration.ofHours(1)).grace(Duration.ofMinutes(5))) .aggregate(() -> 0.0 /* initial value of `total`, per window */, (key, order, total) -> total + order.getPrice(), Materialized.with(Serdes.String(), Serdes.Double())) .suppress(untilWindowCloses(unbounded())) .toStream() .map((windowKey, value) -> KeyValue.pair(windowKey.key(),value)) .to(outputTopic, Produced.with(Serdes.String(), Serdes.Double()));","title":"Implementation"},{"location":"stream-processing/suppressed-event-aggregator/#considerations","text":"To honor the contract of outputting only a single result per window, the suppressed aggregator typically buffers events in some way until the window closes. If its implementation uses an in-memory buffer, then, depending on the number of events per window and their payload sizes, there's the risk to run into out-of-memory errors.","title":"Considerations"},{"location":"stream-processing/suppressed-event-aggregator/#references","text":"The tutorial Emit a final result from a time window with Kafka Streams provides more details about the suppress operator.","title":"References"},{"location":"stream-processing/wait-for-n-events/","text":"Wait for N Events Sometimes Events become significant after they've happened several times. A user can try to log in 5 times, but after that we'll lock their account. A parcel delivery will be attempted 3 times before asking the customer to collect it from the depot. A gamer gets a trophy after they've killed their 100th Blarg. How do we efficiently watch for logically similar events? Problem How can an application wait for a certain number of events to occur before performing processing? Solution To consider related events as a group, we need to group them by a given key, and then count the occurrences of that key. Implementation In ksqlDB we can easily create a Projection Table that groups and counts events by a particular key. As an example, imagine we are handling very large financial transactions. We only want to process them once they've been reviewed and approved by 2 managers. We'll start with a stream of signed events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Querying that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; ...and inserting some data in another: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); Results in a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 | Considerations Note that in the example above, we queried for an exact number of approvals WHERE approvals = 2 . We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ) but that would have emitted a new event for a 3rd approval, and a 4th, and so on. That would be the wrong behavior here, but it might be useful feature in a system where we wanted to reward loyal customers, and send out a discount email for every order after their first 10. References The Event Grouping pattern, for a more general consideration of GROUP BY operations. Designing Event Driven Systems - \"Chapter 15: Building Streaming Services\" for further discussion.","title":"Wait for N Events"},{"location":"stream-processing/wait-for-n-events/#wait-for-n-events","text":"Sometimes Events become significant after they've happened several times. A user can try to log in 5 times, but after that we'll lock their account. A parcel delivery will be attempted 3 times before asking the customer to collect it from the depot. A gamer gets a trophy after they've killed their 100th Blarg. How do we efficiently watch for logically similar events?","title":"Wait for N Events"},{"location":"stream-processing/wait-for-n-events/#problem","text":"How can an application wait for a certain number of events to occur before performing processing?","title":"Problem"},{"location":"stream-processing/wait-for-n-events/#solution","text":"To consider related events as a group, we need to group them by a given key, and then count the occurrences of that key.","title":"Solution"},{"location":"stream-processing/wait-for-n-events/#implementation","text":"In ksqlDB we can easily create a Projection Table that groups and counts events by a particular key. As an example, imagine we are handling very large financial transactions. We only want to process them once they've been reviewed and approved by 2 managers. We'll start with a stream of signed events from managers: CREATE OR REPLACE STREAM trade_reviews ( trade_id BIGINT, manager_id VARCHAR, signature VARCHAR, approved BOOLEAN ) WITH ( KAFKA_TOPIC = 'trade_reviews_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 2 ); We'll group reviews by their trade_id , and COUNT() how many approvals ( approved = TRUE ) we see for each: CREATE OR REPLACE TABLE trade_approval AS SELECT trade_id, COUNT(*) AS approvals FROM trade_reviews WHERE approved = TRUE GROUP BY trade_id; Querying that stream in one terminal: SELECT * FROM trade_approval WHERE approvals = 2 EMIT CHANGES; ...and inserting some data in another: INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'alice', '6f797a', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'alice', 'b523af', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'alice', 'fe1aaf', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'alice', 'f41bf3', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (2, 'bob', '0441ed', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (4, 'bob', '50f237', TRUE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (1, 'carol', 'ee52f5', FALSE); INSERT INTO trade_reviews ( trade_id, manager_id, signature, approved ) VALUES (3, 'carol', '4adb7c', TRUE); Results in a stream of trades that are ready to process: +----------+-----------+ |TRADE_ID |APPROVALS | +----------+-----------+ |2 |2 | |4 |2 |","title":"Implementation"},{"location":"stream-processing/wait-for-n-events/#considerations","text":"Note that in the example above, we queried for an exact number of approvals WHERE approvals = 2 . We could have used a greater-than-or-equal check ( WHERE approvals >= 2 ) but that would have emitted a new event for a 3rd approval, and a 4th, and so on. That would be the wrong behavior here, but it might be useful feature in a system where we wanted to reward loyal customers, and send out a discount email for every order after their first 10.","title":"Considerations"},{"location":"stream-processing/wait-for-n-events/#references","text":"The Event Grouping pattern, for a more general consideration of GROUP BY operations. Designing Event Driven Systems - \"Chapter 15: Building Streaming Services\" for further discussion.","title":"References"},{"location":"stream-processing/wallclock-time/","text":"Wallclock-Time Processing Problem Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested. Solution Pattern Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload. Example Implementation By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime'); References Kafka Tutorial : Event-time semantics","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#wallclock-time-processing","text":"","title":"Wallclock-Time Processing"},{"location":"stream-processing/wallclock-time/#problem","text":"Consistent time semantics are important in stream processing, especially for time-based aggregations when calculating over a window of time. Depending on the use case, the application may use the time when the event occurs (either system wallclock time or embedded time in the payload) or when the event is ingested.","title":"Problem"},{"location":"stream-processing/wallclock-time/#solution-pattern","text":"Every record in ksqlDB has a system-column called ROWTIME that tracks the timestamp of the event. It could be either when the event occurs (producer system time) or when the event is ingested (broker system time), depending on the message.timestamp.type configuration value. ksqlDB also allows streams to use the timestamp from a field in the record payload.","title":"Solution Pattern"},{"location":"stream-processing/wallclock-time/#example-implementation","text":"By default, ksqlDB ROWTIME is inherited from the timestamp in the underlying Kafka record metadata, but it can also be pulled from a field in the event payload itself, as shown below. CREATE STREAM TEMPERATURE_READINGS_EVENTTIME WITH (KAFKA_TOPIC='deviceEvents', VALUE_FORMAT='avro', TIMESTAMP='eventTime');","title":"Example Implementation"},{"location":"stream-processing/wallclock-time/#references","text":"Kafka Tutorial : Event-time semantics","title":"References"},{"location":"table/projection-table/","text":"Projection Table One of the first questions we want to ask of a stream of events is, \"Where are we now?\" We have a stream of sales events, we'd like to have the total sales figures at our fingertips. We have a stream of login events, we'd like to know when each user last logged in. Our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we roll up data efficiently? How do we preserve a complete event log and enjoy the fast queries of an \"update in place\"-style database? Problem How can a stream of change events be summarized into the current state of the world, efficiently? Solution We can maintain projection tables that behave just like materialized views in a traditional database. As new events come in, the table is automatically updated, giving us an always-live picture of the system. Events with the same key are considered related, with newer events being interpreted as updates or deletions (depending on their contents) of older events. Like a materialized view, projection tables are read-only. To change them, we change the underlying data by recording new events to their underlying streams. Implementation ksqlDB supports easy creation of summary tables/materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine we are shipping packages around the world. As they reach each point on their journey, they're logged with their current location. We'll start with a stream of check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and the newest location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Querying that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and inserting some data in another: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); Results in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, package_locations stays updated, so we can see the current location of each package without scanning through the event history every time. Considerations In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, in turn ensuring that for a given package_id , newer events have a higher offset value. Thus when we query for the LATEST_BY_OFFSET , we're always getting the newest event for each package. If we'd chosen a different partitioning key, or not specified one at all, we'd get very different results. LATEST_BY_OFFSET is only one of the many summary functions ksqlDB supports , from simple sums and averages to time-aware functions and histograms. And beyond those, we can easily define our own custom functions or look to Kafka Streams for complete control. References Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. Related patterns: State Table","title":"Projection Table"},{"location":"table/projection-table/#projection-table","text":"One of the first questions we want to ask of a stream of events is, \"Where are we now?\" We have a stream of sales events, we'd like to have the total sales figures at our fingertips. We have a stream of login events, we'd like to know when each user last logged in. Our trucks send GPS data every minute, we'd like to know where each truck is right now. How do we roll up data efficiently? How do we preserve a complete event log and enjoy the fast queries of an \"update in place\"-style database?","title":"Projection Table"},{"location":"table/projection-table/#problem","text":"How can a stream of change events be summarized into the current state of the world, efficiently?","title":"Problem"},{"location":"table/projection-table/#solution","text":"We can maintain projection tables that behave just like materialized views in a traditional database. As new events come in, the table is automatically updated, giving us an always-live picture of the system. Events with the same key are considered related, with newer events being interpreted as updates or deletions (depending on their contents) of older events. Like a materialized view, projection tables are read-only. To change them, we change the underlying data by recording new events to their underlying streams.","title":"Solution"},{"location":"table/projection-table/#implementation","text":"ksqlDB supports easy creation of summary tables/materialized views. We declare them once, and the server will maintain their data as new events stream in. As an example, imagine we are shipping packages around the world. As they reach each point on their journey, they're logged with their current location. We'll start with a stream of check-in events: CREATE OR REPLACE STREAM package_checkins ( package_id BIGINT KEY, location VARCHAR, processed_by VARCHAR ) WITH ( KAFKA_TOPIC = 'package_checkins_topic', VALUE_FORMAT = 'AVRO', PARTITIONS = 3 ); Then we'll create a projection table, tracking each package_id and the newest location : CREATE OR REPLACE TABLE package_locations AS SELECT package_id, LATEST_BY_OFFSET(location) AS current_location FROM package_checkins GROUP BY package_id; Querying that stream in one terminal: SELECT * FROM package_locations EMIT CHANGES; ...and inserting some data in another: INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'New York' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Paris' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'London' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 1, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 2, 'Rome' ); INSERT INTO package_checkins ( package_id, location ) VALUES ( 3, 'Washington' ); Results in a table of each package's last-known location: +------------+------------------+ |PACKAGE_ID |CURRENT_LOCATION | +------------+------------------+ |1 |Rome | |2 |Rome | |3 |Washington | As new data is inserted, package_locations stays updated, so we can see the current location of each package without scanning through the event history every time.","title":"Implementation"},{"location":"table/projection-table/#considerations","text":"In the example above, it's important to consider partitioning. When we declared the package_checkins stream, we marked the package_id as the KEY . This ensures that all events with the same package_id will be stored in the same partition, in turn ensuring that for a given package_id , newer events have a higher offset value. Thus when we query for the LATEST_BY_OFFSET , we're always getting the newest event for each package. If we'd chosen a different partitioning key, or not specified one at all, we'd get very different results. LATEST_BY_OFFSET is only one of the many summary functions ksqlDB supports , from simple sums and averages to time-aware functions and histograms. And beyond those, we can easily define our own custom functions or look to Kafka Streams for complete control.","title":"Considerations"},{"location":"table/projection-table/#references","text":"Aggregate functions in the ksqlDB documentation. Creating custom ksqlDB functions in the ksqlDB documentation. Related patterns: State Table","title":"References"},{"location":"table/state-table/","text":"State Table Event Processors often need to perform stateful operations, such as an aggregation (e.g., counting the number of events). The state is similar to a table in a relational database and is mutable, i.e., it allows for read and write operations. It is essential that the event processor has an efficient and fault-tolerant mechanism for state management, i.e, recording and updating the state while processing input events, to ensure correctness of computations and preventing data loss as well as data duplication. Problem How can an Event Processor manage (mutable) state, similar to a table in a relational database? Solution We need to implement a mutable state table that allows the Event Processor to record and update state. For example, to count the number of payments per customer, the state table provides a mapping between the customer (e.g., a customer ID) and the current count of payments. The state's storage backend can vary by implementation: options include local state stores (e.g., RocksDB), remote state stores (e.g., AWS DynamoDB or a NoSQL database), or in-memory caches. Local state stores are usually recommended as they do not incur additional latency for network roundtrips, which improves the end-to-end performance of the Event Processor. Irrespective of backend choice, the state table should be fault-tolerant to ensure strong processing guarantees, such as exactly-once semantics. Fault-tolerance can be achieved, for example, by attaching an Event Source Connector to the state table to perform change data capture (CDC), thus allowing the Event Processor to continuously backup state changes into an Event Stream and, whenever needed, restore the state table in the case of failure or similar scenarios.here is no network latency to contend with. Also, an approach for restoring the state after a crash destroying the local store occurs. Implementation ksqlDB provides state tables out of the box with its TABLE data collection. Its implementation uses local, fault-tolerant state stores that are continuously backed up into ksqlDB's distributed storage layer (Kafka) so the data is durable. For example, we can maintain a stateful count of all sales with by aggregating the movie_ticket_sales stream into a movie_tickets_sold table: CREATE TABLE movie_tickets_sold AS SELECT title, COUNT(ticket_total_value) AS tickets_sold FROM movie_ticket_sales GROUP BY title EMIT CHANGES; References State store recovery in ksqlDB explains the fault-tolerance of ksqlDB's state management in more detail. Related patterns: Projection Table","title":"State Table"},{"location":"table/state-table/#state-table","text":"Event Processors often need to perform stateful operations, such as an aggregation (e.g., counting the number of events). The state is similar to a table in a relational database and is mutable, i.e., it allows for read and write operations. It is essential that the event processor has an efficient and fault-tolerant mechanism for state management, i.e, recording and updating the state while processing input events, to ensure correctness of computations and preventing data loss as well as data duplication.","title":"State Table"},{"location":"table/state-table/#problem","text":"How can an Event Processor manage (mutable) state, similar to a table in a relational database?","title":"Problem"},{"location":"table/state-table/#solution","text":"We need to implement a mutable state table that allows the Event Processor to record and update state. For example, to count the number of payments per customer, the state table provides a mapping between the customer (e.g., a customer ID) and the current count of payments. The state's storage backend can vary by implementation: options include local state stores (e.g., RocksDB), remote state stores (e.g., AWS DynamoDB or a NoSQL database), or in-memory caches. Local state stores are usually recommended as they do not incur additional latency for network roundtrips, which improves the end-to-end performance of the Event Processor. Irrespective of backend choice, the state table should be fault-tolerant to ensure strong processing guarantees, such as exactly-once semantics. Fault-tolerance can be achieved, for example, by attaching an Event Source Connector to the state table to perform change data capture (CDC), thus allowing the Event Processor to continuously backup state changes into an Event Stream and, whenever needed, restore the state table in the case of failure or similar scenarios.here is no network latency to contend with. Also, an approach for restoring the state after a crash destroying the local store occurs.","title":"Solution"},{"location":"table/state-table/#implementation","text":"ksqlDB provides state tables out of the box with its TABLE data collection. Its implementation uses local, fault-tolerant state stores that are continuously backed up into ksqlDB's distributed storage layer (Kafka) so the data is durable. For example, we can maintain a stateful count of all sales with by aggregating the movie_ticket_sales stream into a movie_tickets_sold table: CREATE TABLE movie_tickets_sold AS SELECT title, COUNT(ticket_total_value) AS tickets_sold FROM movie_ticket_sales GROUP BY title EMIT CHANGES;","title":"Implementation"},{"location":"table/state-table/#references","text":"State store recovery in ksqlDB explains the fault-tolerance of ksqlDB's state management in more detail. Related patterns: Projection Table","title":"References"}]}